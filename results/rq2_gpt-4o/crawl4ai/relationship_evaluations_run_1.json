[
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the 'JsonCssExtractionStrategy' as a feature of Crawl4AI for extracting structured data using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as a feature for extracting data using CSS selectors, but omits the crucial implementation details about how it uses BeautifulSoup and processes the schema with baseSelector and fields",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explains how to use 'JsonCssExtractionStrategy' with the 'AsyncWebCrawler', implying that 'AsyncWebCrawler' utilizes this strategy for web crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main relationship - that AsyncWebCrawler uses JsonCssExtractionStrategy for data extraction. While it's less specific about implementation details, it accurately represents the core functional relationship shown in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions `AsyncWebCrawler` in the code example, showing its use for creating a crawler instance.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only mentions AsyncWebCrawler's existence in code examples, missing the crucial caching functionality that is core to the relationship. While not incorrect, it omits the main functionality being discussed.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is explicitly called within the documented example, demonstrating its role in executing a crawl operation and obtaining results.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() is used for crawling but misses the key relationship about caching functionality that is central to both the code and documentation",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet provides a usage example of `AsyncWebCrawler` for performing a basic crawl, which directly involves utilizing this class in the operation depicted.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that AsyncWebCrawler is used for web crawling, but misses the key context manager pattern (__aenter__/__aexit__) implementation that is central to the ground truth's description of how the class is used.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is explicitly invoked in the example, indicating its role in executing the crawl operation.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method for crawling, but misses crucial aspects of its functionality like HTML extraction, caching, processing and return type that are essential to understanding the code-docs relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "As `AsyncWebCrawler` utilizes a `crawler_strategy`, which can be an instance of `AsyncCrawlerStrategy` or its derivatives, the documentation implicitly relates to the interface or pattern that defines this part of the functionality.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables the basic URL crawling functionality shown in the documentation through its required crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler uses AsyncCrawlerStrategy as an interface/pattern for its functionality, which aligns with the ground truth's explanation that AsyncCrawlerStrategy defines the core interface for URL crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` is a possible default strategy used by `AsyncWebCrawler`, making it an implicit part of how the `arun()` method performs its actions in the provided example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core crawling functionality demonstrated in the documentation's basic usage example by providing the underlying browser automation and HTML extraction capabilities needed for the AsyncWebCrawler.arun() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as being used by AsyncWebCrawler, but calls it a 'possible default strategy' and 'implicit' when the ground truth indicates it definitively implements the core crawling functionality.",
      "error_type": "underspecified_relationship"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text explicitly references the CosineStrategy being instantiated with specific parameters for semantic filtering, word count threshold, and similarity threshold. This demonstrates the usage of the CosineStrategy class as part of the example.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main relationship showing that CosineStrategy can be instantiated with configurable parameters for semantic filtering, word count threshold, and similarity threshold, matching the implementation shown in the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text demonstrates an example of how the AsyncWebCrawler class is used to perform an async crawl with the CosineStrategy, implicitly showing its implementation of web crawling tasks with specified strategies.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship between the code and documentation - that AsyncWebCrawler implements async web crawling with configurable strategies as shown in the example usage. While it's less specific than the ground truth, it correctly identifies the main functionality and relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of AsyncWebCrawler is implicitly referenced by its invocation within the async context, showing how users can specify strategies and perform crawling on specific URLs.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship showing how the arun method is used with strategies to crawl URLs, which aligns with the ground truth's description of an asynchronous crawling method that processes URLs with configurable strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The result of the `arun` method call is stored in a CrawlResult, from which the `extracted_content` attribute is accessed. This implicitly covers how the `extracted_content` is used to hold the processed data from the crawl.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately describes how extracted_content is accessed from the CrawlResult, but omits the crucial semantic filtering and clustering functionality mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides an example that initializes and uses the `AsyncWebCrawler` class within a Python async context manager. This demonstrates its intended interface and operational context.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's use of async context manager, but misses the crucial functionality of customizable extraction strategies and web crawling capabilities that are central to the class's purpose as shown in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the provided code example, `arun()` is called on an instance of `AsyncWebCrawler`, showcasing its role in executing the crawling process with specified strategies.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() and AsyncWebCrawler's basic role in crawling, but misses the crucial aspect of content extraction and filtering functionality that is central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation specifies using `LLMExtractionStrategy` for extracting content, indicating it implements a strategy for content extraction from the web.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy as implementing content extraction, but misses crucial aspects about LLM model integration, instruction-based extraction, and specific capabilities shown in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "\"AsyncWebCrawler\" is explicitly mentioned in the provided documentation snippet as the class being instantiated for asynchronous web crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler but only mentions it superficially as a class for web crawling, missing its core user simulation functionality described in the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method \"arun\" is explicitly mentioned and demonstrated in the code snippet as the primary method for URL crawling in the usage example.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the main crawling method but misses the key relationship with user simulation features that the ground truth emphasizes",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Though not directly mentioned, the `AsyncWebCrawler` uses `AsyncPlaywrightCrawlerStrategy` as a default strategy to perform the crawling actions, such as simulating user interactions.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but it misses the core functionality of user simulation which is a crucial aspect of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` extends `AsyncCrawlerStrategy`, which is a base class providing the interface for any asynchronous crawling strategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class interface, but misses the crucial aspect that it specifically enables user simulation and browser behavior customization features mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation mentions 'Try Cosine for content relevance' indicating the use of CosineStrategy for improving content relevance.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for content relevance, which aligns with the ground truth's explanation of how the class uses cosine similarity for semantic relevance filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The text states 'Use LLM for complex interpretation', referring directly to LLMExtractionStrategy for tasks requiring complex interpretation.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM's role in complex interpretation, while the ground truth describes error handling and retry logic implementation. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The suggestion to 'Start with CSS for structured data' points towards using JsonCssExtractionStrategy for structured data extraction.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements the CSS-based approach recommended in the documentation for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example usage in the documentation involves 'crawler.arun(url, extraction_strategy=strategy)', implying the use of AsyncWebCrawler.arun() method.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method but misses the key relationship between the code and documentation - the error handling patterns and best practices implementation. While it correctly notes the method usage, it fails to capture the deeper alignment with documented error handling and caching practices.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the example, 'data = json.loads(result.extracted_content)' indicates leveraging CrawlResult.extracted_content for accessing the extracted data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that CrawlResult.extracted_content is used to access extracted data, which aligns with the ground truth's explanation of how it stores and handles crawled data",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "Checks for 'not result.success' followed by 'print(f\"Extraction failed: {result.error_message}\")', implying the use of CrawlResult.error_message for error handling.",
    "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how error_message is used in error handling when result.success is False, aligning with the ground truth's explanation of its purpose",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The condition 'if not result.success' directly demonstrates the usage of the CrawlResult.success attribute for determining operation success.",
    "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the success attribute's role in error checking, it misses the broader context of supporting error handling best practices that is core to the ground truth",
      "error_type": "incomplete_context"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet includes an example that explicitly uses the `AsyncWebCrawler` class, showing its instantiation and usage in an async context to crawl a URL.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its async context usage, but misses the core functionality being described - the overlay removal capability which is the main focus of the ground truth.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet makes a direct call to `arun()` as part of the example, indicating its explicit usage for performing crawling operations.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies arun() as being used for crawling operations, it misses the key relationship regarding overlay removal functionality and parameter handling described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, AsyncPlaywrightCrawlerStrategy is the default strategy implementation used by AsyncWebCrawler, as implied by its definition in the AsyncWebCrawler class constructor.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description discusses AsyncPlaywrightCrawlerStrategy being a default strategy of AsyncWebCrawler, while the ground truth describes the implementation of overlay removal functionality using JavaScript. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `arun()` is explicitly invoked in the code example within the documentation snippet. The snippet demonstrates its use for retrieving the `fit_markdown` property from a `CrawlResult` object.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The prediction describes using arun() to get fit_markdown, but misrepresents the code-docs relationship by claiming the function is 'explicitly invoked' in the docs when it's actually just an example usage. The core functionality of markdown extraction is captured though.",
      "error_type": "minor_misrepresentation"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "`fit_markdown` is explicitly accessed and printed in the example provided in the documentation. It represents a property of `CrawlResult` that is retrieved after executing the `arun()` method.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on how fit_markdown is accessed/used in code rather than its core purpose of storing cleaned markdown content, though it correctly identifies it as a property accessed after crawling",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example usage of `arun` showing how to filter content, demonstrating its implicit utilization for controlling content selection.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method implements content filtering functionality as shown in the documentation, even though it doesn't detail all the specific parameters. The core relationship between the code and documentation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned in the snippet, `AsyncWebCrawler.arun()` is fundamentally supported by `AsyncCrawlerStrategy`, which outlines the interface that `AsyncPlaywrightCrawlerStrategy` implements for crawling strategies, underlying the filtering features discussed.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that AsyncCrawlerStrategy defines the interface/methods through which the documented content filtering functionality is implemented, with the predicted description adding some non-contradictory implementation details",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example in the documentation refers to an instance of `crawler`, suggesting the use of `AsyncWebCrawler` class which manages crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct class (AsyncWebCrawler) but only mentions it manages crawling operations, missing the crucial relationship about content filtering functionality described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the `LLMExtractionStrategy` class as part of the example code snippet, demonstrating its use for customizing LLM providers.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy is used for LLM provider customization, but misses the key implementation detail about how it uses the provider and API token parameters to initialize and configure the extraction process",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While not explicitly mentioned in the documentation snippet, `LLMExtractionStrategy` extends `ExtractionStrategy`, thus any instance of `LLMExtractionStrategy` will implicitly involve elements of its base class.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy serving as a foundation for implementing custom LLM providers",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text snippet explicitly mentions the instantiation of the 'CosineStrategy' class with specific parameters such as 'semantic_filter', 'word_count_threshold', 'sim_threshold', etc. The parameters discussed directly relate to the attributes and intended use of the 'CosineStrategy' class within the codebase.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that CosineStrategy class is instantiated with the same configuration parameters shown in the documentation. While the ground truth provides more specific detail about parameter values, the predicted description correctly identifies the key relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "'CosineStrategy' inherits from 'ExtractionStrategy', which is implicitly referenced by the inheritance chain. The documentation outlines parameters related to extraction features, which are foundational in the 'ExtractionStrategy' abstract base class.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, and accurately notes that the parameters in the documentation would be handled through the kwargs mechanism in ExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows how to use the `AsyncWebCrawler` class to handle crawling a protected site. It explicitly mentions instantiating this class with `headless=True`. This is evident from the code line `async with AsyncWebCrawler(headless=True) as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler can handle protected site crawling, but it focuses too narrowly on just the headless mode aspect while missing other crucial functionality like magic mode, overlay removal, and timeout handling that are key parts of the protected site crawling capability.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows the usage of the `arun()` method from the `AsyncWebCrawler` class by calling `await crawler.arun()`. It implies direct usage of this asynchronous method, as shown in `result = await crawler.arun(...)` for crawling the specified URL with given parameters.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic async usage of arun() but fails to capture the crucial aspects of error handling, caching, and configurable parameters that enable the protected site crawling features shown in the documentation",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions 'AsyncWebCrawler' in the example provided, demonstrating its use for crawling with the 'wait_for' parameter.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports the wait_for parameter functionality, even though it doesn't explicitly mention the internal mechanism through **kwargs and crawler_strategy. The core relationship is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet includes an example that calls 'crawler.arun()' within the 'wait_for_parameter_crawl' function. This demonstrates 'arun()' being used as a method to start the web crawling process with specific parameters, including 'wait_for'.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() being used for web crawling with parameters, but misses the key point about how arun() actually implements the wait_for functionality through kwargs and crawler_strategy.crawl()",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example ends with a call to 'crawler.crawler_strategy.kill_session(session_id)', making 'kill_session()' explicitly part of managing the session lifecycle within the demonstrated crawl routine.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted text correctly identifies kill_session as part of session lifecycle management and its role in the crawl routine, which aligns with the ground truth's description of its purpose in cleaning up browser resources after crawling",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The text snippet explicitly specifies the use of 'JsonCssExtractionStrategy' for extracting structured data using a schema. This is illustrated in the 'extraction_strategy' parameter within the code example.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that JsonCssExtractionStrategy is used for schema-based data extraction, which aligns with the ground truth's explanation of how it implements schema-based extraction using BeautifulSoup. While the predicted version is less detailed, it correctly identifies the main purpose and relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly named in the snippet, the use of hooks and certain async functionality implies 'AsyncPlaywrightCrawlerStrategy', given its role in 'AsyncWebCrawler' as a default strategy implementing various async methods.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy and its async functionality, but misses the core focus of the ground truth which is specifically about the wait_for functionality and smart_wait method for handling dynamic content loading",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the provided documentation text as the class being used to perform the web crawling operation, which also includes taking screenshots.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in screenshot functionality but doesn't capture the key implementation details about the base64 format and the actual mechanism of how screenshots are handled through the arun method.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is explicitly used in the code snippet from the documentation, showing how it is called on the `AsyncWebCrawler` instance to perform the crawl operation with screenshot functionality.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for screenshot functionality, but misses the key implementation details about how screenshot data is handled through caching and crawling, and how it's returned in base64 format for saving",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The `screenshot` attribute of the `CrawlResult` class is implicitly used in the method to determine if the screenshot operation was successful and to retrieve the screenshot data.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the screenshot attribute is used to store and handle screenshot data, even though it doesn't explicitly mention base64 encoding.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly mentioned in the usage example within the documentation where it is used to create an instance of a web crawler. This is part of the description for implementing 'Magic Mode'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in the Magic Mode feature, but misses the crucial implementation details about how it actually handles anti-bot protections through crawler_strategy and arun() method parameters.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method arun() is used in the example without being directly explained. It is part of executing the crawling operation with the Magic Mode settings.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as part of the crawling operation but misses the crucial connection to Magic Mode's anti-bot features through the magic parameter in kwargs.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses strategies to perform crawling operations, and AsyncCrawlerStrategy serves as an abstract base class for those strategies. It outlines methods that subclasses like AsyncPlaywrightCrawlerStrategy might implement, important for understanding strategy integration.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawler strategies, but misses the crucial connection to Magic Mode's anti-bot features which is the core purpose according to the ground truth",
      "error_type": "missing_key_purpose"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This is likely the implementation of the strategy used by AsyncWebCrawler when Magic Mode is enabled, showing a concrete subclass fulfilling the expected behavior defined in AsyncCrawlerStrategy.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies this as the AsyncWebCrawler's strategy implementation, it fails to capture that this is specifically about implementing Magic Mode's anti-bot features through various techniques like stealth configurations and behavior simulations.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly mentioned in the documentation text, signifying its key role in supporting session-based crawling through the session_id parameter and related methods, which aligns with the functionality described in managing sessions, executing JavaScript, and more.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship that the AsyncWebCrawler class supports session-based crawling through the session_id parameter, which matches the ground truth's description of implementing session maintenance across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method is utilized to demonstrate the usage of maintaining session state through a session ID, as implicitly suggested by the mention of 'use the same session_id across multiple arun calls' within the document.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the arun method handles session management through session_id parameter to maintain state across requests. The predicted description accurately conveys this main functionality, even if it's less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy provides the abstract methods related to crawling strategies, which AsyncWebCrawler relies on for implementing session-based operations, as is implicitly necessary for its functionality, indicated by the requirement of maintaining dynamic content.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify AsyncCrawlerStrategy as defining the core interface/methods for session-based crawling functionality, with the predicted version accurately capturing the essential relationship even if it's less detailed",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'CosineStrategy' when discussing best practices for clustering strategies, demonstrating how to define parameters such as 'word_count_threshold', 'top_k', and 'sim_threshold'. These attributes are used within the class as shown in the definition of `CosineStrategy`. This demonstrates direct use of this class.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that CosineStrategy implements configurable parameters that align with the documentation's best practices for different content types. The predicted description captures the core relationship between the documentation's recommendations and the class implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `CosineStrategy` is a subclass of `ExtractionStrategy`, implying it inherits and possibly overrides its methods. The documentation's usage examples and explanation of adjusting parameters inherently rely on the abstract structure provided by `ExtractionStrategy`.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship with ExtractionStrategy but misses the crucial aspect of how the class enables configurable parameters through kwargs to support the best practices shown in documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the documentation as part of the code example where it is used to perform web crawling with logging enabled.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's presence in the code example, but misses the core relationship about how it implements RegexChunking functionality through the arun() method for text content splitting",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is explicitly mentioned in the code example as it is called to use the `RegexChunking` strategy and retrieve crawl results.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() uses RegexChunking, but misses the crucial aspect that it validates the chunking strategy and sets it as a default parameter. The relationship is more implementation-focused than just usage.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The `RegexChunking` class is explicitly shown in the code snippet and described as a chunking strategy for splitting text based on a regex pattern.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic purpose of RegexChunking but omits the crucial detail about its iterative pattern application and handling of multiple patterns, which is a significant part of its functionality as shown in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although not mentioned in the snippet, `AsyncWebCrawler` is based on an `AsyncCrawlerStrategy`, indicating an implicit relationship as a strategy interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the crawler to support various strategies like RegexChunking through its abstract crawl methods and configuration options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy serves as a strategy interface, but misses the crucial aspect that it defines core abstract methods for crawling and configuration that enable different strategies like RegexChunking",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "`RegexChunking` implements the `ChunkingStrategy` interface, applying its abstract method for chunk creation. This relationship is inferred from `RegexChunking` usage.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that RegexChunking implements/follows the ChunkingStrategy interface through the abstract method implementation",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation directly mentions using 'crawler.arun' to execute custom JavaScript code during the crawling process, indicating a usage relationship to 'AsyncWebCrawler.arun()'.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for JavaScript execution, but misses the crucial detail that the JS execution happens through kwargs being passed to crawler_strategy.crawl(), instead suggesting a more direct relationship",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The AsyncWebCrawler class uses 'AsyncPlaywrightCrawlerStrategy' as a default crawling strategy. This strategy directly handles the details of executing JavaScript code through the 'arun()' calls seen in the documentation.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JavaScript execution is supported, but incorrectly states it's through 'arun()' calls when the ground truth shows it's actually implemented through the crawl() method with js_code parameter using page.evaluate()",
      "error_type": "method_misattribution"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' inherits from 'AsyncCrawlerStrategy'. This is deduced because the specific implementation of JavaScript handling would be in subclasses like 'AsyncPlaywrightCrawlerStrategy', directly contributing to 'arun()' behavior showcased in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the inheritance relationship aspect but incorrectly specifies 'AsyncPlaywrightCrawlerStrategy' which isn't shown in the code, and focuses on implementation details rather than the abstract class's fundamental role in enabling JavaScript execution as described in the ground truth.",
      "error_type": "incorrect_focus_and_speculation"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the usage of 'AsyncWebCrawler' in examples with proxy configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports proxy configuration through examples, which aligns with the ground truth that the class accepts proxy configuration via its constructor's kwargs parameter and passes it to AsyncPlaywrightCrawlerStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation demonstrates calling the 'arun' method on an instance of 'AsyncWebCrawler' to perform the crawling process.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() method with AsyncWebCrawler but misses the key relationship about proxy configuration support which is the main focus of the documentation",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Though not explicitly mentioned, the 'AsyncWebCrawler' class uses 'AsyncPlaywrightCrawlerStrategy' if a strategy is not provided during construction; hence it's a base strategy for crawling involving proxies.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes the proxy functionality but incorrectly suggests AsyncWebCrawler uses this strategy by default, which isn't shown in the code or documentation. The core proxy implementation aspect is accurate though.",
      "error_type": "unsupported_assumption"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly mentions a CSS Strategy for product listings using a schema. This corresponds to how 'JsonCssExtractionStrategy' likely uses schemas to extract data based on CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy uses schemas with CSS selectors to extract data, which aligns with the e-commerce scraping example in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly makes use of 'LLMExtractionStrategy' for news article extraction, specifying its provider and the use of schema. This matching directly implies the use of 'LLMExtractionStrategy' from the provided artifacts.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship showing LLMExtractionStrategy being used for article extraction with provider and schema parameters, matching the ground truth's explanation of the class implementation and its news article use case.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The snippet describes a use case for topic analysis using the 'CosineStrategy', specifically focusing on semantic filtering and top-k extraction, which aligns with the 'CosineStrategy' for extracting content clusters.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of CosineStrategy for topic analysis using semantic filtering and top-k extraction, which aligns with the ground truth's explanation of content analysis using cosine similarity and semantic filtering for clustering text content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method from the `AsyncWebCrawler` class is explicitly used in the provided documentation snippet as it demonstrates how to handle dynamic web content, such as infinite scroll and form interactions, using JavaScript code injection and waiting parameters.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of the arun() method for handling dynamic content through JavaScript execution and waiting mechanisms, matching the ground truth's explanation of dynamic content handling and form interactions",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The examples in the documentation use `arun()` which is part of `AsyncWebCrawler` that internally makes use of an `AsyncCrawlerStrategy` implementation. `AsyncPlaywrightCrawlerStrategy` is a concrete implementation of `AsyncCrawlerStrategy`, indicating that it is potentially used in these operations to provide JavaScript execution capabilities with Playwright.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as an implementation of AsyncCrawlerStrategy, but misses the core functionality described in the ground truth about how it specifically handles dynamic content loading through smart_wait method and JavaScript execution patterns.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` implements the `AsyncCrawlerStrategy` interface. The documentation indirectly relates to `AsyncCrawlerStrategy` as it dictates the general methods and capabilities that any implementing strategy, like the Playwright one, must fulfill for successful asynchronous crawling and page interaction.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that AsyncCrawlerStrategy is an abstract base class providing foundational methods that enable dynamic content handling and interactions. The predicted description correctly identifies it as an interface that must be implemented for crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly refers to `LLMExtractionStrategy` with different LLM providers for extraction, showing examples of its usage with OpenAI, Hugging Face, and custom headers.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between LLMExtractionStrategy and different LLM providers, mentioning the key providers (OpenAI, Hugging Face, Ollama) that are supported through the class implementation, which aligns with the ground truth's explanation of provider flexibility and authentication handling.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "`LLMExtractionStrategy` is implicitly linked to `ExtractionStrategy` because it extends this abstract class, which is part of its inheritance hierarchy. The connection is necessary to understand how `LLMExtractionStrategy` fits in the overall design hierarchy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy, but misses the key purpose of supporting multiple LLM providers which is central to the ground truth relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides examples of using the `arun()` method of the `AsyncWebCrawler` class to perform crawling operations with specific wait conditions.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling with wait conditions, but misses the key detail that this functionality is implemented through **kwargs parameter accepting 'wait_for' conditions",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` class is likely used within the `AsyncWebCrawler` class as a strategy, leveraging methods like `smart_wait` to implement CSS or JavaScript-based wait conditions specified in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements wait conditions through smart_wait functionality, supporting both CSS and JavaScript-based waiting as documented. The key relationship between the class and its wait condition implementation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly discusses 'JsonCssExtractionStrategy' and provides guidance on its usage, indicating its role as an implementation of CSS-based extraction strategy.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy and its CSS-based extraction role, but misses the crucial connection between the code's BeautifulSoup implementation and how it relates to the documentation's emphasis on selector testing and inspection",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The recommendation to check 'result.success' implies usage of the 'CrawlResult' class, as this is the presumed class containing the 'success' flag, linking it to the checking of a crawl result's success in contextual use.",
    "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between CrawlResult's success flag and error handling, but misses the additional error handling capability provided by the error_message field that's mentioned in the ground truth",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions using the `JsonCssExtractionStrategy` to extract dynamically loaded content according to a defined schema. The text includes an example where this extraction strategy is instantiated and passed a schema dictionary.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that JsonCssExtractionStrategy uses schema-based extraction for dynamic content, which aligns with the ground truth's explanation of structured data extraction using CSS selectors and schema definitions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly uses `AsyncWebCrawler` to perform the web crawling operation. It shows the initialization of this class and employing its `arun` method, which utilizes the extraction strategy and JavaScript code to scrape the webpage.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling and its use of extraction strategy, but misses the crucial aspect of supporting dynamic data extraction through JavaScript execution and waiting conditions, which is a key feature highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text provides a code example where `arun` method of `AsyncWebCrawler` is invoked to crawl a webpage, combining dynamic content loading via JavaScript and a specified extraction strategy. This shows how `arun` facilitates this functionality.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that arun() method supports dynamic content extraction and extraction strategies, matching the ground truth's core point about the method's functionality for dynamic content handling and extraction strategies. Minor omissions of specific parameters do not affect the core relationship description.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as an extraction strategy that uses CSS selectors. This is directly implemented by the class 'JsonCssExtractionStrategy'. The example demonstrates its initialization with a schema, which is a key feature of this class.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies JsonCssExtractionStrategy as using CSS selectors, it misses the crucial implementation detail about using BeautifulSoup's select() method to actually perform the extraction, which is a core part of how the relationship works.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code in the documentation calls the 'arun' method on a 'crawler' object, which is likely an instance of 'AsyncWebCrawler'. This indicates that 'AsyncWebCrawler' is designed to work with an 'extraction_strategy', such as 'JsonCssExtractionStrategy', as shown in the example.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method works with extraction strategies like JsonCssExtractionStrategy. While it's less detailed than the ground truth, it captures the core relationship of how arun integrates with extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly describes the `RegexChunking` class and its functionality, including its purpose, when to use it, and the parameters, specifically the `patterns` parameter used for splitting text.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that RegexChunking uses regular expressions to split text based on patterns. While it doesn't mention implementation details like iterative splitting, it correctly conveys the main functionality and purpose.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' when discussing the schema for data extraction using CSS selectors, highlighting its advantages such as speed, precision, structured output, and lack of external dependencies.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on describing advantages and documentation aspects of JsonCssExtractionStrategy, while the ground truth specifically describes how the code implements the schema structure using BeautifulSoup. While both discuss the strategy, they emphasize different aspects.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned, the 'JsonCssExtractionStrategy' likely extends 'ExtractionStrategy' since it is a common design pattern to have specific strategies like JsonCssExtractionStrategy inherit from a base strategy class.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and JsonCssExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy as a base class for different extraction methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of the `AsyncWebCrawler` class by demonstrating how to instantiate and utilize it with the verbose argument set to `True` for logging purposes.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the verbose parameter enables logging functionality in the AsyncWebCrawler class. While it doesn't list all the specific logging points mentioned in the ground truth, it accurately conveys the core relationship between verbose mode and logging behavior.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example shows the method `arun()` being called on an `AsyncWebCrawler` instance, indicating its operational context within the class.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the code is part of AsyncWebCrawler class but misses the main focus on verbose logging functionality described in the ground truth",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method of the `AsyncWebCrawler` is explicitly mentioned in the documentation snippet as being involved in the configured crawling with anti-detection features. The example includes calling `arun()` with parameters like `simulate_user=True` and `override_navigator=True`.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the arun() method supports anti-bot/detection features through configurable parameters. The predicted text accurately reflects the documented functionality of accepting manual anti-bot options.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is implicitly used in the documentation snippet, as the method `arun()` whose class is used to run the crawler with the specific features mentioned. The class is responsible for providing the structure within which these options are set.",
    "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the connection between AsyncWebCrawler and arun() method, but misses the key point about how the anti-bot options are passed through kwargs to the crawler_strategy's constructor.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` uses `AsyncCrawlerStrategy` as the base strategy for crawling, although not directly visible in the snippet. Its methods can include features like 'simulate_user' and 'override_navigator' that are indirectly referenced to potentially relate to strategy design in crawling options.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy serves as a base strategy for crawling and relates to the anti-bot features, even though it explains it slightly differently than the ground truth. The core relationship between the strategy class and the anti-bot features is captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is explicitly used in the documented Python example code snippet, where it is instantiated and entered asynchronously.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used in the documentation example, but misses the core relationship that the class implements the configuration options through its __init__ method and AsyncPlaywrightCrawlerStrategy",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is implicitly mentioned through its invocation in the example code. It is called on an instance of 'AsyncWebCrawler', allowing a URL to be processed.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as a method of AsyncWebCrawler that processes URLs, but misses the crucial aspect of how it integrates with the configuration parameters described in the documentation",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation explicitly mentions using 'fit_markdown' for articles, demonstrating a usage example with a clear code snippet.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that fit_markdown is intended for articles/blog posts and represents formatted content. The predicted captures the essential relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation provides a specific use case for filtering media by relevance score, explicitly showing how to access the 'media' attribute.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between media filtering and relevance scores shown in the documentation's example, even if it doesn't mention the CrawlResult class name specifically.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation snippet includes code to analyze links, explicitly using the 'links' attribute from 'CrawlResult'.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept of using and filtering links from the CrawlResult, with the predicted focusing on the links attribute and the ground truth elaborating on the specific filtering of internal content links shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The call to 'arun' in the documentation implies the usage of the 'AsyncWebCrawler' class, through which 'arun' is an instance method.",
    "ground_truth_relationship": "The documentation outlines best practices for using the AsyncWebCrawler class's features, specifically showing how to access fit_markdown for articles, filter media by relevance scores, process content links, and customize crawling parameters through the arun() method implementation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncWebCrawler and arun() as a method call, but misses the broader context that the documentation demonstrates multiple features and best practices beyond just the arun() method, including handling media, links, and content processing.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a code snippet showing the invocation of 'arun', establishing a precise usage relationship.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that the documentation shows arun usage, but misses that the documentation actually outlines best practices implemented by arun like content cleaning, media processing, and link handling",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly references the usage of `AsyncWebCrawler` for making requests through an authenticated proxy setup. It is explicitly mentioned in the example code snippet: `async with AsyncWebCrawler(proxy_config=proxy_config)`.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of AsyncWebCrawler accepting and using proxy configuration for authenticated proxy connections, which aligns with the ground truth's explanation of the proxy_config parameter being passed to the crawler strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` of `AsyncWebCrawler` is implicitly used in the documentation via the example: `result = await crawler.arun(url=\"https://example.com\")`. Although not explicitly described, its usage is key in the demonstrated flow.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method's usage in the documentation example but fails to mention its key proxy support functionality shown in the ground truth. While not wrong, it misses this crucial aspect.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within `AsyncWebCrawler`, it defaults to using `AsyncPlaywrightCrawlerStrategy` if no other strategy is provided. This is implied within the context because `AsyncPlaywrightCrawlerStrategy` is the default strategy often used to handle proxy configurations, as seen in the example.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description suggests that AsyncPlaywrightCrawlerStrategy is just a default strategy for handling proxies, while the ground truth specifically explains how it implements authenticated proxy support through ProxySettings configuration. While both mention proxy handling, the prediction misses the core authentication implementation aspect.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The schema described in the documentation is a JSON schema designed for CSS-based extraction, which aligns directly with the `JsonCssExtractionStrategy` class. This class is intended for extracting structured data from HTML using a schema, as described in the text.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses a JSON schema for CSS-based extraction from HTML, which aligns with the ground truth's explanation of how it uses BeautifulSoup's select() method to match CSS selectors defined in the schema.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` class implements a schema-based extraction strategy as described in the text. As it extends `ExtractionStrategy`, this base class forms part of the pathway to understanding the hierarchy and usage of extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the base class for implementing schema-based extraction functionality, which aligns with the ground truth's description of it providing core functionality for schema-based extraction patterns.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document explicitly discusses advanced usage scenarios of the 'JsonCssExtractionStrategy' class, indicating its implementation in handling complex, nested HTML structures.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in handling complex HTML structures, but misses the crucial implementation details about recursive processing and schema-based selectors that are core to how it achieves this functionality",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy class extends the ExtractionStrategy class, making it indirectly related to the advanced usage discussed in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of how ExtractionStrategy provides the foundational architecture and parallel processing capabilities that enable the advanced functionality described in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text snippet explicitly demonstrates usage of the 'CosineStrategy' class, providing configuration for clustering and content filtering.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description acknowledges the CosineStrategy class and mentions clustering/filtering configuration, but misses the crucial aspects of semantic similarity and hierarchical clustering using cosine similarity which are core to how the class functions",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet includes an async function that uses 'AsyncWebCrawler' as part of the content extraction workflow, representing its implicit usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's async functionality and content extraction, but misses crucial aspects like its core role in providing customizable extraction strategies, caching system, and clustering support described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is invoked within the 'extract_pricing_features' async function in the documentation snippet, indicative of its usage in executing the crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage relationship between AsyncWebCrawler.arun and extract_pricing_features, but misses crucial aspects about its role in handling custom clustering configurations and content filtering parameters that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions the 'Cosine Strategy' and describes its functionality in content clustering and extraction based on semantic similarity, which correlates directly with the 'CosineStrategy' class in the code that implements similarity-based clustering.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the Cosine Strategy's purpose for semantic similarity-based clustering, but misses describing the specific 5-step implementation workflow that is crucial to understanding how the strategy actually works, as detailed in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is likely based on the text description illustrating steps such as extraction and content processing, suggesting it builds upon the abstract 'ExtractionStrategy', forming a logical hierarchy in strategy implementation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy extends the abstract ExtractionStrategy class and maintains the core inheritance relationship, even though it doesn't detail all the specific methods and parallel processing capabilities mentioned in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `arun` method is used in `crawler.arun(url=\"https://example.com\", js_code=...)` to execute JavaScript code on a web page. This suggests the usage of an `AsyncWebCrawler` instance (`crawler`) to perform web crawling and execute scripts as described in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler's arun method is used to execute JavaScript code on web pages, which aligns with the ground truth's explanation of JavaScript execution functionality through the arun method accepting js_code parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is explicitly used in the example `result = await crawler.arun(...)`, indicating that it implements the functionality to execute JavaScript on a webpage through the asynchronous crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for executing JavaScript through asynchronous crawling, but fails to mention that it works by accepting js_code parameter through **kwargs and forwarding to crawler_strategy.crawl()",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly mentions the `arun()` method with an example: `result = await crawler.arun(url=\"https://example.com\")`, indicating that this method is used to return a `CrawlResult` object.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() returns a CrawlResult object, but misses explaining the core processing functionality and transformations performed by the method to generate those CrawlResult properties. The ground truth provides a more complete picture of how the method actually works.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation describes that the `arun()` method returns a `CrawlResult` object: \"The `arun()` method returns a `CrawlResult` object with several useful properties.\"",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() returns a CrawlResult object, but fails to mention the crucial relationship that CrawlResult is a class that defines all the properties shown in the documentation example.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The documentation snippet explicitly mentions accessing the `html` attribute of the `CrawlResult` object: `print(result.html)`. This is a direct reference to the `html` property of `CrawlResult`.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the code defines an html string property that contains raw HTML content, matching the ground truth's explanation of the html attribute storing raw HTML content from CrawlResult.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation mentions `cleaned_html`: `print(result.cleaned_html)`, clearly indicating it as a property available in the `CrawlResult` object to access cleaned HTML content.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies cleaned_html as a property of CrawlResult that provides cleaned HTML content, which aligns with the ground truth's description of it being an optional string property containing sanitized HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet directly cites the `markdown` attribute with `print(result.markdown)`, indicating it as a part of the `CrawlResult` data structure.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that markdown is an attribute/property of the CrawlResult class that can be accessed through result.markdown, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation cites `fit_markdown`: `print(result.fit_markdown)`, suggesting this property provides the most relevant markdown content from the `CrawlResult`.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown provides the most relevant markdown content from the CrawlResult, which aligns with the ground truth's explanation of it being an optional string containing the most relevant content in markdown format.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "`print(result.success)` points to the `success` attribute, which indicates whether the crawling operation was successful.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that success is a boolean attribute indicating whether the crawl operation succeeded, which aligns with the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The example `print(result.status_code)` in the snippet denotes a property within `CrawlResult` that provides the HTTP status code of the crawl.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that status_code is a property of CrawlResult that represents the HTTP status code from the web request, which aligns with the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Direct usage example `print(result.media)` references the `media` attribute, used to access media elements discovered during the crawl process.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that media is related to accessing media elements, but misses that the code specifically defines an empty dictionary structure to store these elements. It focuses only on the usage/access aspect rather than the initialization structure.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet's example `print(result.links)` refers to the `links` attribute that holds information about detected links.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that links are stored in a links attribute, but misses the key distinction that it specifically stores both internal and external links from crawling results",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls the 'arun' method of the 'AsyncWebCrawler' class to extract specific content using a CSS selector. This is explicitly shown in the code example.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions CSS selector usage in arun method but incorrectly states it 'directly calls' the arun method of AsyncWebCrawler class, when the code shows this is a method implementation itself. It also misses that the selector is passed through to HTML processing.",
      "error_type": "incorrect_method_context"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun' method is a member of the 'AsyncWebCrawler' class. The class itself is implicitly included through the use of the 'arun' method in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that 'arun' is part of AsyncWebCrawler class, but misses the crucial functional relationship between CSS selectors and content extraction that is central to the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' utilizes the 'AsyncCrawlerStrategy' interface, as the 'arun' method uses one to manage the crawling strategy implicitly required for executing the provided CSS selector extraction example.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection to crawling functionality but incorrectly assumes an 'AsyncWebCrawler' class and 'arun' method that aren't shown in the code. The core idea of CSS selector-based extraction is present, but the implementation details are misattributed.",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Since 'AsyncPlaywrightCrawlerStrategy' is an implementation of 'AsyncCrawlerStrategy', which can be passed as a strategy to 'AsyncWebCrawler', it is an implicit relationship.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the inheritance relationship between AsyncPlaywrightCrawlerStrategy and AsyncCrawlerStrategy, while the ground truth describes the CSS selector functionality implementation within the class.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly mentions `AsyncWebCrawler` in the context of creating an asynchronous web crawler to handle JavaScript execution and LLM extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in handling JavaScript execution and LLM extraction, but misses the crucial implementation details about the arun method's functionality with js_code and wait_for parameters for dynamic content processing",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is called explicitly within the example provided. It demonstrates how to pass JavaScript execution code, wait conditions, and an extraction strategy to crawl a web page.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of arun() being used with JavaScript execution and extraction strategy parameters, which aligns with the ground truth's explanation of how the method handles dynamic content with js_code, wait_for, and extraction parameters.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is instantiated and used as the extraction strategy for summarizing articles in the provided documentation example.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy is used for article summarization, but misses key aspects of it being a general-purpose extraction class that handles various LLM providers, API configuration, and custom instructions beyond just summarization",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet demonstrates using the 'AsyncWebCrawler' class with the 'arun' method to filter content based on domains. Evidence of usage is directly seen in the code example where 'crawler' presumably is an instance of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles domain filtering through the arun method, but fails to mention the specific functionality of excluding domains and social media links, which is a key aspect of the domain-based filtering capability described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly used in the provided code snippet. The documentation shows calling 'arun' with specified parameters (e.g., 'url', 'exclude_domains') to control domain-based filtering.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun is used with parameters, but misses the key point that domain filtering is implemented through **kwargs which gets passed to crawler_strategy.crawl()",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is directly instantiated in the example code, which shows how it's used to initiate a crawling process and execute an asynchronous task.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler through instantiation, but misses crucial functionality around HTML processing, structured data extraction, and the strategy pattern that are central to its purpose as described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` of `AsyncWebCrawler` is invoked in the `extract_openai_fees` function to perform the crawling operation. This is critical for executing the overall task of extracting data from the provided URL.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling operations in the example, but misses significant aspects covered in the ground truth like the LLMExtractionStrategy implementation, caching mechanics, and the complete processing pipeline returning CrawlResult.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The text clearly mentions the use of `LLMExtractionStrategy` to define an extraction strategy within the example, explicitly indicating its role in the context of structured data extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that LLMExtractionStrategy is used for extraction, but misses the crucial aspects of provider-based LLM models and Pydantic schema implementation, which are key components of how the class actually works according to the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation shows usage of an `AsyncWebCrawler` instance for web crawling with anti-detection features enabled through parameters. It directly correlates with the usage pattern involving `arun`.",
    "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between AsyncWebCrawler and anti-detection features through the arun method, but misses the key point that these features are handled by the crawler_strategy rather than directly by AsyncWebCrawler",
      "error_type": "missing_crucial_component"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is explicitly mentioned in the documentation snippet, as it is used to perform a web crawl with anti-detection features.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method but fails to mention how it implements anti-detection features through keyword arguments, which is a crucial aspect of the relationship between the code and documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The usage of `simulate_user`, `override_navigator`, and `magic` features directly corresponds to functionalities typically implemented within a crawling strategy like `AsyncPlaywrightCrawlerStrategy`, which `AsyncWebCrawler` likely adopts as its default strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The prediction correctly identifies that anti-detection features are implemented in AsyncPlaywrightCrawlerStrategy, but incorrectly suggests this is through adoption by AsyncWebCrawler rather than direct implementation through context.add_init_script() and JavaScript injection.",
      "error_type": "implementation_mechanism_mismatch"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncCrawlerStrategy` is likely the base class interface from which `AsyncWebCrawler` derives its crawling strategies including anti-detection functionality as described in the document.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as the base interface class that enables anti-detection functionality, which aligns with the ground truth's description of it defining the interface for implementing stealth features.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The text explicitly mentions `LLMExtractionStrategy` as a component used for different purposes with the AsyncWebCrawler, indicating it is part of the extraction process using Language Models.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that LLMExtractionStrategy is used with AsyncWebCrawler for extraction using Language Models, which aligns with the ground truth's description of implementing asynchronous web content extraction using LLMs",
      "error_type": "none"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned as providing asynchronous extraction capabilities using Language Models, hence implementing the functionality described in the document.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's LLM capabilities, but misses the crucial detail that this is implemented through an extraction_strategy parameter rather than being a direct feature of the class. The core functionality is present but the implementation detail is important for understanding how it works.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned as it is instantiated in the example snippet to perform session-based crawling operations.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description acknowledges the AsyncWebCrawler class is used for session-based crawling but fails to mention key aspects like the core methods (__aenter__, arun) and functionality for handling persistent browser sessions with user interactions that are highlighted in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is used in the example to execute multiple crawls with different options like session ID and JavaScript execution for dynamic content loading.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of the arun method being used for crawling with session-based features and dynamic content handling, which aligns with the ground truth's description of session-based crawling through session_id parameter",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session` method is explicitly used at the end of the example to terminate the session, highlighting its role in session management within the crawler strategy.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies kill_session's role in session termination but omits the crucial implementation details about closing both page and context objects and memory cleanup that are central to the ground truth's explanation.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The `extracted_content` attribute of `CrawlResult` is used implicitly to count and print the number of content items extracted by the crawler.",
    "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that extracted_content stores the HTML content from CSS-selected elements and is used for counting items. The predicted version accurately describes its usage in the example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of the 'CosineStrategy' in the use cases for 'Article Content Extraction', 'Product Review Analysis', and 'Technical Documentation', demonstrating how to configure this strategy for different content types.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy being configurable for different use cases through parameter adjustment, matching the ground truth's explanation of how the strategy can be configured for article content, reviews, and documentation via semantic filters and thresholds.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The use case examples in the documentation demonstrate calling 'arun' on a 'crawler' instance, suggesting the use of an 'AsyncWebCrawler' instance executing 'arun' with a 'CosineStrategy'. Though not named directly, it aligns with 'arun' taking an extraction strategy as a parameter.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() accepts extraction strategy parameters and is called on a crawler instance, which aligns with the ground truth's explanation of how arun() implements the documented use cases through flexible extraction strategy parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example in the provided text snippet uses the `AsyncWebCrawler` class as the context manager for crawling operations. This shows how it serves as the main entry point for initiating crawl operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as a core class for crawling, but misses its crucial multi-format extraction capabilities and the ability to use different extraction strategies that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is called multiple times in the documentation snippet to execute crawling with different extraction strategies and parameters. This indicates its role as the primary execution method within `AsyncWebCrawler`.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that arun() is used with different extraction strategies to handle various content extraction scenarios. The predicted description correctly identifies arun as the primary execution method that handles different parameters/strategies, which aligns with the ground truth's explanation of its varied use cases.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly mentions the `LLMExtractionStrategy` being instantiated as an extraction strategy for one of the `arun` method calls, demonstrating how it is intended to be used within the workflow.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of LLMExtractionStrategy as an extraction strategy, but misses crucial aspects about its functionality with custom providers, schemas, and structured data extraction using language models that are mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "Similarly, `JsonCssExtractionStrategy` is explicitly mentioned as part of another `arun` call, illustrating its role in pattern extraction within the document workflow.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in pattern extraction, but misses the crucial technical detail about using CSS selectors to extract structured data from HTML elements, which is a core aspect of how it works.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet suggests that the result's `media` attribute is accessed for returning media content, indicating its role as part of the `CrawlResult` structure obtained from the `arun` calls.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that media content is stored in a media property that gets returned as part of the crawl_content function's output",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The `extracted_content` attribute from the `pattern_result` and `llm_result` is accessed and parsed using `json.loads`, showing its importance for fetching structured data.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that extracted_content stores crawled data as a string that gets parsed as JSON, specifically in the context of LLM and pattern-based extraction strategies",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet accesses the `fit_markdown` attribute of the `result` object, showing its role in storing and returning the main content structured format.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is used to store and access the main extracted content, with the ground truth providing additional context about it being part of CrawlResult and webpage content specifically.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides an example of how to use the 'AsyncWebCrawler' class by invoking its 'arun' method to extract metadata.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler for metadata extraction but misses the core implementation detail that metadata processing happens in aprocess_html and gets stored in CrawlResult. It oversimplifies the relationship to just method invocation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly demonstrated in the documentation snippet for executing a web crawl, which returns a 'CrawlResult' that includes metadata extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun performs a web crawl and returns CrawlResult, but misses the crucial point about metadata extraction being accessible through the .metadata property",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation snippet implicitly relies on 'CrawlResult' as it demonstrates retrieving metadata, which is a part of 'CrawlResult'.",
    "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation demonstrates metadata retrieval functionality implemented through CrawlResult. While less detailed than the ground truth, it captures the core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "The 'metadata' attribute of 'CrawlResult' is explicitly accessed in the documentation to read the metadata properties like title, description, etc.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately describes that metadata is accessed as an attribute of CrawlResult to read various metadata properties, which aligns with the ground truth's explanation of metadata being stored as a dictionary with those properties",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly demonstrates the usage of the 'AsyncWebCrawler' class to configure the browser type for crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class handles browser configuration/type selection, even though it doesn't detail the specific implementation through crawler_strategy. The core relationship between the class and browser configuration functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class defaults to using 'AsyncPlaywrightCrawlerStrategy' if no strategy is provided. This is implied in the setup of the crawler, where different browser types ('chromium', 'firefox', 'webkit') are utilized.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that different browser types are supported, but incorrectly attributes this to AsyncWebCrawler rather than AsyncPlaywrightCrawlerStrategy which actually implements the browser selection logic.",
      "error_type": "wrong_class_attribution"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class implements the 'AsyncCrawlerStrategy'. The document indirectly refers to interfaces that manage various browser types.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that the code relates to browser implementation but misses that this is an abstract base class defining required interface methods. It incorrectly suggests there's a concrete 'AsyncPlaywrightCrawlerStrategy' class when the code shows an ABC.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly uses the 'arun' method of 'AsyncWebCrawler' in a try block to execute a crawling operation for a URL, thus directly referencing this code artifact.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct arun method but focuses only on its existence and try-block usage, missing the core relationship about comprehensive error handling and CrawlResult return states that the ground truth emphasizes",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The text accesses 'result.extracted_content' after calling 'arun()', showing explicit use of this attribute to process extracted content from the result of crawling.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the result after arun(), but misses the key aspect that it's an optional field specifically designed for storing semantically-matched content using strategies like Cosine matching",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet checks 'result.error_message' when extraction fails, explicitly using this attribute to retrieve error details from a 'CrawlResult' instance.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that error_message is an attribute accessed via result.error_message to get error details when extraction fails, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text specifically discusses 'Cosine Strategy', explaining scenarios where this extraction strategy is effective, and how it works with semantic content understanding, indicating a code strategy implemented as a class.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of CosineStrategy as a semantic-based extraction strategy that handles content understanding, which aligns with the ground truth's explanation of its implementation using cosine similarity and hierarchical clustering for semantic-based content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example code uses 'crawler' implying that 'AsyncWebCrawler' (a primary class for crawling tasks) implements methods like 'arun()', thereby supporting the text's context.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as a primary crawling class with arun() method, but misses the core focus on error handling implementation that the ground truth emphasizes. The error handling aspect is a crucial part of the relationship being described.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet begins with 'async with AsyncWebCrawler() as crawler', which explicitly mentions the usage of the AsyncWebCrawler class for handling a dynamic page.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler is used with async context management, but misses crucial functionality around handling dynamic content, session management, and JavaScript execution capabilities that are central to the class's purpose.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of AsyncWebCrawler is called multiple times in sequence to load the initial page and additional content, explicitly showing how to interact with dynamic content.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for multiple sequential calls, but misses crucial aspects about parameter handling (JS execution, wait conditions, session management) that enable the complex interactions described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler relies on AsyncCrawlerStrategy, typically realized by AsyncPlaywrightCrawlerStrategy, for executing the crawling process under the hood.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but misses the key implementation details about handling dynamic content, session management, and JavaScript execution that are central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The statement 'await crawler.crawler_strategy.kill_session(session_id)' explicitly calls the kill_session method of AsyncPlaywrightCrawlerStrategy, illustrating session cleanup.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The prediction correctly identifies that kill_session is used for session cleanup, but misses the crucial aspect of resource management (closing page/context objects) and preventing memory leaks in multi-page crawling",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "In 'print(f... len(result.cleaned_html))', the documentation snippet shows the use of the cleaned_html attribute of CrawlResult to process HTML content between loads, demonstrating an implicit detail.",
    "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that cleaned_html is used to process content, but misses the key point that it specifically stores the processed HTML content after each load. The 'implicit detail' characterization understates its core storage role.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows an example using 'await crawler.arun(url=\"https://example.com\")', indicating usage of the arun method from the AsyncWebCrawler class.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the async usage of arun(), while missing the core relationship about error handling and CrawlResult status checking shown in the documentation. However, it does correctly identify that this is about the arun method.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation snippet checks the 'success' attribute from the result, showing direct usage of the CrawlResult.success attribute to determine if the crawl was successful.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core purpose of the 'success' boolean being used for error handling and checking crawl operation status. The predicted description accurately reflects how the success attribute is used, even if it doesn't elaborate on the error handling purpose as much as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet includes 'print(f\"Crawl failed: {result.error_message}\")', indicating explicit usage of the CrawlResult.error_message attribute to retrieve the error message from a failed crawl.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the same core relationship - that error_message stores and displays failure messages when crawls are unsuccessful. The predicted description accurately describes how the error_message is used in the error handling example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The snippet uses 'print(f\"Status code: {result.status_code}\")', showing implicit usage of the CrawlResult.status_code attribute to print the HTTP status code received during the crawl.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that status_code is used to show HTTP status codes from crawl results, which aligns with the ground truth's explanation of using status_code for crawl error diagnosis.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly mentioned in the documentation snippet within the context of using custom headers. The snippet demonstrates initializing an AsyncWebCrawler instance with specific headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports custom headers through initialization, as shown in the documentation example. While it doesn't detail the internal mechanism (kwargs passing to strategy), it captures the core relationship between the class and header functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method is explicitly mentioned in the documentation snippet as it shows the usage example where the method is called on the AsyncWebCrawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the presence of the arun method but fails to capture the key relationship about how it handles custom headers through the crawler_strategy component",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The AsyncPlaywrightCrawlerStrategy is implicitly used as it is the default strategy instantiated by AsyncWebCrawler if no other strategy is provided, linking it to the usage pattern illustrated in the document.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncPlaywrightCrawlerStrategy being implicitly used as a default strategy, while the ground truth describes its specific implementation of custom header functionality. These are completely different aspects of the class's functionality.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation presents an example using the 'arun' method of 'AsyncWebCrawler' to perform a crawling operation on a URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() performs crawling operations, but misses the crucial link analysis aspect and the CrawlResult's role in categorizing links that is central to the documented functionality",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "Example code in the documentation accesses the 'links' attribute of 'CrawlResult' to categorize and analyze different types of links.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that links are categorized and accessible through the CrawlResult's links attribute, even though it doesn't detail the dictionary structure and all link properties",
      "error_type": "none"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation explicitly discusses the class `SlidingWindowChunking` and its approach to using a sliding window for chunking text, which matches the `SlidingWindowChunking` class definition found in the source code.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - both describe a sliding window approach for text chunking that creates overlapping chunks. While it's less detailed about the implementation specifics, it correctly identifies the main functionality and purpose.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions an instance of `crawler`, which is configured and executed via the `AsyncWebCrawler` class as indicated by the function call `crawler.arun()`. This demonstrates usage of the `AsyncWebCrawler` class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its arun() method, but misses the core functionality being discussed - the HTML-to-text customization capability. It focuses on class usage rather than the specific feature described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `arun()` is explicitly shown in the documentation as being called with specific parameters like `html2text`. This demonstrates concrete implementation of how to utilize the `arun` function in context.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() function usage but misses the key point about html2text customization options being passed as kwargs for content conversion, which is the main focus of the documentation.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method of the 'AsyncWebCrawler' class is explicitly mentioned in the example code snippets for handling dynamic content and lazy-loaded images. It is used to perform crawling with added conditions for dynamic content handling.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and handling dynamic content, but misses the crucial aspect that it implements this through kwargs parameters for custom JavaScript and wait conditions",
      "error_type": "key_mechanism_omitted"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class implements functionalities used by 'AsyncWebCrawler' through inheritance, providing necessary methods like async crawling and handling of browser activities.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as a class implementation, but focuses on inheritance structure rather than its core dynamic content handling functionality described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncCrawlerStrategy' serves as an abstraction that 'AsyncPlaywrightCrawlerStrategy' directly extends, implementing methods for crawling and browser interactions.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class, but focuses only on inheritance structure while missing the core purpose of enabling dynamic content handling capabilities described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation illustrates handling dynamic content directly using 'AsyncWebCrawler', implicating its key role as the controller or interface for performing web crawls using a specified strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the controller for web crawls, but misses the crucial aspect of how it specifically handles dynamic content through parameters like wait_for, js_code, and delay_before_return_html",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is explicitly used in the example code provided in the text where an asynchronous crawling session is initiated with a specified extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class is used in the example code for asynchronous web crawling with an extraction strategy, which aligns with the ground truth's explanation of how it implements the cryptocurrency price extraction functionality via arun() method and JsonCssExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' of 'AsyncWebCrawler' is explicitly called in the example as part of the crawling process to extract data from a given URL using the specified strategy.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() is called as part of the crawling process to extract data from a URL using a specified strategy, which aligns with the ground truth's description of it implementing the core crawling functionality demonstrated in the example",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' class is explicitly created and used in the example to define the extraction strategy for the web crawling process, facilitated with a specified schema for structured output.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core idea that JsonCssExtractionStrategy uses a schema for extraction, but misses the specific purpose of extracting cryptocurrency data and the key components of the schema structure (baseSelector and fields for crypto name, symbol, price)",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends 'ExtractionStrategy', making it an implicit part of defining the extraction logic in 'arun()', even though it's not mentioned in the snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy, but misses explaining the key purpose of the base class in defining abstract methods that concrete strategies implement. It focuses only on the inheritance relationship without capturing the functionality aspect.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation directly imports and uses the 'AsyncWebCrawler' class in the provided Python code snippet. The snippet shows how to instantiate and use this class to perform crawling operations.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the AsyncWebCrawler class is imported and used for web crawling operations, with the basic usage pattern shown in the documentation matching the implemented class functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the code example, the 'arun' method of the 'AsyncWebCrawler' class is used to initiate the crawling process. The method is invoked on the 'crawler' instance to perform web crawling and return results.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea that arun() is used for web crawling, but misses crucial functionality around content extraction, caching, and return types that are core to understanding the relationship between the code and documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation code snippet accesses the 'markdown' attribute of the result returned by 'arun'. This indicates that 'arun' returns an object with a 'markdown' attribute, accessed for displaying content in markdown format.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the markdown attribute is accessible from the result object returned by arun() and is used to display content in markdown format",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet provides an example where 'AsyncWebCrawler' is used to manage session state across multiple requests by using 'session_id'. The use of 'async with AsyncWebCrawler() as crawler:' sets up an asynchronous operation context, indicating the class's usage as a context manager.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of session management through session_id in AsyncWebCrawler, including its usage as a context manager and the ability to maintain state across multiple requests. While it may not detail every implementation aspect, it accurately represents the main relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The text explicitly calls 'await crawler.crawler_strategy.kill_session(session_id)' to clean up a session by its identifier, indicating direct usage of the 'kill_session' method from 'AsyncPlaywrightCrawlerStrategy'.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct high-level usage of kill_session for cleanup but misses the key implementation detail that it closes both page and context objects and removes them from the sessions dictionary.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is directly called in the text snippet ('result1 = await crawler.arun(...)'), which sends requests with a 'session_id'. This shows how 'arun' is utilized to perform crawler operations within a session management context.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted text correctly captures the core relationship of how arun() handles session management by accepting and using session_id for requests, even though it doesn't explicitly mention the internal assignment to crawl_result.session_id",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions the use of the 'AsyncWebCrawler' class when illustrating how to set a custom user agent and custom headers. The class serves as the main interface for initiating the asynchronous crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class handles identity management through custom user agents and headers, matching the core functionality described in the ground truth. While it's less detailed about the specific implementation through arun method, it captures the main relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly used in the documentation examples, showing how to execute the crawling operation with the specified user agent or headers. This method likely performs the core functionality of fetching web content.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the core method for crawling, but misses the key relationship around identity management through user_agent and headers customization which is the main focus of the ground truth",
      "error_type": "key_aspect_missing"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, the AsyncWebCrawler instantiates AsyncPlaywrightCrawlerStrategy by default if no strategy is provided. This strategy facilitates the customization of user agents and headers during the crawl operation, as depicted in the documentation examples.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core idea that the class handles identity management through custom user agents and headers. The predicted description correctly identifies the functionality even though it approaches it from a slightly different angle (mentioning AsyncWebCrawler's default use of the strategy).",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncPlaywrightCrawlerStrategy' extends 'AsyncCrawlerStrategy'. This base class defines the abstract interface for strategies, explaining the behavior relating to managing identity that the documentation exemplifies. Although 'AsyncCrawlerStrategy' is not directly mentioned, it represents the foundation for the used strategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy defines the interface for crawler identity management functionality. The predicted description accurately recognizes it as a base class defining the abstract interface that enables the documented behavior.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation directly mentions 'JsonCssExtractionStrategy' as a tool for web scraping and data analysis tasks, which relates to its implementation in code for extracting highly structured data from complex web pages.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies JsonCssExtractionStrategy's role in data extraction, but misses the key focus of the ground truth about how the code specifically implements the documented tips through schema-based gradual implementation",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly mentioned in the usage examples in the documentation snippet to invoke different extraction strategies.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies 'arun' usage but misses the key point about it implementing the combining strategies pattern through the extraction_strategy parameter",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation example uses 'css_strategy', which likely corresponds to the `JsonCssExtractionStrategy` class, implementing the CSS extraction strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as implementing CSS extraction, but misses the key point about it being one part of a combinable strategy system",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet uses 'llm_strategy', which likely refers to the `LLMExtractionStrategy` class for semantic analysis.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the LLMExtractionStrategy is used for semantic analysis, but misses the crucial aspect that it's a complete implementation enabling combined crawling strategies with specific methods (extract() and run()) for processing content after CSS extraction.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation uses the `arun` method of a `crawler` instance to demonstrate accessing different output formats. It explicitly mentions using `await crawler.arun(url=\"https://example.com\")` to perform a crawl operation.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling with different output formats, but misses the key point that arun() actually implements the core processing that generates these formats through content sanitization and processing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The example in the documentation showcases accessing the `html` attribute from the result of the `arun` method. It explicitly mentions `result.html` as a way to get the raw HTML content.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that 'html' is used to access raw HTML content, which aligns with the ground truth's description of 'html' being a string property for storing raw HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation explicitly accesses `result.cleaned_html` to demonstrate retrieving sanitized HTML, indicating the use of the `cleaned_html` attribute of `CrawlResult`.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that cleaned_html is accessed via result.cleaned_html and provides sanitized HTML content, which aligns with the ground truth's explanation of it being a sanitized HTML version of the crawled webpage.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The text accesses `result.markdown` to illustrate retrieval of content in markdown format, explicitly tying `markdown` to the `CrawlResult` class.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that markdown is a field/property accessed via result.markdown that provides content in markdown format from the HTML",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation showcases retrieving the most relevant content in markdown format via `result.fit_markdown`, explicitly pointing to the `fit_markdown` attribute.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown returns the most relevant content in markdown format, which aligns with the ground truth's explanation of the property's purpose",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly calls `crawler.arun()` twice to execute crawling and extraction strategies using `JsonCssExtractionStrategy` and `LLMExtractionStrategy`.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used with different extraction strategies, but it oversimplifies by focusing only on the example usage rather than describing the core functionality and purpose of the method itself",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet in the documentation explicitly mentions `JsonCssExtractionStrategy` in defining a structured extraction strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used in the documentation example, but misses explaining its core functionality of using CSS selectors with schema structure for data extraction",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly uses `LLMExtractionStrategy` to form an extraction strategy that provides semantic analysis of the article.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as a semantic analysis tool, which aligns with its implementation shown in the documentation example",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not explicitly named in the example, the `AsyncWebCrawler` class is implicitly used as `crawler` is instantiated from it.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class being used, but fails to mention its core functionality for both structured and LLM-based extraction capabilities, which is a crucial aspect highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly instantiated in the example given in the documentation snippet. The example shows how to use this class to perform asynchronous web crawling by calling the class constructor.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler class instantiation but misses the crucial async context manager functionality that enables the 'async with' syntax shown in the documentation",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` is implicitly used in the example provided. The snippet shows using the `.arun()` method to run the crawler and fetch results asynchronously.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the usage of arun() for asynchronous crawling, but misses crucial aspects like content extraction, error handling, and caching that are core to the implementation shown in the code and mentioned in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly mentions the `arun()` method in its usage example, indicating how to invoke it with certain parameters like `word_count_threshold`, `exclude_external_links`, `remove_overlay_elements`, and `process_iframes`.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation, noting that the arun() method accepts configurable parameters as shown in the documentation example. While it doesn't mention all implementation details like kwargs handling, it correctly identifies the main relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` class, to which the `arun()` method belongs, likely uses `AsyncPlaywrightCrawlerStrategy` for its execution as it's the default strategy set within the `AsyncWebCrawler` class constructor. The specific parameters such as `remove_overlay_elements` and `process_iframes` are handled by the crawler strategy implementation, as observed in the `AsyncPlaywrightCrawlerStrategy` class methods.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements the documented crawling options through its implementation, including the handling of specific parameters like remove_overlay_elements and process_iframes. While the predicted version mentions it through AsyncWebCrawler's perspective, it still captures the core relationship of the crawler strategy implementing these features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation presents an example using the 'AsyncWebCrawler' class to perform a web crawl and extract product data. It demonstrates initializing this class and using it within an async context manager to execute a web crawling task.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of AsyncWebCrawler being used for web crawling, but it misses crucial aspects mentioned in the ground truth - specifically the caching functionality and support for JsonCssExtractionStrategy for schema extraction, which are core features.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet includes 'JsonCssExtractionStrategy' for specifying an extraction strategy while using 'AsyncWebCrawler'. It shows how to create an instance of this class with a schema and pass it as an argument to 'AsyncWebCrawler.arun()'.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as working with AsyncWebCrawler but misses the core functionality of schema-based HTML parsing and data extraction using BeautifulSoup that is central to the class's purpose",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' is invoked on the 'AsyncWebCrawler' instance to carry out the crawling of the provided URL. This text describes invoking 'arun()' with parameters for URL and extraction strategy, showcasing its role in performing the crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic role of arun() for crawling URLs but misses crucial aspects like caching, async functionality, and structured data extraction that are key to the documented workflow",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The result of the crawl executed by 'arun()' is stored in 'CrawlResult', specifically using 'extracted_content' to capture structured data output. The text documents processing this result into JSON, highlighting 'extracted_content' as a key output field.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies extracted_content as storing the crawl output, but misses the crucial detail that it's stored as a raw string requiring JSON parsing to access structured data.",
      "error_type": "omitted_key_detail"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example of using the 'arun' method with parameters 'url', 'page_timeout', and 'delay_before_return_html'.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method and some parameters, but misses the key relationship that these timing controls are implemented through **kwargs and passed to crawler_strategy.crawl()",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun' method is part of the 'AsyncWebCrawler' class, implicitly referenced in the context of method execution.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between the arun method and AsyncWebCrawler class, but misses the crucial timing control functionality (timeouts and delays) that is the main focus of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly describes nested and list structures for extracting data from HTML, which aligns directly with the schema-driven approach used by JsonCssExtractionStrategy. This class implements extraction based on CSS selectors and nested structures as described.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the JsonCssExtractionStrategy class and its schema-driven approach using CSS selectors and nested structures. Both descriptions convey the same fundamental functionality of processing nested JSON schemas with CSS selectors for data extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly instructs the user to create an instance of `AsyncWebCrawler` using Python\u2019s async context manager. This indicates the use and role of this artifact in setting up an asynchronous web crawler.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship where AsyncWebCrawler is used with async context managers, matching the code's implementation of __aenter__ and __aexit__ methods that enable this functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation explicitly names the `FixedLengthWordChunking` class and describes its purpose of splitting text into chunks based on a fixed number of words, matching the description provided in the `content` of the class artifact.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the class and its general purpose correctly, but omits the crucial implementation detail about how it processes words into chunks using list slicing and rejoining with spaces",
      "error_type": "incomplete_implementation_details"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "While the documentation snippet does not directly mention `ChunkingStrategy`, `FixedLengthWordChunking` is part of its extension. The latter class derives from `ChunkingStrategy`, suggesting the implementation of the abstract method `chunk` as stated in the base class.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that ChunkingStrategy is the base/abstract class that FixedLengthWordChunking implements/extends to provide text chunking functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly uses the `LLMExtractionStrategy` class to define an extraction strategy leveraging a large language model for content extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy uses LLMs for content extraction, but misses crucial details about its structured nature and schema-based parsing functionality that are core to the ground truth explanation",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is used in the example code to run the crawling process with the defined `LLMExtractionStrategy`, although it is not named directly in the snippet.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used with LLMExtractionStrategy, but misses explaining that it specifically implements structured content selection through its arun() method and how it handles extraction strategies",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method of the `AsyncWebCrawler` class is demonstrated in the snippet to kick off the asynchronous crawling process with the defined strategy.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an asynchronous crawling method but misses the crucial aspect of LLM-based content extraction functionality that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` class, used in the snippet, is a subclass of `ExtractionStrategy` as part of setting up the content extraction functionality.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy as a subclass of ExtractionStrategy, but misses the crucial aspect that ExtractionStrategy is an abstract base class that defines the core interface and foundation for content extraction.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides a code example using the `AsyncWebCrawler` class directly. This class is used to instantiate a crawler object in the `main()` function and shows how it's intended to be used for extracting and summarizing key points from a URL.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class usage but fails to mention its error handling capabilities which is the main focus of the ground truth documentation",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` class is instantiated in the example provided in the documentation as part of setting up the extraction strategy. The snippet illustrates creating an instance of `LLMExtractionStrategy` with specific parameters.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the LLMExtractionStrategy class but focuses only on instantiation, missing the core functionality of error handling and parallel processing that is central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The provided code snippet in the documentation includes a call to the `arun` method of the `AsyncWebCrawler` class. This method is used within the `extract_with_retry` function to perform the content extraction operation, demonstrating its essential role in executing the crawling strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes arun() is used for crawling/extraction but misses the core focus on error handling that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly uses the 'AsyncWebCrawler' class to initiate a web crawler in the example provided. Evidence: 'async with AsyncWebCrawler(...) as crawler:'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that the AsyncWebCrawler class is used for web crawling, but fails to mention the extensive configuration capabilities and features that are central to the ground truth relationship. The example usage is mentioned but the full scope of functionality is missing.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' of the 'AsyncWebCrawler' class is explicitly called in the example to execute a crawl. Evidence: 'result = await crawler.arun(...)'.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method being used but misses the crucial aspect of how it processes various configurations, error handling, and caching which is the main focus of the ground truth description.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The example implies return of results as an instance of 'CrawlResult'. This is evident as properties like 'result.markdown', 'result.screenshot', and 'result.success' are accessed, which are attributes of the 'CrawlResult' class.",
    "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is used as the return type containing fields like markdown, screenshot, and success that are accessed in the example code. This aligns with the ground truth's explanation of CrawlResult as the structured response object.",
      "error_type": null
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly mentions 'arun' in 'result = await crawler.arun'. This indicates usage of the 'AsyncWebCrawler.arun()' method, as it aligns with the method's documented call signature and functionality of processing iframe content.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() method usage but misses explaining how iframe processing is actually implemented through kwargs parameters, instead only describing the method call signature",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' constructor defaults to using 'AsyncPlaywrightCrawlerStrategy' when no other strategy is provided, making it an implicit but crucial part of handling iframe content, as indicated by 'crawler_strategy: Optional[AsyncCrawlerStrategy] = None'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the incorrect aspect - it incorrectly discusses the AsyncWebCrawler constructor's default strategy. The ground truth describes how AsyncPlaywrightCrawlerStrategy actually handles iframe processing through its specific process_iframes method.",
      "error_type": "wrong_focus_and_relationship"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly mentions using the `arun` method on a `crawler` object, which aligns with the `AsyncWebCrawler` class's functionality as it implements crawling strategies, including handling external and social media links.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and crawling functionality, but misses the key focus on link filtering capabilities described in the ground truth. It doesn't mention the specific link filtering parameters that are central to the documented functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is specifically used in the example provided in the text snippet, demonstrating its role in executing the specified crawling functionalities like link filtering based on exclusions.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's role in executing crawling functionalities, but incorrectly suggests it's only used in the example shown rather than acknowledging its general capability to accept filtering parameters through kwargs",
      "error_type": "scope_limitation"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet includes a usage example that directly calls the 'arun' method on an instance of 'AsyncWebCrawler'. This directly connects 'AsyncWebCrawler' to the functionality described in cleaning content.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler implements content cleaning through the arun method, but misses the key role of WebScrappingStrategy in implementing the actual cleaning functionality described in the documentation",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is called in the example to perform crawling and content cleaning, as explicitly shown by its invocation with parameters like word_count_threshold and excluded_tags.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the arun() method and content cleaning functionality, noting how it's used with parameters to control cleaning behavior, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The result of the 'arun' method contains 'cleaned_html', which the documentation describes as the output for clean content.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that cleaned_html contains the cleaned content output, which aligns with the ground truth's core meaning. While it omits details about the specific cleaning steps, the fundamental relationship is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example in the document uses 'result.markdown' to represent clean markdown content, indicating its role in the data returned by 'arun'.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies markdown as part of the result output but misses the key point about it being a cleaned, text-based version after noise removal",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a code example of using `crawler.arun()` to execute a crawl operation, with specific parameters to remove certain HTML tags and attributes, which reflects the public interface of the method.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for crawling with configurable parameters, but misses the core HTML cleaning/sanitization functionality that is central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` class utilizes a `crawler_strategy` parameter, which can default to `AsyncPlaywrightCrawlerStrategy` implementing the `AsyncCrawlerStrategy` interface. The method `arun()` invokes crawl operations based on strategies defined in `AsyncCrawlerStrategy`.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface for web crawlers that enable HTML cleaning through required methods like crawl() which processes URLs and returns cleaned responses as documented in the example usage.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an interface for crawling functionality, but incorrectly specifies implementation details about AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy that aren't shown in the code or documentation. The core interface for HTML cleaning is mentioned in the ground truth but not the prediction.",
      "error_type": "added_unsupported_details"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly mentioned, `AsyncPlaywrightCrawlerStrategy` is the specific implementation of the `AsyncCrawlerStrategy` that can handle the actual roaming and content exclusion, thereby applying crawling and cleaning procedures discussed in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML cleaning by removing unwanted elements through methods like remove_overlay_elements() and processes iframes while respecting the excluded_tags parameter during page content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as implementing AsyncCrawlerStrategy and handling crawling operations, but misses the key focus on HTML cleaning and element removal functionality emphasized in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The `cleaned_html` attribute is directly referenced in the example output (`result.cleaned_html`) section of the documentation. It represents the cleaned version of HTML performed by the AsyncCrawler workflow.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that cleaned_html represents sanitized HTML content and is accessible as a result attribute, matching the ground truth's core meaning.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation specifically mentions using 'await crawler.crawler_strategy.kill_session(session_id);' in Resource Management to clean up sessions, directly referencing the method in the code.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify kill_session() as a method for cleaning up/managing session resources, though they describe it at different levels of detail",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method is shown in the State Management example, where it runs session-based crawling with various operations like login and page authentication.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used for session-based crawling operations across multiple requests, which aligns with the ground truth's explanation that it enables stateful crawling by storing session_id.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "As 'AsyncPlaywrightCrawlerStrategy' is the default strategy used in 'AsyncWebCrawler', it's implicitly involved whenever 'arun()' is invoked in session management examples, supporting crawling operations.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncPlaywrightCrawlerStrategy's role in crawling operations but oversimplifies the session management capabilities and misses the key implementation details about the sessions dictionary and kill_session method that are core to the functionality described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncPlaywrightCrawlerStrategy' is a concrete implementation of 'AsyncCrawlerStrategy', thus indirectly related whenever 'AsyncPlaywrightCrawlerStrategy' is used.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class serves as the foundation for implementing the session management patterns shown in the documentation by defining core methods like crawl() and crawl_many() that handle the session-based navigation and resource cleanup.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies inheritance relationship but misses the core purpose of session management and navigation patterns described in ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation directly mentions `CosineStrategy` as a similarity-based clustering strategy used to extract relevant content sections, which is explicitly implemented in the class `CosineStrategy`.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies CosineStrategy as a similarity-based clustering strategy for content extraction, it fails to mention the key configurable parameters and their role in controlling the clustering process, which is a crucial aspect highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the documentation example, `arun` is an asynchronous method used to process the crawl with the `CosineStrategy` as an extraction strategy. This usage is an implicit reference to the method `arun` within the `AsyncWebCrawler` class, demonstrating how to utilize the strategy during a crawl.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun is an asynchronous method that can use CosineStrategy as an extraction strategy during crawling, which aligns with the ground truth's description of how arun implements crawling logic that can apply CosineStrategy for content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "`CosineStrategy` is expected to extend some form of extraction strategy, leading to the abstract base class `ExtractionStrategy` which defines standard methods for extraction strategies. This form of extension is a logical inference based on design patterns typically seen in strategy-based classes.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between CosineStrategy and ExtractionStrategy as a base-derived class relationship, which aligns with the ground truth's description of ExtractionStrategy providing the foundation for derived strategies like CosineStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly referred to in the provided documentation snippet as the main class used for configuring the proxy setup in the example code.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class is used for proxy setup configuration, which aligns with the ground truth showing it implements proxy support through its initialization. While the predicted version is less detailed, it captures the core relationship without contradicting the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the documentation example, the `arun()` method is called on an instance of `AsyncWebCrawler`. It's implicit in understanding that `arun()` performs the actual crawling action.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() performs crawling but omits the crucial aspect about its relationship with proxy configuration handling mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' is used explicitly in the code snippet provided in the documentation to demonstrate media and image processing, including how to handle lazy-loaded content. Evidenced by the example calls to `await crawler.arun(...)`.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() usage for media processing but describes it only as an example from documentation rather than recognizing it as the core implementation method that enables the functionality",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The `CrawlResult` class is implicitly utilized in the documentation example as the return type of `arun()`. This is inferred from the iteration over `result.media[\"images\"]`, indicating the structure of the `CrawlResult` object to access media details like 'src', 'alt', 'desc', 'context', and 'score'.",
    "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that CrawlResult implements media functionality through its media field and demonstrates correct usage through the example. While it focuses more on implicit usage vs explicit definition, it conveys the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The relationship is evidenced explicitly in the documentation where the media information (e.g., images) is accessed through `result.media['images']`, showing it is an attribute of the `CrawlResult` class and is integral to media processing in the context of crawling results.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that media information, including images and their metadata, is stored in the CrawlResult.media dictionary. It correctly identifies the core relationship and usage pattern.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example function `crawl_dynamic_content` directly creates an instance of `AsyncWebCrawler`, indicating it is an explicit use of this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler through instantiation, but misses the key functionality around dynamic content crawling, session management, and JavaScript execution capabilities that are central to the class's purpose as described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `crawl_dynamic_content` calls the `arun` method, explicitly using this method to perform the crawling action as shown in the example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the arun method is used for crawling, but misses key aspects like session management and dynamic crawling capabilities that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "Within the cleanup section of the example, `kill_session` method is called on `crawler.crawler_strategy` indicating explicit use to end sessions.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session is used for cleanup and session ending, which aligns with the ground truth's explanation of it cleaning up browser resources at the end of the crawling example",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is instantiated in the example as `extraction_strategy`, specifically for handling content extraction based on a defined schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used with a schema for extraction, but misses the crucial implementation detail that it uses BeautifulSoup to parse HTML and apply selectors for extracting commit data, which is a key part of how it functions.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The document explicitly discusses the 'NlpSentenceChunking' class which uses NLP models to split text into sentences, as described under the 'NlpSentenceChunking' header.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that NlpSentenceChunking uses NLP models for sentence splitting, which aligns with the ground truth's explanation of using NLTK's sent_tokenize for sentence boundary detection.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates creating an instance of `AsyncWebCrawler` with `verbose=True` to initialize the web crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions creating an AsyncWebCrawler instance with verbose=True, but misses the core functionality of handling dynamic content, pagination, and content updates through JavaScript injection and wait conditions, which are central aspects of the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is used within the `for page in range(3)` loop to crawl pages with specific strategies like `JsonCssExtractionStrategy`, highlighting its role in executing the crawl action.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's basic crawling functionality but misses its key role in enabling dynamic content crawling through session-based features and JavaScript execution capabilities",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` class is instantiated with a schema and used as the extraction strategy in the `arun` method call, showing its role in managing how data is extracted during crawling.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in extraction during crawling, but misses the crucial aspect of how it implements structured data extraction using schema's baseSelector and fields for parsing GitHub commit information",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session` method of `AsyncPlaywrightCrawlerStrategy` is called to cleanly terminate the session identified by `session_id`, indicating its role in session management.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of kill_session as being responsible for terminating/cleaning up a session, which aligns with the ground truth's description of cleaning up browser resources by closing Playwright objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'CosineStrategy' as the class being used to initialize with various parameters. These parameters: 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k' are directly associated with instances of 'CosineStrategy'.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between the documentation parameters and CosineStrategy class, but misses explaining how these parameters are functionally implemented in the class methods, which is a key part of the relationship described in the ground truth.",
      "error_type": "key_omission"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' likely inherits from 'ExtractionStrategy', as it would be common for naming conventions indicating subclassing, and it is also part of the extraction strategy module. The base class provides the necessary interface for extraction mechanisms referenced in the documentation's context.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and its child classes (like CosineStrategy), and recognizes it as a base class providing an interface for extraction functionality. While it doesn't detail all the parameters, it captures the core architectural relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes an example usage of the async method 'arun'. The example illustrates crawling a URL with an option to include links in the markdown result.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality - using arun() to crawl a URL and generate markdown output with link options, which aligns with the ground truth's description of processing HTML to markdown with configurable link handling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example shows printing the 'markdown' attribute of the result, which is an instance of the 'CrawlResult' class returned by 'AsyncWebCrawler.arun()'.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the 'markdown' attribute being accessed from a result object, but misses the key point that it stores HTML-to-Markdown converted text and is an optional string value",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions using 'AsyncWebCrawler' in a code example, indicating that this class is used to manage the crawling with proxy updates.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler is used for managing crawling with proxy updates, which aligns with the ground truth's explanation of the class providing the foundational structure and async functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is demonstrated in the example code as 'await crawler.arun(url=url)', showing its usage in executing a crawl for a given URL.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used for executing crawls with URL parameters, which aligns with the ground truth's description of it being the core crawling functionality. While the predicted description is less detailed, it captures the essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation provides an example of how to use JsonCssExtractionStrategy to extract structured information from a complex HTML layout. The example HTML structure represents a typical use case that this extraction strategy would handle.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy is used for extracting structured data from HTML, but misses the crucial technical implementation detail about using BeautifulSoup and schema-based CSS selector mapping, which is core to how it works.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a specific implementation of the broader ExtractionStrategy, which it extends. This relationship is inferred as JsonCssExtractionStrategy would inherit the method interfaces and potentially some functionality from ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but focuses only on inheritance, missing the core purpose of HTML data extraction and processing described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation explicitly mentions accessing media content through `result.media[\"videos\"]` and `result.media[\"audios\"]` which corresponds to the `media` attribute in `CrawlResult` that deals with videos and audios metadata.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the media dictionary stores video and audio metadata in list format that can be accessed and processed. The predicted description correctly identifies the key structural relationship even if slightly less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses an instance of AsyncWebCrawler to perform a crawl operation with the method named `arun`. Although not explicitly named, it implies the utilization of this class given the context of async crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler handles crawling operations through its arun method, which is the core functionality described in the ground truth. While it doesn't mention the specific CrawlResult and HTML property details, it correctly identifies the main relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The ARUN function within AsyncWebCrawler is directly referenced in the example code as the method used to obtain the raw HTML. This functionality is used explicitly in the code snippet provided.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun function is used to obtain raw HTML, which aligns with the ground truth's explanation of AsyncWebCrawler.arun() retrieving unmodified webpage content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The `print(result.html)` statement accesses the `html` attribute of a CrawlResult object, which is the async response obtained from executing the `arun` method. It's implicit since the document expects the retrieval of HTML content.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies html as a property accessed via result.html, but misses key aspects about it storing complete unmodified HTML content and its purpose for custom processing/debugging",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides an example using the `AsyncWebCrawler` class, showing its use to configure proxies and headers and to perform web crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for web crawling and can be configured, but misses the crucial point about how the proxy and magic mode parameters are passed through kwargs to AsyncPlaywrightCrawlerStrategy",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the method `arun()` of the `AsyncWebCrawler` class is called to execute a crawling task with specific settings like the URL and using 'magic' mode.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a crawling method but misses the key relationship with proxy and magic mode functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` class is the default strategy used in `AsyncWebCrawler` for crawling. Although not directly mentioned in the example, its role as a strategy is implied, as it enables core crawling functionalities when `arun()` is invoked with 'magic' mode settings.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncPlaywrightCrawlerStrategy as a core crawling component but misses the main focus of the ground truth - the specific relationship between proxy configuration and magic mode implementation. While it mentions magic mode, it doesn't capture the proxy integration aspect that is central to the ground truth.",
      "error_type": "key_aspect_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example in the documentation explicitly imports and uses the `AsyncWebCrawler` class to perform web crawling tasks. This is evident from the line 'from crawl4ai import AsyncWebCrawler'.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately notes the import and usage of AsyncWebCrawler but misses describing the class's comprehensive implementation of the documented functionality, including content filtering, processing, and cache control features.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` of the `AsyncWebCrawler` is explicitly called in the documentation example. The call is used to perform a web crawl with several parameters set, as shown in 'result = await crawler.arun(...)'.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method's usage in the documentation but misses the key relationship between the documentation example parameters and their implementation in the actual method. The ground truth emphasizes how the example parameters map to the method's internal logic.",
      "error_type": "incomplete_core_relationship"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example processes images found in the result by iterating over `result.media['images']`, which implies access to the `media` attribute of `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that CrawlResult.media stores images that can be accessed via media['images'], as demonstrated in the example code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The code example iterates over `result.links['internal']` for processing links, showing indirect access to `links` attribute of `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core idea of accessing links through result.links['internal'], but misses the important fact that links are categorized into internal/external in the dictionary structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The script prints `result.error_message` in case of a crawl failure, indicating reliance on the `error_message` attribute from `CrawlResult` for error handling.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that error_message is used for error handling when a crawl fails, with result.success being False. The predicted version accurately describes the essential functionality, even though it's slightly less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly references 'LLMExtractionStrategy' when describing how it uses Language Models to extract structured data. Evidence from the text is its direct mention and usage example provided in the strategy initialization.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that LLMExtractionStrategy uses Language Models and mentions key parameters, it misses the crucial implementation aspects of how it processes HTML content and extracts structured data according to schemas and instructions",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a specific implementation of the ExtractionStrategy class hierarchy. While the document references LLMExtractionStrategy explicitly, ExtractionStrategy is its base class, implicitly involved in the functionality description.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and LLMExtractionStrategy, but misses the crucial aspect of how ExtractionStrategy provides the core infrastructure and required methods that enable LLMExtractionStrategy's functionality",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet provided in the documentation utilizes an instance of AsyncWebCrawler to execute the extraction process through its 'arun' method in the strategy application example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler's arun method is involved in the extraction process, but misses the crucial aspects of LLMExtractionStrategy support and integration with the caching system mentioned in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly called in the usage example within the provided documentation snippet, serving as the mechanism through which the LLMExtractionStrategy is invoked.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as being used with LLMExtractionStrategy but treats it merely as an invocation mechanism, missing its crucial role in implementing the core crawling logic and orchestrating the entire extraction process",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the `TopicSegmentationChunking` class as implementing a text chunking strategy using the TextTiling algorithm to segment text into topic-based chunks.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of using TextTiling algorithm for topic-based text segmentation, which aligns with what is implemented in the code and described in the ground truth",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is explicitly mentioned in the documentation snippet as part of the async function 'integrated_js_and_wait_crawl', where it is used to create a crawler instance with a specific verbose setting.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's presence in the documentation and its basic usage, but misses the crucial aspect of how it implements integrated JavaScript execution and extraction functionality through its arun method, which is a core part of the relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is explicitly invoked in the example to crawl pages, passing parameters including 'url', 'session_id', 'css_selector', and other strategies.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun being used for crawling and accepting parameters, but misses the core functionality described in ground truth about integrated JavaScript execution and waiting via js_code parameter",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The 'kill_session' method is explicitly called in the snippet to terminate the crawling session 'integrated_session', showing its explicit use for session management.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is called to terminate the session, but misses the important detail about cleaning up browser resources by closing page and context objects",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet describes using 'JsonCssExtractionStrategy' for extracting commits from the GitHub page using a CSS-based extraction schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for extracting structured data using CSS-based schema, which aligns with the ground truth's explanation of the class implementing HTML content extraction based on defined schema and selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly demonstrates the usage of `AsyncWebCrawler.arun()` to extract content from a URL, specifically highlighting how `fit_markdown` is used to get the main content.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() extracts content using AsyncWebCrawler but oversimplifies the process by focusing only on fit_markdown usage while missing the core extraction and chunking strategy implementation that enables this functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although `AsyncWebCrawler` is not explicitly discussed, its instantiation in the code example indicates its implicit role in supporting the `arun()` method for crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description acknowledges AsyncWebCrawler's role in crawling but misses the core relationship about how it implements the fit_markdown feature through content extraction and processing",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation refers to retrieving the processed content using `fit_markdown`, highlighting its role in extracting and preserving relevant content from crawled pages as mentioned directly in the example.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used for extracting and preserving relevant content from crawled pages, which aligns with the ground truth's description of it being a property that stores extracted main content after applying content extraction heuristics.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation shows a Python snippet demonstrating the use of `AsyncWebCrawler` to perform web crawling operations with screenshot capabilities. The method `arun` is used to initiate a crawl with specific parameters including `screenshot=True`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of AsyncWebCrawler's screenshot capabilities through the arun() method, mentioning the screenshot parameter and overall purpose. While it doesn't detail the internal storage mechanism, it correctly describes the key relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` from `AsyncWebCrawler` is invoked in the code snippet to perform a crawl on 'https://example.com'. Parameters such as `screenshot=True` and `screenshot_wait_for=2.0` are used to configure the screenshot capabilities, indicating that `arun` handles these parameters as part of its operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that arun handles screenshot capabilities through its parameters and execution. While it provides an example usage rather than implementation details, it accurately reflects the same high-level relationship described in the ground truth - that arun processes screenshot functionality via parameters and includes it in its results.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The example code in the documentation snippet checks the `screenshot` attribute of the `CrawlResult`. This indicates that `CrawlResult.screenshot` is used to retrieve the screenshot data (in base64 format) after a crawling task has completed.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage image as a base64-encoded string that can be decoded into a PNG file as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the screenshot property contains base64-encoded image data that can be retrieved after crawling and decoded into a PNG file",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` is explicitly mentioned in the example given within the documentation snippet. It is used to set up the crawling session and handle the crawling execution with customized hooks.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for setting up crawling sessions with custom hooks, which aligns with the ground truth's explanation of how the class provides infrastructure for implementing execution hooks through its crawler_strategy attribute.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The method `set_hook` is explicitly used within the example to set a custom hook `on_execution_started` in the crawling process.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that set_hook is used to set custom hooks, but misses the key implementation detail about storing hooks in a dictionary for execution at specific lifecycle points",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The method `kill_session` is explicitly called in the example to terminate a session after crawling multiple pages, as described in the text.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is called to terminate a session, but misses the key functionality of cleaning up browser resources (closing page and context objects) which is the main purpose described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the default strategy for `AsyncWebCrawler` and is thus implicitly involved in the example execution. The strategy encompasses the `set_hook` and `kill_session` methods utilized in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements hooks and mentions set_hook/kill_session methods, but incorrectly states it's 'implicitly involved' when the relationship is actually explicit and direct. The strategy class directly implements the hook functionality rather than being just an implicit default.",
      "error_type": "mischaracterization_of_relationship"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "`AsyncPlaywrightCrawlerStrategy` extends `AsyncCrawlerStrategy`. Although not explicitly mentioned, this relationship is necessary to use the abstract methods implemented by `AsyncPlaywrightCrawlerStrategy`.",
    "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on class inheritance between AsyncPlaywrightCrawlerStrategy and AsyncCrawlerStrategy, while the ground truth describes the purpose and functionality of the set_hook method for registering custom callbacks during crawling.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions using 'JsonCssExtractionStrategy' for pattern-based extraction, providing example usage. This class implements the extraction strategy described.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for pattern-based extraction and corresponds to the schema-based implementation shown in the code. While it's less detailed than the ground truth, it captures the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation shows 'arun()' being used as the method called on the crawler object with the 'JsonCssExtractionStrategy' passed as a parameter, indicating its role in executing the extraction strategy.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that arun() accepts and executes the extraction strategy, which aligns with the ground truth's explanation of arun() processing web pages using the JsonCssExtractionStrategy for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet involves 'AsyncWebCrawler' as the class whose instance calls the 'arun()' method, facilitating the usage of 'JsonCssExtractionStrategy' for web crawling and extraction tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between AsyncWebCrawler and JsonCssExtractionStrategy, but misses the key pattern-based extraction functionality and schema-based structured data extraction from repeating HTML elements that is central to the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly uses JsonCssExtractionStrategy in the first example snippet to perform pattern-based extraction, indicating it serves as the core extraction strategy in this context.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy is used for pattern-based extraction, but misses the crucial implementation details about using BeautifulSoup and CSS selectors to extract structured data according to a schema",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The text explicitly mentions LLMExtractionStrategy in the second code example to analyze dynamic content, showcasing its role in handling complex content evaluation via LLMs.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy being used for analyzing dynamic content through LLMs, which aligns with the ground truth's explanation of how it processes HTML content using LLM models with configurable settings.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of AsyncWebCrawler is implicitly utilized in both example snippets. It is invoked with parameters including 'url', 'js_code', and 'extraction_strategy' showing its role in orchestrating the crawling and extraction process.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that arun() works with extraction strategies and handles web crawling with parameters like url, js_code, and extraction_strategy, which aligns with the ground truth's explanation of its implementation of structured data extraction functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation includes a code snippet that explicitly creates an instance of 'JsonCssExtractionStrategy'. It demonstrates how to use this class to extract structured content from a webpage by defining a schema for repeated content patterns.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of JsonCssExtractionStrategy - using schema definitions to extract structured content from webpages based on patterns. While it doesn't explicitly mention recursion or nested data, it correctly identifies the main purpose and usage pattern.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code snippet shows an asynchronous call to 'crawler.arun()', indicating that the 'arun' method of 'AsyncWebCrawler' is implicitly used to initiate the crawling process with the specified extraction strategy.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an asynchronous method used for crawling, but misses the crucial aspect of how it processes the extraction strategy to extract structured data according to defined selectors and patterns",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly mentioned, the example implies the utilization of the 'AsyncWebCrawler' class to perform web crawling using the 'JsonCssExtractionStrategy'. This implies construction or existence of an 'AsyncWebCrawler' instance referred to as 'crawler'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncWebCrawler class uses JsonCssExtractionStrategy for web crawling, which aligns with the ground truth's explanation of pattern-based selection implementation. While it's less detailed, it doesn't contradict or misunderstand the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'crawler' in the usage example likely refers to an instance of the 'AsyncWebCrawler' class, which is responsible for running the crawler task with the specified configurations such as page timeout and delay_before_return_html. Evidence is the method call 'crawler.arun'.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncWebCrawler class handles crawler tasks with configurations, even though it doesn't explicitly mention the timeout and waiting functionality implementation details. The core relationship between the class and its functionality is accurate.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Explicitly mentioned in the example 'await crawler.arun', this method implements the crawling process described, managing behavior through parameters like 'page_timeout' and 'delay_before_return_html'.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method implements the documented timeout and waiting functionality, even though it doesn't explicitly mention the specific implementation through **kwargs. The core relationship between the code and documentation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result of 'arun' is likely an instance of 'CrawlResult', as 'arun' is designed to conduct a crawl and return structured results, which fits the context of capturing the page HTML and perhaps other metadata post process.",
    "ground_truth_relationship": "The CrawlResult class stores the final output data after applying the documented timeout and waiting configurations during web crawling, including success status, HTML content, and any error messages that may occur if timeouts are exceeded.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is used to store the output of the 'arun' crawling operation, which matches the ground truth's explanation of CrawlResult storing final output data after crawling with timeout configurations.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet demonstrates a call to 'crawler.arun()', explicitly using this method to perform crawling and apply content filters like 'word_count_threshold', 'exclude_external_links', 'exclude_external_images', and 'excluded_tags'.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that arun() handles content filtering through parameters that control how the crawled content is processed. While the predicted description focuses on specific examples from the documentation and the ground truth gives a more general explanation, they convey the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun()' function is implemented as part of the 'AsyncWebCrawler' class, which is implied by the reference to 'crawler.arun()' in the documentation.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies arun() as part of AsyncWebCrawler, but misses the core relationship that the documentation describes specific filtering parameters that are directly implemented in the arun() method",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' utilizes a strategy pattern through 'AsyncCrawlerStrategy', indicating a relationship with 'arun()'. However, 'AsyncWebCrawler' uses an instance of 'AsyncCrawlerStrategy' for its operation, showing indirect dependence.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables content filtering capabilities through its crawl method's kwargs parameter, which allows passing filtering options like word_count_threshold and excluded_tags as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncCrawlerStrategy as a strategy pattern class but focuses on its relationship with AsyncWebCrawler and arun() rather than its core purpose of enabling content filtering through its methods' kwargs parameters.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly shows how the 'arun' method of 'AsyncWebCrawler' is used to retrieve media elements from a URL.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used to retrieve media elements, but misses crucial information about the CrawlResult object and the structured media dictionary that allows access to different media types (images, videos, audios) with their metadata.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "While not mentioned by name, 'CrawlResult' is the type of result returned from 'arun()', and it is used to access media attributes such as 'media'.",
    "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that CrawlResult is used to access media attributes, but misses explaining the key functionality of media selection through the media dictionary that stores different media types as shown in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "'media' is explicitly mentioned in the documentation as a dictionary attribute accessed from CrawlResult, containing information about images, videos, and audios.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that media is a dictionary property containing categorized media elements (images, videos, audios) accessed via type keys. The predicted description correctly identifies the core relationship even if it doesn't elaborate on all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly demonstrates usage of the AsyncWebCrawler class by showing example code snippets where different browser types are configured using AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports different browser types through configuration, and demonstrates this with code examples. While it doesn't explicitly mention the crawler_strategy parameter and PlaywrightCrawlerStrategy, it captures the core relationship that the class enables browser selection functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler initializes with a crawler strategy that likely adheres to the AsyncCrawlerStrategy interface. While not directly mentioned, it is implied because AsyncWebCrawler must use a strategy that defines how crawling is done asynchronously.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that AsyncWebCrawler uses the AsyncCrawlerStrategy interface to define crawler behavior. While it doesn't explicitly mention browser types, it correctly identifies the strategy pattern relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The implicit relationship is based on the idea that the examples involving browser switching would require a concrete implementation of a strategy suitable for browser interaction. AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy and is therefore a likely candidate for what was configured in AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements the browser switching functionality described in the documentation, even though it doesn't spell out all the technical details about the start() method and browser types.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun() method of AsyncWebCrawler is explicitly demonstrated in the text snippet as the method to perform actual crawling using configured browser types.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the method for crawling but misses the key point about how browser selection actually works through the crawler_strategy initialization. It describes the usage rather than the mechanism.",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions using 'CSS Strategy' for well-structured HTML, which directly maps to the 'JsonCssExtractionStrategy' class, which is designed to handle structured content extraction using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements the CSS Strategy from the documentation and accurately notes its purpose of handling structured HTML using CSS selectors",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The mention of 'LLM Strategy' for natural language text directly maps to the 'LLMExtractionStrategy' class, which utilizes large language models for semantic understanding, as explicitly stated under 'Accuracy Needs'.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy implements the LLM Strategy option for natural language text processing and semantic understanding, which aligns with the ground truth's core relationship description.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text references 'Cosine Strategy' for both mixed/complex content and best content relevance, mapping directly to the 'CosineStrategy' class for content extraction based on semantic similarity, which is also explicitly mentioned in 'Accuracy Needs'.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy's role for mixed/complex content, but incorrectly emphasizes 'best content relevance' as a key feature when the ground truth focuses on moderate performance characteristics as a defining trait",
      "error_type": "partial_mischaracterization"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation mentions using the LLMExtractionStrategy class to perform structured data extraction. This is explicitly demonstrated in the example where an instance of LLMExtractionStrategy is created with a schema and instruction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of LLMExtractionStrategy for structured data extraction but omits crucial functionality around chunking, rate limiting, and parallel processing for different providers that is core to how the class actually works according to the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The relationship is implicit in how the example uses the class to define an extraction strategy, which is then passed into a method to function. This implies the existence of methods or functionality within LLMExtractionStrategy enabling such usage.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage pattern but fails to acknowledge the key functionality of structured data extraction and the specific handling of different LLM providers that is central to the class's purpose",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy's functionality is described in the context of extending an abstract base strategy. Implicitly, its implementation must extend ExtractionStrategy to conform to framework architecture.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly indicates that LLMExtractionStrategy extends ExtractionStrategy, it misses the crucial aspect of the base class providing core infrastructure for extraction methods and parallel processing functionality described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example explicitly demonstrates use of the arun method from AsyncWebCrawler, passing the LLMExtractionStrategy as an argument to perform extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that arun() accepts an LLMExtractionStrategy for extraction, which aligns with the ground truth's explanation of how the method implements structured data extraction through the extraction_strategy parameter.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The text mentions the `JsonCssExtractionStrategy` as a tool to extract structured data using a schema definition. It demonstrates its usage in combination with JavaScript execution for handling dynamically loaded content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JsonCssExtractionStrategy as a tool for structured data extraction and its ability to work with dynamic content through JavaScript execution.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text includes an example demonstrating the use of `AsyncWebCrawler` to perform crawling with dynamic JavaScript execution and a specific extraction strategy. Although not extensively discussed, it is integral to the example's execution.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports dynamic JavaScript execution and extraction strategies, which aligns with the ground truth's explanation of the class's implementation of these features through the arun method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `arun` is used within the example to execute the crawling task with the defined extraction strategy, demonstrating its operational role in the process described in the text.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's basic role in executing crawling with extraction strategy, but misses the key aspect about support for dynamic content extraction through js_code and related capabilities that the ground truth emphasizes",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly mentioned in the example code where an instance of it is created to perform crawling operations. The snippet shows how AsyncWebCrawler is used to handle the crawling in an asynchronous context.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for crawling operations in an async context, but misses the crucial functionality of handling complex page interactions through specific parameters (js_code, wait_for, session management) that is central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method of the AsyncWebCrawler class is explicitly used in the example. It's called to perform the actual web crawling actions, such as loading the initial page and executing JavaScript.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for web crawling but misses crucial functionality around handling dynamic content, session management, and complex interactions that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The document code snippet explicitly calls the kill_session method of the crawler_strategy, indicating its use to manage and terminate the session after all dynamic interactions have been completed.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is used for session termination, but misses the crucial aspect of resource cleanup (closing page/context objects) and preventing memory leaks, which is a key part of its purpose according to the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is not explicitly mentioned in the text, but it is an implicit part of AsyncWebCrawler initialization, given that it is used as the default crawler_strategy unless otherwise specified.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes AsyncPlaywrightCrawlerStrategy's connection to AsyncWebCrawler but mischaracterizes it as implicit/default rather than explaining its direct role in implementing the complex interactions shown in the example.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class of AsyncPlaywrightCrawlerStrategy, which defines the crawling interface used by AsyncWebCrawler. Though not mentioned directly in the documentation snippet, its role as a base class is crucial.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies AsyncCrawlerStrategy as the abstract base class that defines the interface used for crawling functionality, which aligns with the ground truth's explanation of how it enables the dynamic page interactions shown in the example.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document snippet directly references the 'JsonCssExtractionStrategy' class when it demonstrates pattern-based selection by setting up a schema for extracting news article components.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy uses schema and CSS selectors for extracting content, but misses the crucial aspects of recursive/nested extraction and handling repeating elements",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation does not directly name the 'AsyncWebCrawler' class, but the example code involves an 'arun' call on a 'crawler', which implies the use of 'AsyncWebCrawler', as it contains the 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the connection between AsyncWebCrawler and the arun method, but misses the key relationship about pattern-based selection through JsonCssExtractionStrategy which is the main focus of the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code shows 'crawler.arun' being called to execute the crawling process, implicitly referring to the 'AsyncWebCrawler.arun()' method.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that crawler.arun is called to execute crawling, which is true but misses the core functionality of processing the extraction strategy to extract structured data according to defined selectors and patterns",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method of AsyncWebCrawler is explicitly invoked in the example code within the documentation snippet, configuring a crawl operation with various options.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as a method for crawling, but incorrectly states it is 'explicitly invoked' in the documentation when the documentation actually shows it as an example of how to use the method with its configurable options",
      "error_type": "minor_misinterpretation"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler's arun method internally utilizes AsyncPlaywrightCrawlerStrategy for certain options like processing iframes and removing overlay elements, as suggested by the specific example flags used.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the options are used for processing iframes and removing overlays, but incorrectly states these are used through AsyncWebCrawler's arun method when the code shows they are implemented directly in AsyncPlaywrightCrawlerStrategy's crawl method",
      "error_type": "class_attribution_error"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text provides code snippets that explicitly use the 'arun' method of an instance of 'crawler', as seen in the examples demonstrated for both CSS and JavaScript-based waiting.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is used for crawler functionality with wait conditions, even though it focuses more on the example usage rather than the implementation details. The core relationship between the code and documentation is accurately captured.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Through the usage of 'arun', which expects an AsyncCrawlerStrategy-based object to perform operations like waiting for dynamic content, this relationship is shown implicitly as the operation is directed towards strategies formed on the AsyncCrawlerStrategy base.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that AsyncCrawlerStrategy provides the base functionality for handling wait conditions through its crawl method, though they express it slightly differently. The predicted description correctly identifies that the strategy pattern is used for these operations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The code snippet likely operates using 'AsyncPlaywrightCrawlerStrategy', which implements the 'AsyncCrawlerStrategy' and provides specific methods to handle browser-based operations indicated by these wait conditions.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements AsyncCrawlerStrategy and handles wait conditions, which aligns with the ground truth's explanation of how it implements wait conditions through the smart_wait method.",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation explicitly discusses the class 'NlpSentenceChunking', describing its purpose and when to use it. It states that the class uses NLP models to split text into sentences and provides an example of its usage, demonstrating its sentence-chunking functionality.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that NlpSentenceChunking uses NLP models to split text into sentences, which aligns with the ground truth's description of using NLTK's sent_tokenize for sentence boundary detection.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet discusses executing an asynchronous crawl using an instantiated object of the class AsyncWebCrawler. It gives an example usage of calling `.arun()` to perform a web crawl.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method being used for web crawling, but misses the core focus of the documentation which is about content filtering options and parameters available in arun()",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly demonstrates the usage of the `arun` method of the AsyncWebCrawler class by showing how to call it with parameters like url and word_count_threshold.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method that takes parameters, but misses its core purpose of content filtering as described in the ground truth. The prediction focuses only on method usage rather than its content filtering functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the documentation snippet as the object created within an async context manager (`async with AsyncWebCrawler(verbose=True) as crawler`). This demonstrates its initialization and usage for web crawling.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its usage with async context manager, but misses the crucial relationship between the arun() method and its implementation in the code, which is a key aspect highlighted in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is called in the example (`result = await crawler.arun(url=\"https://www.nbcnews.com/business\")`) in the documentation snippet. Thus, it is explicitly used to perform the crawling operation.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling but misses crucial aspects of its functionality like caching, HTML processing, and return type that are core to understanding the relationship between code and docs",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The `markdown` attribute of the `CrawlResult` class is accessed in the example (`result.markdown[:500]`), indicating its use in extracting a summary of the crawl result, even though it's not directly discussed in the snippet.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that markdown is used to store and display the crawl results, as shown in the example where result.markdown is accessed to print the crawled content",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an `async with` statement using `AsyncWebCrawler`, indicating its direct usage for web crawling with custom headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the async with usage correctly but misses the key point about how custom headers are supported through kwargs and AsyncPlaywrightCrawlerStrategy. It describes the usage pattern but not the underlying mechanism.",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is explicitly called in the example (`result = await crawler.arun(...)`), showcasing its role in executing the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the executing method for crawling but misses the key relationship about custom header support through crawler_strategy, which is the main focus of the ground truth.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is used implicitly through inheritance by default when `AsyncWebCrawler` is instantiated without a specified strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted text focuses on AsyncWebCrawler inheritance while the ground truth describes the custom header functionality implementation. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows an example using the 'arun' method of the AsyncWebCrawler class to perform a web crawl with custom HTML-to-text options.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is used for web crawling and can accept HTML-to-text customization options, which aligns with the ground truth's explanation of how html2text options are passed as kwargs to arun()",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is implicitly part of the trace as 'arun()' is a method within this class. Although not directly mentioned, it's implicitly referenced by the usage of 'arun'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the basic class relationship but misses the key functionality of HTML-to-text customization, instead focusing only on the existence of the arun method. It fails to capture the main purpose described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates using the 'AsyncWebCrawler' class via 'async with AsyncWebCrawler(verbose=True) as crawler'. This class is central to the example provided in the documentation snippet, showing how to instantiate it for web crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler class usage with async context manager, but misses key functionality around dynamic content crawling, session management, and JavaScript execution that are central to the class's purpose as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method of 'AsyncWebCrawler' is invoked in the documentation example, specified by calling 'await crawler.arun(...)'. This illustrates how the method is used to initiate a crawl for each page in the example session.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling pages in the example, but misses describing its core functionality around dynamic web crawling and session management which are central aspects mentioned in the ground truth",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The 'kill_session()' method is utilized in the provided example via 'await crawler.crawler_strategy.kill_session(session_id)', showcasing its role in terminating a session after completing the crawl.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that kill_session() is used to terminate/clean up a session after crawling is complete, which aligns with the ground truth's explanation of cleaning up browser resources at the end of the GitHub crawling example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' is explicitly instantiated with a schema in the documentation example: 'extraction_strategy = JsonCssExtractionStrategy(schema)', which defines a strategy for extracting specific content from a webpage.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses a schema, but misses the crucial aspect of how it implements the extraction logic using BeautifulSoup and applying selectors to extract structured data",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly instantiated and used in the `main()` asynchronous function. The documentation example uses it with a `verbose=True` argument, indicating usage of its initialization parameters.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on initialization and usage of AsyncWebCrawler, while the ground truth describes its error handling functionality. While both describe aspects of the class, they focus on different core functionality.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly instantiated within the `main()` function of the example. It is used as an argument to the `extract_with_retry` function, indicating its role in the extraction process.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy's role in extraction but misses the core focus on error handling and parallel processing implementation that is central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the `extract_with_retry` function, `crawler.arun` is called to perform the web crawling process using an `LLMExtractionStrategy`. This showcases its role in executing the asynchronous crawling task.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that crawler.arun is used for web crawling, but misses the core focus of the ground truth - the comprehensive error handling functionality that returns CrawlResult objects with error details.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example accesses `result.extracted_content` after calling `arun`. This attribute retrieval demonstrates its role in obtaining data following the crawling operation.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies that extracted_content is accessed from the result of arun(), but misses the crucial aspect that it stores text data retrieved from web pages and the retry mechanism for handling extraction failures",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses 'await crawler.arun' to perform a crawling operation, which indicates that the 'AsyncWebCrawler' class is being utilized as it defines the 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and async nature but misses the core functionality of content filtering described in the ground truth. It focuses only on the async implementation rather than the filtering capabilities.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly mentioned in the documentation snippet as part of invoking a web crawl with filter options such as 'word_count_threshold' and 'exclude_external_links'.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method and its connection to content filtering, but oversimplifies by only mentioning it as 'explicitly mentioned' rather than explaining how the filtering parameters are implemented as kwargs and processed during HTML extraction",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions `LLMExtractionStrategy` as part of the functionality provided by `AsyncWebCrawler` for using Language Models (LLMs) to extract structured data or content from web pages.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between LLMExtractionStrategy and AsyncWebCrawler for extracting structured data using Language Models from web pages, matching the implementation shown in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet discusses the ability of `AsyncWebCrawler` to utilize Language Models for extracting data through mentioned strategies like `LLMExtractionStrategy`, indicating that `AsyncWebCrawler` implements this functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of AsyncWebCrawler supporting LLM-based extraction through extraction strategies. The predicted description accurately reflects the main relationship between AsyncWebCrawler and LLM extraction capabilities, even though it's less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet contains example code using the 'arun' method of the 'crawler', which is initialized as an 'AsyncWebCrawler' instance. The text provides example usage for handling dynamic content via JavaScript code execution in a browser context, specifically through 'arun'.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method for web crawling with browser context, but misses the crucial functionality around dynamic content handling and form interactions described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, the AsyncWebCrawler.arun() method likely utilizes 'AsyncCrawlerStrategy' or its derived class as the strategy design pattern is employed within AsyncWebCrawler by default. The arun method's behavior aligns with the asynchronous crawling strategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as using the strategy pattern, but introduces speculation about AsyncWebCrawler.arun() without focusing on the core functionality of the strategy interface enabling dynamic content handling as described in the ground truth.",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The documentation indicates using JavaScript for scan and load interactions which aligns with the capabilities of 'AsyncPlaywrightCrawlerStrategy', a direct subclass of 'AsyncCrawlerStrategy', showcasing it's typical usage through JavaScript execution.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles JavaScript interactions, but misses the crucial detail about the smart_wait method that specifically handles both scroll-to-bottom and form interaction patterns with selector/condition waiting functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly demonstrates the use of the AsyncWebCrawler class by showing how an instance of it is created with various configurations such as 'browser_type', 'headless', etc.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler can be instantiated with various configurations, but it misses describing the full range of functionality shown in the ground truth, particularly the implementation of content handling, timing controls, and anti-detection features through the arun method.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example function 'crawl_with_advanced_config' in the documentation invokes 'arun' on an instance of AsyncWebCrawler, which is responsible for executing a crawl with specified settings.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic relationship of arun() being called through AsyncWebCrawler, but misses key aspects like error handling, caching, and content extraction mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The AsyncWebCrawler instantiates an AsyncPlaywrightCrawlerStrategy as a default strategy which handles interaction with the browser, a crucial part of web crawling settings outlined in the example.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but misses the main point that the code implements features being demonstrated in the documentation example, instead focusing only on instantiation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The documentation returns 'result.screenshot' showing a result of the crawl, which corresponds to the screenshot attribute in the CrawlResult class being accessed.",
    "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the screenshot is returned as part of the crawl result and accessed via result.screenshot, which aligns with the ground truth's explanation of the screenshot property storing screenshot data requested through screenshot=True",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The 'success' state is returned in the example function, corresponding to 'result.success' in CrawlResult, indicating completion status.",
    "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that 'success' is a boolean field returned as part of the function's output dictionary to indicate the successful completion of the crawling operation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet includes examples that show direct instantiation of the CosineStrategy class with various clustering and filtering parameters. This indicates direct use of this class to demonstrate its application in clustering and content filtering pipeline implementations.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the class instantiation and parameter usage, but misses the core functionality of combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity that is central to the ground truth description.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The provided documentation contains an example function 'extract_pricing_features' which uses an instance of AsyncWebCrawler within an asynchronous context manager, indicating its use in the content extraction pipeline facilitated by the CosineStrategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's use with async context manager and CosineStrategy, but misses key aspects like caching, customizable extraction strategies, and the broader clustering support mentioned in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Inside the function 'extract_pricing_features', the method arun() is implicitly called using the AsyncWebCrawler instance to perform crawling with the specified strategy, indicating how this method is utilized within the usage example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as being used within extract_pricing_features for crawling, but misses crucial aspects about its role in handling custom clustering, content filtering, caching, and error handling described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example code accesses the extracted_content attribute of the CrawlResult returned by the arun() method. This demonstrates implicit usage as the function processes the crawling result to filter information.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the crawl result, but misses the key aspect that it specifically contains JSON-serialized clustering/filtering results with pricing features and similarity scores",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'LLMExtractionStrategy' as a class that can be customized by providing a provider and API token, allowing integration with various LLM providers.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main concept that LLMExtractionStrategy can be customized with different providers and API tokens for LLM integration, which matches the implementation shown in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates usage of the 'arun' method of the 'AsyncWebCrawler' class to control link filtering based on different criteria like excluding external and social media links.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the arun method allows for link filtering through various parameters like excluding external and social media links. While it doesn't explicitly mention the **kwargs passing mechanism described in the ground truth, this is a minor implementation detail that doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The class 'AsyncWebCrawler' is explicitly mentioned in the documentation snippet as part of the usage example demonstrating how to enable verbose mode for detailed logging.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the verbose parameter in AsyncWebCrawler but only mentions it as part of a usage example, missing the crucial understanding that it controls conditional logging statements throughout the code's execution.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the 'AsyncWebCrawler' class is implicitly utilized in the example provided within the documentation text for executing a web crawl on a specified URL.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that arun is used for web crawling, it misses the core relationship about verbose logging functionality that the ground truth describes. The documentation specifically focuses on verbose logging capabilities.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' can be initialized with any strategy derived from 'AsyncCrawlerStrategy', with 'AsyncPlaywrightCrawlerStrategy' being its default strategy, making its role implied in the operation of 'arun'.",
    "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the inheritance relationship between AsyncWebCrawler and AsyncCrawlerStrategy classes, while the ground truth specifically describes the verbose logging functionality implementation in AsyncPlaywrightCrawlerStrategy. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncCrawlerStrategy' serves as a base class for 'AsyncPlaywrightCrawlerStrategy', which is implicitly used as the default crawling strategy in 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The abstract AsyncCrawlerStrategy class provides the foundation for implementing verbose logging through its crawl methods, which the documentation demonstrates how to enable via the verbose parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but misses the main relationship about verbose logging functionality shown in the documentation. It also makes assumptions about PlaywrightCrawlerStrategy that aren't evidenced in the given code/docs.",
      "error_type": "incomplete_and_presumptive"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text explicitly references `CosineStrategy` and describes its purpose as using similarity-based clustering. The usage example shows how `CosineStrategy` is instantiated and used within the code, confirming it implements the functionality described in the documentation.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that CosineStrategy implements similarity-based clustering with configurable parameters, matching the ground truth's explanation of how the class uses these parameters for content extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not explicitly mentioned in the text, `AsyncWebCrawler` is seen in the usage example where it uses the `CosineStrategy` as part of its workflow to perform crawling with the `arun` method. The method invocation suggests that `AsyncWebCrawler` utilizes `CosineStrategy` to execute the task described in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that AsyncWebCrawler integrates with CosineStrategy through its arun() method as an extraction strategy option. The predicted description captures the core relationship even though it's less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example directly calls `arun` method of `AsyncWebCrawler` with `CosineStrategy` for extracting relevant content from a URL. While not explicitly described in the summary text, the example illustrates the method's operation within the workflow.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() is used with CosineStrategy for content extraction, which aligns with the ground truth's explanation of arun() implementing crawling logic that accepts and applies CosineStrategy for content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is explicitly mentioned in the provided example code snippet. It demonstrates a usage scenario where a CSS selector is used to extract parts of a webpage, directly implementing the documented functionality of extracting content using CSS selectors.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship that the arun method handles CSS selector-based content extraction, which aligns with the ground truth's explanation of how CSS selectors are used to filter and extract specific webpage elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example uses 'crawler.arun(...)' to extract relevant content as markdown. This implies the usage of the 'AsyncWebCrawler' class, which provides the 'arun' method for crawling operations.",
    "ground_truth_relationship": "The .fit_markdown property mentioned in the documentation is implemented through the aprocess_html method which extracts and processes the result['fit_markdown'] field during web content crawling to provide cleaned, main content-focused markdown output.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the use of AsyncWebCrawler's arun method for content extraction, but misses the key relationship about the fit_markdown property's implementation through aprocess_html and its role in providing cleaned main content.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is directly shown in the code example within the provided documentation snippet, indicating that it performs the content extraction process highlighted in the text.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun is involved in content extraction but misses the crucial async crawling and markdown conversion aspects of the functionality that are central to the ground truth relationship",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "This attribute 'fit_markdown' of the 'CrawlResult' is specifically mentioned in the documentation as the field printed to access the main content extracted in markdown format.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used to store/access the main content in markdown format after extraction, which aligns with the ground truth's core meaning",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly shows the instantiation of the AsyncWebCrawler class, using it within an async context manager. This is directly mentioned in the text snippet provided.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the async context manager usage shown in the documentation, but misses mentioning the key proxy updating functionality which is a crucial aspect highlighted in the documentation example",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the AsyncWebCrawler class is explicitly called within the example code provided in the documentation.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes that arun() is used in the documentation example, but misses the core point about its functionality as the main crawling method that supports proxy configuration",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy by default if no 'crawler_strategy' is provided. This relationship is implied because, without specifying a custom strategy, the default behavior relies on AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description discusses AsyncWebCrawler using AsyncPlaywrightCrawlerStrategy as a default strategy, while the ground truth describes proxy handling functionality within AsyncPlaywrightCrawlerStrategy. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation mentions the 'JsonCssExtractionStrategy' explicitly in the context of extracting information from a hypothetical e-commerce website's HTML structure. This strategy is used to parse and extract specific elements like product categories, product details, reviews, and related items.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used to extract structured data from HTML elements in an e-commerce context, which aligns with the ground truth's explanation of using BeautifulSoup to parse HTML based on CSS selectors and schema mapping.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The document snippet explicitly mentions instantiation and usage of the `AsyncWebCrawler` class, demonstrating how to pass `proxy_config` with username and password.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes the usage of AsyncWebCrawler with proxy_config, but misses explaining that the proxy_config gets passed to the underlying AsyncPlaywrightCrawlerStrategy, which is a crucial aspect of how the authentication actually works.",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is invoked for executing the crawling process with the proxy configuration, as shown in the example provided in the snippet.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly convey that the arun() method works with proxy configuration to execute crawling, with the predicted description capturing the core relationship even though it omits some implementation details about crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The document does not directly mention `AsyncPlaywrightCrawlerStrategy`, but given `AsyncWebCrawler` initializes with a `AsyncCrawlerStrategy`, and the proxy configuration features, it's implied `AsyncPlaywrightCrawlerStrategy` plays a role.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies a connection between AsyncWebCrawler and proxy configuration, but misses the key implementation details about how authenticated proxies are handled through ProxySettings in the start() method.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is implicated in the example as the instance `crawler` responsible for initiating the `arun` method call.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's involvement but misses its core role in processing and organizing media types into structured dictionaries, which is the main focus of the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is directly mentioned and demonstrated in the snippet for crawling operations on a URL.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the key method for crawling but misses its crucial role in media selection and organization through the CrawlResult object's media dictionary structure",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The attribute `media` of the `CrawlResult` class is directly accessed in the example to retrieve various types of media details, including images, videos, and audios.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that media is accessed via the media property to get different types of media details. Both descriptions convey that media is a dictionary containing categorized media elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation mentions using 'LLM for complex interpretation', directly relating to the 'LLMExtractionStrategy' class, which is designed for leveraging language models for such tasks.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM's complex interpretation capability, while the ground truth describes the error handling and retry logic implementation. These are distinct functional aspects.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The guidance about starting with 'CSS for structured data' references using CSS-based strategies, which aligns with the 'JsonCssExtractionStrategy' for extracting structured information using CSS selectors.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the JsonCssExtractionStrategy implements the CSS-based approach mentioned in the documentation for handling structured data extraction using CSS selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text advises trying 'Cosine for content relevance', directly pointing to the 'CosineStrategy', which is designed for determining the relevance of content using cosine similarity.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the CosineStrategy is used for content relevance evaluation using cosine similarity, which aligns with the ground truth's explanation of how the class implements this functionality through cosine similarity metrics.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Within the error handling code example, the use of 'result.success' and 'result.error_message' indicates the use of the 'CrawlResult' class, which contains these fields.",
    "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the CrawlResult class contains the fields (success and error_message) used in the error handling example code, capturing the core relationship between the class and its usage in error handling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code snippet in the documentation ('result = await crawler.arun(...)') directly involves calling the 'AsyncWebCrawler.arun()' method, indicating its key role in initiating crawls.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies the arun() method's role in crawling, but misses the key ground truth relationship about error handling patterns and caching implementation that match the documentation's best practices",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The documentation example demonstrates setting a hook through `crawler.crawler_strategy.set_hook` where `crawler_strategy` is implicitly an instance of `AsyncPlaywrightCrawlerStrategy`, given the `crawler` is an instance of `AsyncWebCrawler` which defaults to using `AsyncPlaywrightCrawlerStrategy` if none is provided.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncPlaywrightCrawlerStrategy implements a hook system allowing custom functions to be executed at specific stages of the crawling process. The predicted description correctly identifies the connection between the class and its hook functionality, even though it approaches it from a slightly different angle.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The documentation snippet explicitly calls the `set_hook` method on the `crawler_strategy` attribute of the `AsyncWebCrawler` instance. The function `on_execution_started` is set as a hook to execute when 'execution_started' is triggered.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of set_hook - it allows setting custom hooks that get executed at specific points in the crawling process, as demonstrated in the example with on_execution_started.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation includes a line where `crawler.crawler_strategy.kill_session(session_id)` is called, explicitly utilizing the `kill_session` method to terminate a specific session.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly indicate that kill_session is used to clean up/terminate a session, with the predicted description accurately identifying its usage in the documentation example. While the ground truth provides more implementation details, the core relationship is captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example explicitly instantiates `AsyncWebCrawler`, setting it up as the main class to coordinate the crawl process, utilizing its methods such as `arun`.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the main coordinator class but misses the crucial aspect of custom execution hooks through crawler_strategy that is central to the ground truth relationship",
      "error_type": "key_feature_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` is explicitly called in the example, to execute the web crawling task on a specified URL, iterating over pagination of the commits.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as being used for web crawling, but misses the crucial aspect that it enables custom hooks through crawler_strategy, instead focusing only on the pagination example usage",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions using `async_webcrawler.arun()` for crawling, which directly suggests the use of the `AsyncWebCrawler` class to perform the crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions using AsyncWebCrawler's arun() method for crawling, but fails to mention the key screenshot capability relationship that is the focus of the ground truth description. The prediction misses the core functionality being documented.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The documentation example accesses `result.screenshot`, indicating the result of the crawl is an instance of `CrawlResult`, which has an attribute `screenshot` used to store Base64-encoded images.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage image as a base64-encoded string that can be decoded into a PNG file as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the screenshot is stored as a base64-encoded attribute in the CrawlResult object, which aligns with the ground truth's explanation of the relationship",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is explicitly called in the code snippet, as part of setting up the crawl request with screenshot capabilities.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles screenshot functionality, but incorrectly states it's 'explicitly called' in the code snippet when the code actually shows the method's implementation rather than its usage.",
      "error_type": "misinterpretation_of_code_context"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the 'JsonCssExtractionStrategy' as a tool for extracting structured data using schemas. The text provides guidance on advanced techniques for using this strategy, indicating that it is directly implemented for handling schema-driven extraction tasks.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that JsonCssExtractionStrategy is a schema-based data extraction tool, with the predicted description capturing the core concept of schema-driven extraction that matches the ground truth's explanation of schema-based implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses the `AsyncWebCrawler` class in the example of session-based crawling. This class is instantiated and managed within the asynchronous context, showing its role in handling web crawling sessions.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler in session-based crawling, but misses crucial aspects about how it handles persistent browser sessions and specific user interactions mentioned in the ground truth. The handling of core methods like __aenter__ and crawler_strategy.kill_session is not addressed.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is explicitly called in the documentation's code example to perform crawling operations with specific parameters like URL, session ID, and JavaScript execution.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the arun() method implements session-based crawling functionality through a session_id parameter, as demonstrated in the documentation example. The predicted description correctly identifies the key relationship even though it's less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session()` method is explicitly mentioned as being used to close the session after the crawling operations, highlighting its utility in managing session lifecycle.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies kill_session's role in closing sessions but omits the crucial implementation details about closing both page and context objects and removing the session from memory",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly stated in the text, the `AsyncPlaywrightCrawlerStrategy` is implicitly involved since `AsyncWebCrawler` uses it as the default strategy for crawling operations.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling by maintaining a sessions dictionary that maps session_ids to browser contexts and pages, allowing for stateful interactions across multiple requests as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy's involvement in crawling but misses the core session management functionality described in the ground truth. It fails to explain the key purpose of maintaining stateful interactions across requests.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the use of the 'AsyncWebCrawler' class with different browser types including 'chromium', 'firefox', and 'webkit'. This demonstrates that 'AsyncWebCrawler' is configured to support multiple browser engines.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that AsyncWebCrawler supports multiple browser types (chromium, firefox, webkit) through configuration, which aligns with the ground truth's explanation of browser type selection via the crawler_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the provided examples, the 'arun' method is called on instances of 'AsyncWebCrawler'. This method is indirectly highlighted in the code snippet as being part of the workflow for each browser engine example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun is used with AsyncWebCrawler but misses the key relationship about browser_type parameter determining the engine selection, which is the main focus of the documentation.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly describes creating an extraction strategy instance using JsonCssExtractionStrategy by defining a schema for extracting data on cryptocurrency prices from Coinbase.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of using JsonCssExtractionStrategy with a schema for Coinbase data extraction, but misses the crucial detail about how it processes HTML using CSS selectors to extract specific fields (name, symbol, price) from table rows",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "AsyncWebCrawler is used in the example to perform a web crawl with the previously defined extraction strategy, showcasing how JsonCssExtractionStrategy integrates within a larger crawling framework.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler handles web crawling while integrating with JsonCssExtractionStrategy, which aligns with the ground truth's explanation of how the crawler implements the cryptocurrency price extraction example using arun() method and JsonCssExtractionStrategy schema.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the documentation, AsyncWebCrawler is used with its arun method to configure and execute a crawl operation on the given URL with extraction strategy settings.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's arun method being used for crawling URLs, but misses the crucial aspects of extraction strategy implementation and structured data return that are core to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation demonstrates the usage of the `arun()` method multiple times within the example code. It is explicitly mentioned and used to perform various crawl tasks, such as extracting the main content and obtaining structured data.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core idea that the documentation demonstrates multiple uses of the arun() method. While it doesn't explicitly mention the different extraction strategies, it correctly identifies the relationship between the code and documentation showing multiple use cases of arun().",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` is constructed with a default `AsyncPlaywrightCrawlerStrategy`. Although the strategy is not explicitly mentioned in the text, it is implicitly used as it is the default strategy when no other strategy is provided.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the core functionality shown in the documentation example by providing methods for concurrent web crawling with multiple output formats through its crawl() method, which supports various extraction strategies and configurations like LLM and pattern-based extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncPlaywrightCrawlerStrategy as being involved with web crawling but describes it as a default strategy used by AsyncWebCrawler, while the ground truth correctly describes it as implementing core crawling functionality and supporting multiple extraction strategies directly. The predicted version misses key functionality aspects.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions creating a new instance of `LLMExtractionStrategy` with specific parameters for extracting data with a language model, as shown in the example code.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that LLMExtractionStrategy is used for extracting data with language models and takes specific parameters, which aligns with the ground truth's description of its functionality in processing URLs with custom providers, schemas, and instructions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet includes code that creates an instance of `JsonCssExtractionStrategy` to demonstrate extracting repeated patterns using a specific schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies JsonCssExtractionStrategy's role in pattern extraction with a schema, it misses the key aspect of how it systematically uses CSS selectors to extract structured data from repeated HTML elements",
      "error_type": "critical_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code accesses `extracted_content` from the `CrawlResult` object to obtain structured data and pattern information in the usage example.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that extracted_content stores crawled data as a string that gets parsed as JSON in the example for structured and pattern data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The `fit_markdown` attribute of the `CrawlResult` object is accessed to get the main content in the provided example.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used to access main content in the example, which aligns with the ground truth showing it being used to store and access the main extracted webpage content",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example code accesses the `media` attribute of the `CrawlResult` object to include media information in the returned result.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that media information from the CrawlResult is included in the final returned result, which aligns with the ground truth showing the media dictionary being returned in the function's response.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of the `AsyncWebCrawler` class in the code example for managing web crawling with caching functionality.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its caching functionality, but misses key implementation details about async_db_manager and the bypass_cache parameter that are central to how the caching actually works",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is explicitly called in the provided code snippet to crawl a given URL, demonstrating its functionality and ability to bypass the cache through the `bypass_cache` parameter.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method and its cache bypass capability, but misses the core focus on caching functionality and implementation that the ground truth emphasizes. The ground truth specifically describes how caching works and demonstrates it with example code showing both cached and bypass scenarios.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly initialized with a proxy argument in the provided code snippet, demonstrating how to use it for proxy configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description states that the proxy is passed directly to AsyncWebCrawler initialization, but the ground truth clarifies that proxy support is actually implemented through the crawler_strategy parameter. While both describe proxy configuration, the implementation mechanism is misrepresented.",
      "error_type": "implementation_mechanism_misunderstanding"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` is implicitly involved when the example `await crawler.arun(url=\"https://example.com\")` is used within the code snippet, indicating its role in executing the crawl with proxy settings.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun method is the core implementation used when executing crawls with proxy settings through AsyncWebCrawler, which aligns with the ground truth's explanation of arun's role in implementing crawling functionality and handling proxy configurations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet includes an example of using a class named 'LLMExtractionStrategy'. It demonstrates how to create an instance of this class with a specific provider, api_token, schema, and instruction, which aligns with the class definition's initialization parameters.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic constructor parameters and class name, but misses crucial functionality aspects like chunking, rate limiting, and parallel processing for different LLM providers that are central to the class's purpose",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code example in the documentation uses an instance of 'AsyncWebCrawler' to execute the 'arun' method, which is responsible for performing the crawl with the specified strategy ('LLMExtractionStrategy'). This artifact provides the context in which the strategy is applied.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented LLM-based extraction functionality through its arun() method, which accepts an extraction_strategy parameter that can be set to LLMExtractionStrategy for structured data extraction using various LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler's arun method is used to execute crawls with specified strategies, which aligns with the ground truth's description of how the class implements LLM-based extraction through arun() with extraction_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example code snippet, the 'arun' method is invoked with parameters including a URL and the extraction strategy. This illustrates the method's role in processing the model's extraction tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that 'arun' accepts extraction strategy as a parameter but misses the crucial aspect of how it implements structured data extraction through LLM providers and their configuration",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the example, 'result.extracted_content' is used to retrieve extracted content after running the crawl, aligning with obtaining the processed output of the 'LLMExtractionStrategy'. Though not directly mentioned, it implies the usage of 'CrawlResult' model's 'extracted_content' attribute.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that result.extracted_content contains the extracted data from LLMExtractionStrategy, which can be processed further. While it doesn't explicitly mention JSON strings or Pydantic models, it conveys the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the `JsonCssExtractionStrategy` as a feature used for structured data extraction using CSS selectors. The description elaborates on how it defines a schema for selecting repeating elements and fields.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JsonCssExtractionStrategy as a tool for structured data extraction using CSS selectors with a schema for repeating elements and fields, aligning with the implementation shown in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation shows how to use the `JsonCssExtractionStrategy` with `AsyncWebCrawler`, suggesting that the AsyncWebCrawler class can be configured or integrated to work with this extraction strategy, although it is not the primary focus.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler can work with JsonCssExtractionStrategy but misses that it's specifically implemented in aprocess_html to extract structured data using CSS selectors from HTML content",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is likely extending from the `ExtractionStrategy`, which is an abstract base class for extraction strategies. This relationship is not directly mentioned in the text but is inferred from the naming convention and typical class design in such libraries.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted text correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, it mentions this as an inference rather than describing the foundational functionality that ExtractionStrategy provides for JSON CSS extraction, which is a key aspect highlighted in the ground truth.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation describes a JSON configuration for extracting nested objects and lists using selectors, which directly correlates to the 'JsonCssExtractionStrategy' class designed to parse HTML content according to a JSON schema by selecting elements via CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship between the JsonCssExtractionStrategy class and the JSON schema configuration for extracting nested data using CSS selectors. While it's less detailed than the ground truth, it correctly identifies the main functionality and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates and uses the class AsyncWebCrawler with a context manager for crawling a webpage.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler and its context manager usage, but misses the core dynamic content crawling functionality with pagination and JavaScript injection capabilities that are central to the class's purpose.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a call to the arun() method of the AsyncWebCrawler class, signifying direct usage to perform the web crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for web crawling but misses the crucial aspect that it specifically enables session-based dynamic crawling with JavaScript execution and content update handling",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides an instance of JsonCssExtractionStrategy, using it as an extraction strategy within the arun method parameters.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of JsonCssExtractionStrategy but misses its core implementation purpose of parsing schemas to extract structured data based on selectors and fields",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "An implicit call to kill_session() is shown in the code example, indicating session management for dynamic web content.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies session management but mischaracterizes the explicit kill_session() call as implicit. The ground truth more accurately describes the actual cleanup functionality.",
      "error_type": "incorrect_call_description"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation explicitly mentions the 'TopicSegmentationChunking' class, explaining that it employs the TextTiling algorithm to segment text into topic-based chunks. It identifies thematic boundaries in the text, which is precisely the functionality provided by this class implementation.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of using TextTiling algorithm for topic-based text segmentation, which aligns with the ground truth's explanation of the implementation using NLTK's TextTilingTokenizer",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows usage of the `arun()` method from the `AsyncWebCrawler` class to initiate a web crawling operation, as demonstrated by `result = await crawler.arun(url=\"https://example.com\")`.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its basic crawling functionality, but misses the crucial metadata extraction aspect that is central to the ground truth relationship. The ground truth specifically emphasizes the metadata processing and access through .metadata property.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly mentioned, `AsyncWebCrawler` is implicitly involved as `crawler` likely refers to an instance of this class, which provides the `arun()` method.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncWebCrawler's involvement in metadata extraction but only mentions it implicitly and misses explaining how it actually processes metadata through aprocess_html. It doesn't explain the core metadata extraction functionality that's central to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "The documentation explicitly accesses the `metadata` attribute from a CrawlResult instance using `metadata = result.metadata`, which indicates that the metadata extraction is a recognized field within the CrawlResult class.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that metadata is an attribute of the CrawlResult class that contains extracted metadata, which aligns with the ground truth's description of metadata being a property storing page metadata in a dictionary format.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The `result` variable contains a `CrawlResult` object, implicitly implying that the class holds metadata and other results that are extracted via the `arun()` method.",
    "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that CrawlResult holds metadata and other extracted results, which aligns with the ground truth's description of the metadata field storing page metadata in the class",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a code example where `AsyncWebCrawler.arun()` is called as `crawler.arun(url=\"https://example.com\")`. This indicates that the `arun()` method is used to perform the asynchronous crawling operation which eventually yields a `CrawlResult` containing `fit_markdown`.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() performs asynchronous crawling and returns a CrawlResult, but misses the crucial aspect of how it processes the HTML through extraction/chunking strategies to enable the fit_markdown feature",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "In the documentation, `fit_markdown` is mentioned as a feature of the `CrawlResult` object resulting from the `arun()` method. The feature is described as using advanced heuristics to extract main content, which is directly referenced in the `CrawlResult` model's `fit_markdown` attribute.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies fit_markdown as a feature that uses heuristics to extract main content from webpages, which aligns with the ground truth's description of it being an optional string property that stores extracted main content after applying content extraction heuristics.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "While `fit_markdown` is highlighted, `markdown` is compared with `fit_markdown` in the example code, implying its usage. It represents the regular markdown content from the `CrawlResult`, offering a basis for comparison to show the effectiveness of `fit_markdown`.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the relationship between markdown and fit_markdown, noting they are comparable properties with fit_markdown being more focused/filtered compared to the regular markdown content",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation implicitly involves `AsyncWebCrawler` when executing the `arun()` method. It's the class within which `arun()` is implemented, orchestrating the crawling and processing workflow discussed in the doc.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's involvement in executing arun(), but misses the key point about how it specifically implements fit_markdown feature through aprocess_html and the content extraction mechanisms",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` from the `AsyncWebCrawler` class is explicitly mentioned in the documentation example as the method being called to initiate the crawling process with anti-detection features.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method for crawling, but misses the key relationship that arun() implements anti-detection through keyword arguments passed to crawler_strategy.crawl()",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the text, `AsyncWebCrawler.arun()` relies on the `AsyncPlaywrightCrawlerStrategy` to implement the crawling operation, which includes handling 'simulate_user', 'override_navigator', and 'magic' for anti-detection.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy implements anti-detection features through parameters like simulate_user, override_navigator, and magic. While it doesn't specify the exact implementation details using context.add_init_script(), it accurately represents the core relationship and functionality.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The text snippet describes how to use the RegexChunking class to split text based on a given regex pattern. This is explicitly shown in the code example where RegexChunking is instantiated with a pattern for splitting.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of RegexChunking - splitting text based on regex patterns. While it doesn't mention the default pattern, this is a minor detail that doesn't affect the main relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the provided code sample, the AsyncWebCrawler class is used to initialize a crawler instance that is then utilized to run the specific crawl operation with the RegexChunking strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core relationship between AsyncWebCrawler and RegexChunking but misses the crucial detail that RegexChunking is the default chunking strategy used for splitting extracted text content",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method of the AsyncWebCrawler class is implicitly involved as it orchestrates the crawling operation that utilizes the RegexChunking strategy.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun uses RegexChunking but misses the key point about parameter validation and default value setting. While it captures the basic involvement, it omits the crucial implementation details mentioned in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the documentation snippet as it is used to create a crawler instance with: `async with AsyncWebCrawler(verbose=True) as crawler:`. This indicates direct implementation and usage within an example.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class is designed to be used with async context managers, as shown in the documentation example. While it doesn't detail the __aenter__/__aexit__ methods, it accurately captures the core usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly demonstrates the instantiation of a `CosineStrategy` class. It is used as the extraction strategy to process content during the crawler operation, representing a direct usage of this artifact.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used as an extraction strategy with the crawler, but misses the key aspect that it implements configurable parameters for semantic filtering, word count thresholds, and similarity thresholds which is central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example provided in the documentation shows the use of `AsyncWebCrawler` to execute the crawl operation with the specified extraction strategy. The async context management (`async with`) suggests the artifact being a key component of this functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of AsyncWebCrawler - its async context management and crawling capabilities with extraction strategy support. While less detailed than the ground truth, it correctly identifies the key components without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the documentation's code example, the method `arun` of the `AsyncWebCrawler` is called to perform the crawling task. This implies its implicit usage in the working of the provided example, as no direct reference to this method by name is in the text.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship that arun() is used in the documentation example, but misses crucial aspects about its role in processing extraction strategies and returning crawl results that are core to the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The variable `content` in the documentation example accesses the `extracted_content` attribute of the `CrawlResult` class returned by `arun`. This implies an implicit trace to the `extracted_content` attribute due to its use in handling the result of the crawling operation.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed as an attribute containing crawl results, but misses the key point about it storing semantically filtered and clustered text content",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The schema documented specifies the use of CSS selectors to extract data into a structured format, directly aligning with the JsonCssExtractionStrategy which extracts data using a provided schema with CSS selectors.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the schema and the JsonCssExtractionStrategy class, describing how CSS selectors are used to extract structured data. While it's less specific about BeautifulSoup implementation, it correctly conveys the main functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, which serves as a base class providing abstract methods for extracting data. This is not explicitly described in the snippet but inferred through class inheritance structure and typical usage in codebases.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship where ExtractionStrategy serves as a base class that enables inheritance and implementation of specific extraction methods. While it doesn't explicitly mention the parallel processing capabilities, it accurately describes the fundamental inheritance relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides example code directly invoking the `arun()` method of an `AsyncWebCrawler` object. This method is used to configure the crawler to handle dynamic content by taking parameters such as `url`, `wait_for`, `process_iframes`, etc.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship that arun() is used for dynamic content handling, but misses the crucial aspect that it implements this through **kwargs supporting custom JavaScript and wait conditions",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `arun()` method belongs to the `AsyncWebCrawler` class, which is implicitly referred to in the documentation when showing how to invoke the `arun()` method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between arun() and AsyncWebCrawler class, but misses the crucial aspect of dynamic content handling functionality that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet directly discusses the 'Cosine Strategy', describing its use of similarity-based clustering to extract content from web pages. This corresponds to the 'CosineStrategy' class which is implemented for that specific functionality.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy uses similarity-based clustering for content extraction, but fails to describe the specific 5-step workflow that is central to the ground truth explanation. The omission of this key procedural aspect represents a significant gap in understanding the implementation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' class is part of the extraction strategies in the system, which would logically extend the 'ExtractionStrategy' abstract base class to inherit its interface and common implementation patterns.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that CosineStrategy would extend the ExtractionStrategy abstract base class. While it's more concise than the ground truth and doesn't detail the specific methods, it captures the essential inheritance relationship accurately.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly instantiated and utilized in the example code to create a strategy for extracting content using LLMs. The snippet shows how the class is used by passing a provider, a schema derived from `ArticleContent`, and specific instructions for content extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how LLMExtractionStrategy is used for structured content extraction with LLMs, including the key aspects of using a schema, provider, and instructions. While it focuses more on the usage example than implementation details, it conveys the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` of the `AsyncWebCrawler` class is implicitly used in the snippet where the `crawler.arun()` is called with the `LLMExtractionStrategy`. This demonstrates its role in executing a web crawl based on the strategy.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between arun() and extraction strategy, but misses the key aspect of LLM-based content extraction according to schema and instructions that is central to the ground truth.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Implicitly related because the result from `crawler.arun()` returns `result.extracted_content`, which suggests the use of a `CrawlResult` object that encapsulates extracted content as per how the strategy and `arun()` operate. Although not directly mentioned, it is structurally inferred from the result handling.",
    "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult.extracted_content stores extraction results from the strategy, matching the core relationship described in the ground truth",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a code example that directly calls the `arun` method of the `AsyncWebCrawler` class. This is an explicit use case showing how the method is utilized to obtain a `CrawlResult`, which is further used to analyze links.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship that arun() is used to get a CrawlResult for link analysis, but misses the crucial aspect that it's the core crawling functionality that actually powers the link analysis features through asynchronous fetching and processing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "In the provided code snippet, `result.links` is accessed to analyze internal and external links, making this an explicit mention of the `links` attribute in context of its usage.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies links as an accessed property but misses the crucial detail that it's a dictionary organizing different link types with detailed link information",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates using the 'AsyncWebCrawler' class in its usage example, showing handling overlays and screenshots while performing a crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage and screenshot capability, but misses the core relationship regarding overlay removal being implemented through the arun() method's remove_overlay_elements parameter.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet implicitly uses 'arun()' method of 'AsyncWebCrawler', as the example in the text uses 'arun' for executing the web crawl with parameters such as 'bypass_cache', 'word_count_threshold', and 'remove_overlay_elements'.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method for web crawling with various parameters, but fails to explicitly acknowledge the core overlay removal functionality that is central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within 'AsyncWebCrawler', the 'AsyncPlaywrightCrawlerStrategy' is used internally to facilitate crawling strategies. The usage example in the snippet sets parameters relevant to strategies managed by this strategy class.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncWebCrawler using AsyncPlaywrightCrawlerStrategy internally, while the ground truth specifically describes the overlay removal functionality implemented in the remove_overlay_elements method. These are completely different aspects of the code.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' extends 'AsyncCrawlerStrategy', implying that functionalities related to strategies in 'AsyncWebCrawler' are influenced by this base class.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class, but focuses on AsyncPlaywrightCrawlerStrategy which isn't mentioned in the ground truth, and misses the core purpose of enabling overlay removal and content fitting functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation snippet describes accessing video and audio data through `result.media`, where `media` is explicitly referred to as holding video and audio elements. The code example showcases iteration over `result.media[\"videos\"]` and `result.media[\"audios\"]`, which directly relates to the `CrawlResult.media` attribute within the given model.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that media is a dictionary storing video and audio elements with their metadata that can be accessed and iterated over. It aligns with the ground truth's explanation of the data structure and purpose.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The given code snippet explicitly uses 'AsyncWebCrawler' in the context of simulating user behavior with the crawler. It instantiates the AsyncWebCrawler using a context manager, which shows direct usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in user simulation but presents it as explicit code usage rather than explaining that the class implements this functionality through its parameters and underlying strategy",
      "error_type": "incomplete_mechanism_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly called within the AsyncWebCrawler context, indicating its direct usage to execute the crawling process with options like 'simulate_user' and 'override_navigator'.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method for crawling, but incorrectly states these features are 'explicitly' called when they are actually implemented through **kwargs parameter implicitly",
      "error_type": "minor_misunderstanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly referenced in the snippet, 'AsyncPlaywrightCrawlerStrategy' is an implementation for the 'crawler_strategy' used by 'AsyncWebCrawler'. The strategies like 'simulate_user' and 'override_navigator' are likely implemented within this strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements user simulation functionality through simulate_user and override_navigator parameters. While it's less detailed than the ground truth, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the example code snippets where it is instantiated with custom user agent and headers to control how the crawler appears to websites.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that AsyncWebCrawler handles identity management through customization of user agent and headers. The predicted description captures the core functionality even if it doesn't mention the specific method (arun) used for implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method of `AsyncWebCrawler` is explicitly used in the example to run a crawl on a specified URL. This demonstrates the primary method of initiating a crawl within the crawler setup.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the main method for crawling but misses its key identity management functionality through user_agent and headers customization, which is the core focus of the ground truth.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Though not directly mentioned by name, the base class `AsyncCrawlerStrategy` is implicit because `AsyncWebCrawler` relies on the strategy pattern, with `AsyncPlaywrightCrawlerStrategy` using this interface for its operation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class using the strategy pattern, but misses the core point about its role in identity management and user agent customization that is central to the ground truth",
      "error_type": "missing_core_purpose"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet shows how to use the `AsyncWebCrawler` class to perform web crawling with specified options such as verbose logging and bypassing cache.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic web crawling functionality but misses the key dynamic content extraction capability through JavaScript execution and coordination with extraction strategies that is central to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` from `AsyncWebCrawler` is explicitly called in the example to execute the crawler with additional JavaScript code and wait conditions, connecting the process to LLM extraction strategies.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that arun() is used to execute the crawler with JavaScript and connects to LLM extraction, which aligns with the ground truth's explanation of how the method handles dynamic content execution and extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` class is instantiated with specific parameters in the example to handle the extraction of summarized content from crawled web pages. The usage elucidates its role in processing dynamic web content.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that LLMExtractionStrategy is used for extracting and summarizing content from web pages with configurable parameters, which aligns with how it's used in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the likely default strategy used by `AsyncWebCrawler` as implied by its initialization code. The strategy is crucial in handling specific web crawling tasks such as executing JavaScript and processing web pages using Playwright.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy's role in web crawling and JavaScript execution, but incorrectly states it is 'likely the default strategy' when this is not mentioned in the ground truth. It also misses emphasizing the dynamic content extraction capabilities that are central to the ground truth description.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions the `CosineStrategy` class and provides an initialization example with parameters matching the class constructor.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that the CosineStrategy class implements the documented configuration parameters with matching default values.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `CosineStrategy` class likely extends the `ExtractionStrategy` class as it implements extraction capabilities and fits the structure of such strategies described in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy extends ExtractionStrategy as an implementation class, which aligns with the ground truth's description of ExtractionStrategy as a foundation for implementing configurable extraction strategies like CosineStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides a code example using 'AsyncWebCrawler', indicating direct usage for executing a web crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler for web crawling, but misses the crucial aspect of integrated JavaScript execution and extraction strategy implementation described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' within 'AsyncWebCrawler' is directly invoked in the code example for defining a crawling operation with integrated JavaScript execution and waiting.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used in crawling operations, but misses the crucial aspect that it implements the JavaScript execution functionality through kwargs parameters. The prediction suggests direct invocation while the ground truth explains the implementation mechanism.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet includes creating an 'extraction_strategy' using 'JsonCssExtractionStrategy' to specify how commits are extracted, highlighting its role in data extraction from HTML content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy handles data extraction from HTML, but omits the crucial aspect of how it implements the schema-based structured extraction according to baseSelector and field selectors",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "Although not discussed in detail, the method 'kill_session()' from 'AsyncPlaywrightCrawlerStrategy' is implicitly used in the example code for managing session closure after crawling.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that kill_session() is used in the example code for session cleanup, which aligns with the ground truth's explanation of it being used to clean up browser resources at the end of crawling",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'CosineStrategy' and provides usage examples indicating how the 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k' parameters should be utilized. This shows that 'CosineStrategy' implements clustering based on these configurable parameters.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the documentation parameters and the CosineStrategy class's functionality, recognizing that it implements clustering based on the configurable parameters. While less detailed than the ground truth, it conveys the same essential relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is likely to extend 'ExtractionStrategy' because it is mentioned as a class in the same context (extraction strategies) and must be part of the interface that 'CosineStrategy' implements to perform document clustering.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy extends/inherits from ExtractionStrategy as a base class implementation, which aligns with the ground truth's explanation of ExtractionStrategy serving as a base template for implementing various extraction configurations.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned as being used in the example where it is instantiated within the `extract_openai_fees` function to perform crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage in the example but fails to capture its broader implementation of web crawling, HTML processing, and data extraction functionality described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is used to run the asynchronous web crawling task. Although not named in the document, it is critical for executing the crawling process described in the example.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method for web crawling, but misses significant functionality shown in the ground truth - particularly the extraction strategy, caching system, and return of CrawlResult that are demonstrated in the example.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` class is explicitly referenced in the code example to perform the extraction of model names and fees from the OpenAI pricing page.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for extraction from the OpenAI pricing page, but misses crucial aspects about its implementation of structured data extraction using Pydantic schema and provider-based LLM models",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the usage of the 'AsyncWebCrawler' class in a Python code example, indicating how to set up a crawler with a proxy and magic mode for anti-detection.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler usage with proxy and magic mode, but fails to mention that these parameters are passed through kwargs to AsyncPlaywrightCrawlerStrategy, which is a crucial implementation detail in the ground truth.",
      "error_type": "important_implementation_omission"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "While not explicitly named in the snippet, 'arun()' is the method being invoked on an 'AsyncWebCrawler' instance (`crawler.arun()`), demonstrating how to perform a crawl operation with specified parameters.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a method of AsyncWebCrawler that performs crawling operations, but misses the core functionality related to proxy and magic mode configuration described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example calling `arun()` method of a crawler object, which takes parameters related to timeouts and waiting configurations.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation shows an example of calling arun() with timeout and waiting parameters, which aligns with the ground truth showing these parameters are passed through kwargs to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is implicitly related as it contains the `arun()` method shown in the example. The class is responsible for the crawler's behavior which includes handling timeouts and waiting parameters.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles timeouts and waiting through arun(), but it doesn't explain that this happens via **kwargs being passed to the underlying crawler strategy, which is a crucial aspect of how the functionality is implemented.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document snippet explicitly mentions a CSS strategy for product listings using a schema. JsonCssExtractionStrategy likely implements such functionality by allowing the extraction of fields like 'name' and 'price' using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements CSS-based extraction using a schema for product listings, which aligns with the ground truth's explanation of how it processes HTML elements according to schema-defined selectors",
      "error_type": null
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document snippet explicitly references the use of LLMExtractionStrategy for extracting elements like title, content, author, and date in a news article. This strategy is mentioned with a specific provider and schema indicating it implements handling for large language models.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between LLMExtractionStrategy and article extraction functionality, including its use of schema and provider parameters for extracting article elements, which aligns with the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The document snippet explicitly describes using a Cosine Strategy for topic analysis with a semantic filter for 'technology trends'. This indicates the use of CosineStrategy for clustering and analyzing topics.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main purpose of CosineStrategy being used for topic analysis with semantic filtering and clustering, which aligns with the ground truth's explanation of content analysis functionality using cosine similarity and semantic filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as the subject of advanced usage scenarios, particularly dealing with nested HTML structures.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the prediction correctly identifies JsonCssExtractionStrategy and mentions nested structures, it misses the crucial implementation details about BeautifulSoup and schema-based processing that enable this functionality",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The provided schema example in the text is designed for structured extraction using CSS selectors, a functionality directly suited for a strategy like JsonCssExtractionStrategy which implements schema-driven extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy implements schema-based extraction using CSS selectors, which aligns with the ground truth's description of how the class uses BeautifulSoup's select() method to match CSS selectors defined in the schema",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy likely extends ExtractionStrategy as it fits within the overarching strategy paradigm of extracting meaningful content using structured methods, implied from the schema extraction focus present in the text.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base for extraction methods, but incorrectly specifies JsonCssExtractionStrategy and makes assumptions about implementation details not shown in the code",
      "error_type": "unsubstantiated_specificity"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation explicitly discusses the `RegexChunking` class, describing its functionality, parameters, and usage. The class implements text splitting using regular expressions, which aligns with the documentation example and explanations.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of RegexChunking as a text splitter using regular expressions, which aligns with the ground truth's explanation of using re.split() with patterns",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The `RegexChunking` class, as part of its implementation, extends the `ChunkingStrategy` class. While this is not explicitly mentioned in the documentation snippet, it is a necessary relationship for understanding the class hierarchy and its implementation context.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that RegexChunking extends/implements ChunkingStrategy, which aligns with the ground truth's explanation of the inheritance relationship and implementation requirement.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation demonstrates the use of the 'AsyncWebCrawler' class directly by creating an instance of it in the code snippet shown in the 'Quick Start' section. The example code explicitly imports and makes use of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used in the documentation, but misses the crucial point about the async context manager implementation that enables the 'async with' syntax",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly invoked as part of the usage example in the provided documentation snippet, showcasing how to asynchronously run a crawl using the specified URL.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is demonstrated in the documentation example, showing its async usage for web crawling. While it doesn't detail all the implementation aspects mentioned in the ground truth, it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The 'markdown' attribute of the result from 'arun' is used to print extracted content, suggesting the return type 'CrawlResult' and accessing its 'markdown' field, although it is not explicitly discussed in the documentation.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the markdown property/attribute contains extracted content that can be printed, capturing the same core relationship between the markdown field and its role in storing formatted content",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'arun()' method on an instance of 'crawler' to perform a crawl operation and retrieve the resulting data in various formats.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between arun() and the different output formats - that arun() is used to perform crawling and get results in various formats. While it doesn't detail the internal processing, it accurately represents the high-level relationship shown in the documentation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The 'html' attribute of 'CrawlResult' is explicitly mentioned in the documentation snippet as 'raw_html', representing the original HTML collected from a web crawl.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that 'html' is a string attribute that stores raw HTML content from web crawling, as shown in the documentation's Basic Formats section.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The 'cleaned_html' attribute is explicitly mentioned as 'clean_html' in the documentation, indicating its role in providing sanitized HTML output.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that cleaned_html provides sanitized HTML output, which aligns with the ground truth's explanation of it being a sanitized HTML version of the crawled webpage.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The 'markdown' attribute, accessed as 'markdown' in the snippet, denotes its explicit role in providing a standard markdown version of the crawl output.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the markdown attribute provides standard markdown output from the crawl, which aligns with the ground truth's explanation of the markdown field converting HTML to standard markdown format.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "Referenced in the documentation as 'fit_md', 'fit_markdown' provides filtered and most relevant content markdown, part of 'CrawlResult's attributes.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown provides the most relevant content in markdown format, which aligns with the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly instantiated in the documented code example within the text snippet. It is used for managing crawl sessions asynchronously.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's existence and asynchronous nature, but misses the crucial aspect of its functionality for custom extraction strategies and content filtering shown in the documentation example.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method AsyncWebCrawler.arun() is explicitly called in the code example to run the web crawl with a specified URL and extraction strategy.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of arun() but misses the crucial aspect of content filtering and extraction strategy implementation shown in the ground truth. The ground truth emphasizes the specialized functionality of content filtering based on criteria, which is a key part of the relationship.",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is explicitly mentioned and instantiated in the code snippet as the strategy for content extraction, using the provider 'openai/gpt-4o'.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy's presence and basic use but misses the core functionality of processing HTML through LLM models with specific instructions for targeted content extraction",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The extracted_content attribute of the CrawlResult is accessed in the code example to retrieve the tech-related items.",
    "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that extracted_content is used to access the extracted content (tech-related items in this case), which aligns with the ground truth explanation that it stores filtered content that is then parsed as JSON.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions using 'JsonCssExtractionStrategy' in the sample code with a defined schema to perform pattern-based extraction after interacting with a web page.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that JsonCssExtractionStrategy uses a schema for pattern-based extraction, which aligns with the ground truth's explanation of using BeautifulSoup and CSS selectors for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "In the documentation example, 'LLMExtractionStrategy' is used explicitly for content analysis, specifying the use of a provider, schema, and instruction which aligns with the strategy practiced for dynamic content.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy being used for content analysis with configurable provider, schema, and instruction parameters, which matches the implementation shown in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Though 'AsyncWebCrawler' is not directly mention, it's implicitly used in the code examples provided for executing the 'arun' method, which is significant for combining page interaction with structured extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship between AsyncWebCrawler and extraction strategies through the arun method, even though it's less detailed than the ground truth. Both descriptions highlight how AsyncWebCrawler enables structured extraction through its arun method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly mentions 'arun', a method used for initiating crawling with specific strategies, illustrating its use for both JsonCssExtractionStrategy and LLMExtractionStrategy.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of the arun() method working with extraction strategies, matching the ground truth's explanation of how it handles both JsonCssExtractionStrategy and LLMExtractionStrategy for web crawling.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'Magic Mode' feature in the documentation is demonstrated with the `AsyncWebCrawler` class. The text snippet explicitly shows code using this class to facilitate the 'Magic Mode' functionality with a method call that enables anti-detection features.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class is involved with Magic Mode functionality, but it oversimplifies and misses that Magic Mode is actually implemented through the crawler_strategy parameter and AsyncPlaywrightCrawlerStrategy rather than being a direct feature of AsyncWebCrawler.",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is explicitly used in the documentation snippet to perform a 'Magic Mode' crawl. This is central to implementing 'Magic Mode', as seen in the given code example.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but incorrectly states that it's 'explicitly' used for Magic Mode when the ground truth shows Magic Mode is actually enabled through an optional kwargs parameter",
      "error_type": "feature_mischaracterization"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` class is an implementing strategy that likely uses functionality for simulating human-like behavior and managing browser fingerprinting, as required in 'Magic Mode', even though it is not explicitly mentioned.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncPlaywrightCrawlerStrategy implements functionality for anti-bot features and human behavior simulation, which aligns with the ground truth's description of Magic Mode implementation. While the prediction is more general, it captures the core relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides an example of using the AsyncWebCrawler class, indicated by its direct instantiation and use within the provided code snippet.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship that the documentation shows usage of the AsyncWebCrawler class, but misses the crucial aspect that the code fully implements all the documented functionality including content filtering, processing, and cache control features shown in the example.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method arun() of AsyncWebCrawler is called in the provided code example, showing its usage pattern within the AsyncWebCrawler class.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used in AsyncWebCrawler, but misses the crucial aspect of how the method's implementation aligns with the documented usage patterns and parameters shown in the example.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The code snippet accesses 'links' through the CrawlResult object to retrieve internal links, signifying its practical application during a crawl operation.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that links are accessed through the CrawlResult object but misses the crucial information that the links dictionary specifically categorizes them into internal and external links",
      "error_type": "missing_key_detail"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The 'media' attribute of CrawlResult is used to iterate over images in the documentation example, implying its provision of media-related data post-crawl.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the media attribute stores and provides access to media-related data (specifically images) after crawling, which aligns with the ground truth's explanation of CrawlResult.media being used to store and access media elements like images.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The example checks the 'error_message' attribute from CrawlResult to handle failure scenarios, reflecting its inclusion in error diagnostics.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that error_message is used to provide failure information when success is false in the error handling flow.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of 'JsonCssExtractionStrategy' for pattern-based extraction, particularly for elements with repetitive patterns such as product listings. It provides a concrete example, using 'JsonCssExtractionStrategy' with a schema that targets specific elements on a webpage.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JsonCssExtractionStrategy as a pattern-based extraction tool for repetitive elements using CSS selectors and schema definitions, matching the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet demonstrates the use of 'AsyncWebCrawler' through the method call 'await crawler.arun(...)'. This is an implicit relationship as 'AsyncWebCrawler' is necessary for executing the extraction strategy within the broader crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler but misses the core relationship about pattern-based extraction using JsonCssExtractionStrategy for structured data extraction from repeating elements. It only mentions extraction strategy in a general sense.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is referenced in the snippet 'result = await crawler.arun(...)' indicating that the 'AsyncWebCrawler.arun()' function is used to execute the extraction strategy as part of a web crawling operation.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is used to execute extraction strategies during web crawling, which aligns with the ground truth's explanation of arun() processing web pages using extraction strategies like JsonCssExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' of the 'AsyncWebCrawler' class is explicitly mentioned in the documentation snippet. This is evidenced by the code example where it is used to perform a domain-based filter during a web crawl.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used for web crawling with domain-based filtering, even though it doesn't detail all the specific filtering options. The core relationship between the code and documentation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class implements the domain-based filtering capabilities demonstrated in the documentation snippet. Its constructor and usage context are implied indirectly due to the explicit mention of its 'arun()' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler implements domain-based filtering through its arun() method, which matches the core relationship in the ground truth. While it doesn't detail the specific parameters, it accurately identifies the main functionality and class relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet uses the `arun` method of `AsyncWebCrawler` to demonstrate controlling page load timeouts and delays before capturing content. The method is directly referenced in the example code `await crawler.arun(...)`.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() method as handling timing controls, but incorrectly suggests direct reference of timeouts in the code rather than explaining they are passed through **kwargs to crawler_strategy.crawl()",
      "error_type": "implementation_misunderstanding"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example demonstrates using an instance of `AsyncWebCrawler`, indicating that `arun` is a method from this class. This is implied from the presence of `await crawler.arun(...)`, where `crawler` is assumed to be an instance of `AsyncWebCrawler`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as a method of AsyncWebCrawler, but misses the core timing control functionality described in the ground truth - specifically the ability to control page timeouts and content capture delays through parameters.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the provided snippet, `AsyncWebCrawler` by default uses an instance of `AsyncPlaywrightCrawlerStrategy`, as seen in its constructor, linking the strategy to the `arun` method usage indirectly.",
    "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the link between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, while the ground truth describes the actual timing control functionality implemented through specific parameters in the crawl method. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "By inheritance, `AsyncPlaywrightCrawlerStrategy` implements `AsyncCrawlerStrategy`, which defines the abstract methods that `AsyncPlaywrightCrawlerStrategy` would use, linking back to the `arun` behavior indirectly.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship but focuses on implementation inheritance rather than the key timing control functionality enabled through the interface's kwargs parameter",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation explicitly states 'content = result.fit_markdown', indicating direct use of the 'fit_markdown' attribute from the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description identifies fit_markdown as an attribute correctly, it misses the key purpose described in the ground truth - that it's specifically for formatting blog posts and articles. This functional purpose is a crucial aspect of understanding the relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation states 'img for img in result.media[\"images\"]', clearly using the 'media' attribute to filter images based on score.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the media dictionary attribute is used for filtering images based on score, which aligns with the ground truth's explanation of the CrawlResult.media dictionary property's purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet references 'link for link in result.links[\"internal\"]', indicating usage of the 'links' attribute to analyze internal content links.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of using the links attribute to analyze internal content links, which aligns with the ground truth's explanation of CrawlResult.links enabling internal link filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly invoked in the snippet 'result = await crawler.arun(...)' as part of the content cleaning process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() method usage but misses its key relationship to the best practices - it only mentions content cleaning while ignoring media processing and link handling aspects outlined in the ground truth.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation explicitly mentions using 'crawler.crawler_strategy.kill_session(session_id)' to clean up sessions in the resource management context.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session() is used for resource cleanup/management as shown in the documentation, even though it doesn't detail the specific cleanup actions.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation contains examples of using 'crawler.arun()' for session state management. This implies the use of the 'AsyncWebCrawler' class, which provides the 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between the documentation and AsyncWebCrawler's arun method, but misses the crucial aspect of session management through the session_id parameter that enables state maintenance across page visits",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly demonstrated in documentation examples showing session management state through web interactions.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies session management but misses the key point about session_id storage from kwargs enabling stateful crawling across requests",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows the usage of the `arun` method with different extraction strategies. This indicates that `arun` is used to execute the web crawling process with specified extraction strategies.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() method is used with different extraction strategies, which aligns with the ground truth's explanation of how the method implements combining strategies through its extraction_strategy parameter.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "Though not named, the mention of 'CSS strategy' implies the use of a CSS-based extraction strategy, which is implemented by the `JsonCssExtractionStrategy`.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements CSS-based extraction, which aligns with the ground truth's explanation of it being part of a combinable strategy using CSS selectors for data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document demonstrates using 'LLM' strategy, directly linking it to the `LLMExtractionStrategy` class that implements the strategy using LLMs for semantic analysis.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy as using LLMs for semantic analysis, but misses the crucial aspect of it being part of a combined strategy workflow where it can process content after CSS-based extraction.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly uses the `AsyncWebCrawler` class to initialize a web crawler instance, as demonstrated in the `async with AsyncWebCrawler(verbose=True) as crawler:` statement.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler class usage but misses the key relationship about the wait_for parameter support through the arun method. It describes initialization but not the functionality described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is implicitly used when calling `await crawler.arun(...)` in the documentation snippet. It is central to executing the crawl operation with specified parameters.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as being used for crawl execution, but misses the key relationship around the wait_for parameter functionality and its role in controlling page loading conditions through crawler_strategy.crawl()",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "As the `AsyncWebCrawler` utilizes a strategy pattern and by default relies on `AsyncPlaywrightCrawlerStrategy`, which implements `AsyncCrawlerStrategy`, this abstract class implicitly forms part of the crawling strategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as part of the strategy pattern, but misses the key relationship with wait_for functionality described in the ground truth. The main purpose of controlling page load completion conditions is not mentioned.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The documentation's `kill_session` functionality hints at the default strategy used, `AsyncPlaywrightCrawlerStrategy`, which explicitly handles tasks like session management.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on session management, which is present in the code but misses the main focus of the ground truth - the wait_for functionality and smart_wait method for handling dynamic content loading. While not entirely wrong, it highlights a less relevant aspect of the code-documentation relationship.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session` method of `AsyncPlaywrightCrawlerStrategy` is explicitly called using `await crawler.crawler_strategy.kill_session(session_id)`, indicating its role in terminating session-specific resources.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of kill_session being used to terminate session resources, which aligns with the ground truth's explanation of cleaning up browser resources after crawling is complete.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is explicitly instantiated and used for content extraction from HTML using CSS selectors, evident from `extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of JsonCssExtractionStrategy as a class that extracts content using CSS selectors according to a schema. While the predicted description is less detailed, it correctly identifies the main purpose and usage without contradicting the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly refers to the `AsyncWebCrawler` class as supporting session-based crawling, highlighting its role in maintaining a persistent browser session.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the AsyncWebCrawler class implements session-based crawling functionality for maintaining persistent browser sessions. The predicted description captures the core relationship even though it doesn't mention specific implementation details like the session_id parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text mentions using the same `session_id` across multiple `arun` calls, implying the method's role in maintaining session state during crawling processes.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of using session_id across multiple arun calls to maintain state, which aligns with the ground truth's explanation of how the method implements session-based crawling via the session_id parameter",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is used by `AsyncWebCrawler` as the default strategy, indicating its role in session management.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly indicates AsyncPlaywrightCrawlerStrategy's relation to AsyncWebCrawler, but misses the core aspect that it directly implements session management through its sessions dictionary and kill_session() method",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "As the base class for `AsyncPlaywrightCrawlerStrategy`, it implicitly contributes to the mentioned capabilities of session-based crawling by defining the structure for strategies that involve sessions.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies it as a base class for crawling strategies but understates its direct role in defining the session-based crawling interface, presenting it as more implicit than the ground truth indicates",
      "error_type": "underspecification"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides a code sample using `arun` method of the `AsyncWebCrawler` class with ant-bot options, specifically `simulate_user` and `override_navigator`. This demonstrates the usage of the method within an anti-bot configuration context.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the arun() method and anti-bot configuration options, matching the ground truth's explanation of how the method accepts arguments for controlling anti-bot features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The document mentions options such as `simulate_user` and `override_navigator`, which are operational aspects likely implemented in the strategy pattern that `AsyncPlaywrightCrawlerStrategy` forms a part of. The document indirectly refers to features that a strategy would control.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the class implements anti-bot options through parameters that control automation masking and behavior simulation, even though it frames it in terms of strategy pattern. The core functionality relationship is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'LLMExtractionStrategy' class as the core component for utilizing different LLM providers such as OpenAI, Hugging Face, and Ollama for data extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the LLMExtractionStrategy class and multiple LLM providers, matching the ground truth's explanation of provider flexibility and authentication handling through the constructor parameters.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses 'AsyncWebCrawler' in the example method 'crawl_protected_site', suggesting implicit usage of the class for a crawling task.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler in the example but misses the crucial aspect that the class specifically implements protected site crawling functionality with special parameters",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is explicitly mentioned in the text snippet as a part of the crawling process in the 'crawl_protected_site' function.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes arun's usage in crawling but misses crucial aspects of its implementation - the error handling, caching, and customizable parameters that enable protected site crawling, which are central to the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is specifically called in the example snippet provided in the documentation to perform a crawl operation. It is directly used to demonstrate how to obtain the `CrawlResult` object for error checking.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's usage but misses the core error handling relationship - it focuses on the method call rather than how it enables error checking through the returned CrawlResult object",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The `CrawlResult` class is implicitly traced as it is the return type of the `arun` method. The documentation refers to checking `result.success` and `result.error_message`, which are attributes of `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult provides the attributes used in the documentation's error handling example, even if it describes it slightly differently by mentioning implicit tracing.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The `success` attribute of `CrawlResult` is referenced in the documentation snippet where it checks if the result was successful: `if not result.success:` indicating its usage.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the success boolean is used to check if a crawl operation succeeded through the documented if-check pattern, which aligns with the ground truth's description of its error handling purpose",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "Within the snippet, `result.error_message` is accessed to provide error details in the event of a failed crawl, indicating direct usage of the `error_message` attribute from `CrawlResult`.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that error_message is used to provide error details when crawls fail, with the same core relationship between error reporting and failed crawls being preserved",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The `status_code` attribute from `CrawlResult` is fetched and printed in the documentation snippet to provide additional error context when a crawl fails.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that status_code is used for error handling context in failed crawls, which aligns with the ground truth's explanation",
      "error_type": null
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation 'SlidingWindowChunking' explicitly describes the class `SlidingWindowChunking`, outlining its approach, use cases, parameters, and provides an example of usage.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately describes what the documentation contains, but fails to explain the relationship between the code and documentation - specifically how the code implements the sliding window algorithm described in the docs",
      "error_type": "missing_core_relationship"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet directly instantiates an 'AsyncWebCrawler' with configuration parameters such as 'headless', 'verbose', and 'sleep_on_close', clearly indicating its usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of AsyncWebCrawler with configuration parameters, but misses the key implementation detail that these parameters are passed through kwargs to an underlying AsyncPlaywrightCrawlerStrategy",
      "error_type": "omitted_implementation_detail"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the provided example, 'crawler.arun(url=\"https://example.com\")' invokes the 'arun' method of the 'AsyncWebCrawler' class, showcasing how to run the crawler for a specific URL.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and its basic URL parameter, but misses the crucial aspect that this is an implementation that works with the documented configuration parameters (headless, verbose, sleep_on_close) passed through the constructor.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the snippet, 'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy' by default if no other strategy is specified, implicating its use in the given configuration context.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies a relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, it makes an unsupported assumption about default strategy usage that isn't evidenced in the code. The ground truth more accurately describes how the Strategy class implements the documented configuration options.",
      "error_type": "unsubstantiated_assumption"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example that directly calls the 'arun' method of the AsyncWebCrawler class. This involves the use of the method to perform content cleaning with parameters like word_count_threshold, excluded_tags, and remove_overlay_elements.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that the arun() method implements content cleaning functionality through parameters like word_count_threshold and excluded_tags. The predicted description accurately captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The example in the documentation uses the 'result.cleaned_html' and 'result.markdown', indicating that the 'arun()' method returns a 'CrawlResult' object, which contains these attributes.",
    "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult stores cleaning outputs through its attributes, even though it mentions fewer specific fields than the ground truth. The core relationship between the class and its content cleaning functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation explicitly mentions 'cleaned_html', which is an attribute of the 'CrawlResult' object returned by the 'arun()' method, showing how to access the cleaned HTML content.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies cleaned_html as an output attribute, but misses the core purpose of storing sanitized HTML after content cleaning steps described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation explicitly mentions 'markdown', indicating how to access the cleaned markdown version of the content using this attribute from the 'CrawlResult' returned by the 'arun()' method.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the markdown attribute provides access to a cleaned markdown version of the content, which aligns with the ground truth's explanation of the markdown property providing cleaned content after noise removal.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly mentions `arun` method in the usage example. It's a part of the `AsyncWebCrawler` class, which is used to perform the crawling operation including processing of iframes.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies arun() as the core method for crawling and mentions iframe processing, which aligns with the ground truth's explanation of how arun() handles iframe processing through its parameters. While the predicted description is less detailed about the specific parameters, it captures the main relationship correctly.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly mentioned in the text, `AsyncWebCrawler` is clearly the class instance (`crawler`) referred to in the snippet where `arun` is called. It suggests that `arun` is a method belonging to this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun is a method of AsyncWebCrawler, but misses the main point about iframe processing functionality which is the core relationship being described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "`AsyncWebCrawler` uses an instance of a `crawler_strategy`, which defaults to `AsyncPlaywrightCrawlerStrategy`. The option `process_iframes` and `remove_overlay_elements` in the `arun` method suggests operations that are implemented in the strategy class, specifically for handling iframe content.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles iframe content processing through configuration options, which aligns with the ground truth's explanation of how the strategy processes iframes by extracting and replacing their content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly mentioned and used in the provided code example as the primary class for creating a web crawler instance.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies AsyncWebCrawler as the main class, it omits the crucial asynchronous functionality and context manager pattern that are core aspects mentioned in the ground truth. The predicted description is overly simplistic.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method of the AsyncWebCrawler class is explicitly invoked in the example to perform the crawling operation.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses crucial aspects of its functionality including content extraction, caching, and returning a CrawlResult object. The description is oversimplified.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "CrawlResult.markdown is accessed through the result variable in the example. The arun method returns a CrawlResult object, and the markdown field is used immediately after.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that markdown is accessed as a field on the result object returned by arun(), which matches the ground truth's explanation of the relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet mentions the `arun()` method, which is defined in the `AsyncWebCrawler` class. It relates implicitly because the example shows how to call `arun()` on `crawler`, indicating an instance of `AsyncWebCrawler`.",
    "ground_truth_relationship": "The `arun()` method returns a CrawlResult object containing the documented properties like html, cleaned_html, markdown, fit_markdown, success, status_code, media and links, which are populated during the web crawling and processing pipeline.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is a method of AsyncWebCrawler, but fails to capture its core functionality of returning a CrawlResult with specific properties. The ground truth focuses on the output structure and properties, which is a crucial aspect of the relationship that's missing from the prediction.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet directly calls and discusses the `arun()` method of `AsyncWebCrawler`, showing how it processes a URL and returns a `CrawlResult`.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that arun() processes a URL and returns a CrawlResult object. While it omits specific details about the properties like media and links that are mentioned in the ground truth, it correctly identifies the main functionality and return type.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation explicitly mentions that `arun()` returns a `CrawlResult` object. It lists several properties of `CrawlResult`, directly linking it to the method's functionality.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is returned by arun() and contains the properties shown in the documentation. While it doesn't enumerate all fields explicitly, it captures the core relationship between the class and its usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The example showcases accessing the `html` attribute of the `CrawlResult` object to get the raw HTML content.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that html is an attribute of CrawlResult that contains raw HTML content, which aligns with the ground truth's explanation of it being a string property storing raw HTML content.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The example demonstrates using the `cleaned_html` attribute of the `CrawlResult` object for accessing cleaned HTML content.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that cleaned_html is used to access cleaned HTML content, but misses the crucial aspect that it's an Optional[str] property",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet directly points to `markdown` as an attribute of the `CrawlResult` for obtaining the markdown version of the content.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that markdown is an attribute/property of CrawlResult that provides the markdown version of the content, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet specifically shows accessing `fit_markdown` to get the most relevant content in markdown via the `CrawlResult` object.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown provides the most relevant content in markdown format via CrawlResult, which aligns with the ground truth's core meaning",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The example in the text explicitly references `success` as a property of the `CrawlResult` object to verify if crawling was successful.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that success is a property of CrawlResult that indicates whether the crawl succeeded, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "Mentions `status_code`, used to retrieve the HTTP status from the `CrawlResult` object.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that status_code is used to retrieve/store the HTTP status from the CrawlResult, which aligns with the ground truth. While it doesn't mention all implementation details, the main functionality is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Depicts `media` as part of the `CrawlResult` attributes which holds a dictionary of extracted media.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies 'media' as being a property/attribute that holds extracted media data from the crawl results, which aligns with the ground truth's explanation of it being a dictionary for storing media elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation lists `links` as part of the `CrawlResult` content to retrieve internal and external links from the result.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that links are stored in the CrawlResult object and accessed through the links property to get internal/external links, which aligns with the ground truth relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document explicitly mentions 'CSS Strategy' which corresponds directly to the 'JsonCssExtractionStrategy' class, recognizing structured HTML content extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements the CSS Strategy from the documentation and recognizes its purpose for structured HTML content extraction, which aligns with the ground truth's core message.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text explicitly advises using 'Cosine Strategy' for mixed or complex content and optimal content relevance, which is implemented by the 'CosineStrategy' class.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for mixed/complex content and content relevance, which aligns with the ground truth's description of using cosine similarity and hierarchical clustering for analyzing mixed content with moderate performance.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document mentions choosing 'LLM Strategy' for natural language text and semantic accuracy, corresponding directly to the 'LLMExtractionStrategy' class.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that LLMExtractionStrategy implements the LLM Strategy for natural language text processing and semantic understanding, which aligns with the ground truth's main points",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly references the 'JsonCssExtractionStrategy' as a strategy for using CSS selectors for extraction purposes. Evidence is seen in the instruction to use browser developer tools and verifying selectors, which aligns with the functionality of 'JsonCssExtractionStrategy' to select and extract data from HTML content based on CSS selectors.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship between the code and documentation - that JsonCssExtractionStrategy uses CSS selectors to extract data, and the documentation provides relevant guidance for using these selectors effectively.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' extends 'ExtractionStrategy', as evidenced by the use of super().__init__() in its constructor. This suggests that 'JsonCssExtractionStrategy' inherits common extraction functionalities defined in 'ExtractionStrategy', and thus 'ExtractionStrategy' is indirectly tied to elements discussed in the documentation snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on inheritance relationship of JsonCssExtractionStrategy, while ground truth emphasizes the base class's foundational role in providing error handling and parallel processing. Though related, it misses key functional aspects.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls the 'arun' method of a 'crawler' instance, mapping to 'AsyncWebCrawler.arun()'. The Python code snippet demonstrates usage of this method.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called on a crawler instance, but misses the core functionality of processing HTML and converting it to markdown format, which is a crucial aspect mentioned in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The result of calling 'arun' in the documentation includes accessing 'markdown' on the result, which matches 'CrawlResult.markdown'. This attribute is used implicitly to demonstrate the returned result's interface.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that markdown is accessed on the result object, but misses the key aspect that it stores HTML-to-Markdown converted text output as described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides code examples explicitly showing how to use the 'AsyncWebCrawler' class with different browser types to crawl URLs.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler can work with different browser types, but misses the key implementation detail that this is done through the crawler_strategy parameter and PlaywrightCrawlerStrategy configuration. The prediction presents it as a more direct browser selection.",
      "error_type": "implementation_mechanism_omitted"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The asynchronous usage examples within the documentation implicitly use the 'arun' method of 'AsyncWebCrawler' to crawl a URL.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method's usage with AsyncWebCrawler but misses the key relationship about how browser selection is enabled through the crawler_strategy object and browser_type initialization",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class defaults to using 'AsyncPlaywrightCrawlerStrategy', as suggested by the examples where browser types are specified (indicating internal adjustment of strategy).",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text focuses on AsyncWebCrawler's default strategy, while the ground truth describes how AsyncPlaywrightCrawlerStrategy implements browser selection. While both discuss browser types, the relationship focus is different.",
      "error_type": "incorrect_relationship_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The 'CosineStrategy' class is explicitly mentioned and discussed directly in the documentation snippet. The snippet includes usage examples of 'CosineStrategy' where various parameters such as 'word_count_threshold', 'top_k', 'semantic_filter', 'sim_threshold', and 'max_dist' are configured, indicating its use case for handling content clustering strategies.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that CosineStrategy implements configurable parameters (word_count_threshold, sim_threshold, max_dist etc.) which align with the documentation's recommendations for different content types and use cases.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions using 'CosineStrategy' for various use cases like Article Content Extraction, Product Review Analysis, and Technical Documentation. It provides Python code examples demonstrating its instantiation with different parameters to fit specific needs like 'semantic_filter', 'word_count_threshold', 'top_k', and 'sim_threshold'.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that CosineStrategy is a configurable text extraction class that uses cosine similarity and can be adapted for different use cases by adjusting parameters like semantic_filter, word_count_threshold, top_k, and sim_threshold. The predicted description correctly identifies the main functionality and use cases shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the examples, 'arun' is used to execute the crawling process with the 'CosineStrategy' object as its 'extraction_strategy' parameter. Though 'arun' is not explicitly discussed, its implementation is inferred as an application point for the strategy based on its usage in the provided snippet.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies arun's core purpose of executing crawling with extraction strategies, which aligns with the ground truth's explanation of how it implements the use cases by accepting flexible extraction strategy parameters. While the predicted description is less detailed, it captures the essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is directly referenced in the code example, demonstrating its `arun` method to execute JavaScript code.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that AsyncWebCrawler uses the arun method, it fails to mention that js_code can be either a single command or list of commands, which is a significant aspect of the JavaScript execution functionality shown in the ground truth.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` is directly shown in the example code executing JavaScript commands on the target webpage.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is involved with JavaScript execution, but misses that it handles this through **kwargs and crawler_strategy.crawl() rather than direct execution. It oversimplifies the implementation.",
      "error_type": "oversimplification_of_mechanism"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Though not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the likely strategy employed by `AsyncWebCrawler` for crawling, including support for JavaScript execution.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles JavaScript execution, but it frames this as an implied relationship with AsyncWebCrawler rather than describing the direct JavaScript execution capabilities through js_code parameter and page.evaluate() as detailed in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation directly mentions and describes the `JsonCssExtractionStrategy` class as a tool for extracting data using CSS selectors. The usage example in the snippet demonstrates how to instantiate this class with a schema and use it for data extraction. This establishes a direct implementation relationship.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that JsonCssExtractionStrategy implements CSS-based data extraction functionality described in the documentation. While it's less specific about BeautifulSoup implementation details, it captures the main purpose and relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example in the snippet shows the use of the `arun()` method on an `AsyncWebCrawler` object to perform a crawl using the `JsonCssExtractionStrategy`. While `AsyncWebCrawler` is not explicitly discussed, the code implies the method's usage within the data crawling task.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between arun() and JsonCssExtractionStrategy - that arun() uses the extraction strategy as part of the crawling process. While the predicted description is less detailed, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly describes using an 'AsyncWebCrawler' to perform web crawling tasks with an extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of AsyncWebCrawler and extraction strategy, but misses crucial functionality related to caching, processing URLs, and the specific capabilities of the JsonCssExtractionStrategy mentioned in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet includes a usage example of the 'arun' method of the 'AsyncWebCrawler' class to execute the crawling process and apply the extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic purpose of arun() for web crawling and extraction, but misses crucial aspects like caching handling, structured result formatting, and the asynchronous nature that enables the documented JSON workflow",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation mentions explicitly creating a 'JsonCssExtractionStrategy' object with a schema to initialize the extraction process.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic initialization with schema but misses the core functionality of processing HTML with BeautifulSoup and extracting structured data according to field definitions",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation explicitly mentions the `FixedLengthWordChunking` class and describes its functionality as splitting text into chunks based on a fixed number of words. The example code directly demonstrates instantiation and usage of this class.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of splitting text into fixed-length word chunks, matching the ground truth's implementation description",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "`FixedLengthWordChunking` extends `ChunkingStrategy`, as it implements a strategy for chunking text into fixed lengths using abstract methods defined in `ChunkingStrategy`. This is implied by the hierarchical relationship in Python classes and not explicitly stated in the doc.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that FixedLengthWordChunking implements the abstract ChunkingStrategy class as a concrete implementation for word-based text chunking",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation shows usage examples of the AsyncWebCrawler class for setting up a crawler with proxy configurations. This class is explicitly mentioned in the text snippet through `async with AsyncWebCrawler(...)`.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that AsyncWebCrawler supports proxy configuration functionality, even though it focuses on usage examples rather than implementation details. The high-level relationship is consistent with the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the AsyncWebCrawler class is implicitly demonstrated in the examples `result = await crawler.arun(url=\"https://example.com\")` where the documentation explains how to perform crawling tasks.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic existence and usage of the arun method but misses the core relationship about how it handles proxy configurations through crawler_strategy.crawl()",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler likely uses AsyncPlaywrightCrawlerStrategy internally by default when no other strategy is provided, although this usage is not directly shown in the documentation snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description makes an unsupported claim about AsyncWebCrawler using AsyncPlaywrightCrawlerStrategy by default, while the ground truth focuses on how AsyncPlaywrightCrawlerStrategy implements proxy support through ProxySettings objects. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a usage example of calling the 'arun' method on a 'crawler' instance, highlighting its role in executing a web crawling task using the provided URL and extraction strategy, particularly in the context of handling URLs for crawling.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method's basic purpose for web crawling but misses the crucial error handling focus that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The 'extracted_content' attribute of 'CrawlResult' is directly referenced when checking if the result contains any content. This snippet exemplifies handling the outcome of the crawl operation executed by 'arun'.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic checking of extracted_content but misses the key semantic matching and strategy aspects mentioned in the ground truth",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In the error handling section, there is a direct reference to 'result.error_message', indicating its use for debugging or logging purposes when extraction fails.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that error_message is used for debugging/logging when extraction fails, which aligns with the ground truth's explanation of storing and displaying failure details via result.error_message.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The 'Cosine Strategy' is mentioned as a strategy option that is effective under specific content structure conditions, aligning with the text's advice on when to apply this strategy.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy's applicability for inconsistent content structures, but misses key technical aspects like cosine similarity and hierarchical clustering that are central to how it achieves semantic understanding according to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet implies the existence of an 'AsyncWebCrawler' instance used to call 'arun()', which coordinates web crawling tasks and uses different strategies.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements comprehensive error handling through try-catch blocks in its arun() method, which directly corresponds to the documented error handling pattern for managing crawler execution failures and content extraction issues.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler and its role in coordinating crawling tasks, but misses the core focus on error handling which is the main relationship described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` class is explicitly mentioned in the documentation as the primary implementation used for LLM-based data extraction. It is introduced with context on its functionality and usage for semantic understanding of web content.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the existence of LLMExtractionStrategy and its high-level purpose, but misses crucial aspects about its implementation including the extraction process, schema handling, and chunking functionality that are core to how it works with language models.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `AsyncWebCrawler.arun()` method is implicitly involved as it is shown in the usage example where an `LLMExtractionStrategy` instance is passed to this method to perform crawling with a specified extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() works with LLMExtractionStrategy but misses explaining its core crawling functionality and role in content retrieval and processing",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is a subtype of `ExtractionStrategy`, which is not directly mentioned but is part of the class hierarchy, underpinning the behavioral foundation of the LLM extraction method.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is a subtype of ExtractionStrategy and inherits its foundation. While it's more concise than the ground truth, it captures the core inheritance relationship without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is directly invoked in the provided code snippets documented for media processing and handling lazy-loaded content. The first code snippet includes a call to 'arun' that returns a 'result', which is integral to accessing media data such as images and their metadata.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as being used for media processing, but fails to capture its broader role in implementing core crawling functionality with caching, screenshots, and customizable parameters for media content handling",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "In the code snippet from the documentation, the 'result' object is used to access media data, such as image metadata. Although not directly mentioned, 'result' is an instance of 'CrawlResult', implying the usage of this class to handle and store crawling results.",
    "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CrawlResult is used to store crawling results and handle media data, but misses explaining the specific media processing functionality and structure through the media dictionary field that stores detailed metadata like source, alt text, context, etc.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation explicitly accesses 'result.media' to retrieve image details, indicating direct use of the 'media' attribute within the 'CrawlResult' class in the context of handling and processing media content.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the media attribute is used to store and access media content/metadata within CrawlResult, which aligns with the ground truth's explanation of the media dictionary storing media-related data and metadata.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation snippet explicitly mentions 'result.cleaned_html' indicating that the 'cleaned_html' attribute of CrawlResult is accessed in the example provided. The emphasis is on showcasing the cleaned HTML which is a key feature being explained in the provided documentation.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that cleaned_html contains sanitized/cleaned HTML content after processing, with the predicted description accurately mentioning it as an attribute accessed via result.cleaned_html",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code within the documentation snippet shows 'await crawler.arun(...)'. This demonstrates the use of the 'arun' method on a crawler instance, indicative of its role in initiating the crawling process leading to the resulting 'CrawlResult' showcasing 'cleaned_html'.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and its connection to the crawler, but misses the core functionality of HTML cleaning and sanitization that is central to the ground truth description.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet includes `AsyncWebCrawler` as a class to be used within an asynchronous context manager (`async with`) for crawling web pages. This indicates explicit utilization of the `AsyncWebCrawler` for capturing screenshots.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's async context manager functionality but doesn't mention the core screenshot functionality described in the ground truth - specifically the base64 encoding and return format of screenshots.",
      "error_type": "key_feature_omission"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the provided snippet, the method `arun` is explicitly called on an instance of `AsyncWebCrawler`. This shows direct interaction with `arun` to execute the web crawling process with the `screenshot` parameter set to `True`.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the screenshot functionality being implemented through arun() but misses crucial details about the caching mechanism and how the screenshot data is processed/returned in CrawlResult for base64 decoding",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The result object from `arun` with the attribute `screenshot` is accessed to check if a screenshot is captured successfully. This is implied through the snippet by checking `result.screenshot` to process the screenshot data.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the screenshot attribute stores screenshot data accessed via result.screenshot, even though it doesn't explicitly mention base64 encoding.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly instantiates `AsyncWebCrawler` within a context manager (`async with AsyncWebCrawler() as crawler:`), indicating the use of this class for managing web crawling sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager functionality but omits the crucial aspect of its support for different extraction strategies (JsonCssExtractionStrategy and LLM-based) which is a core part of the relationship shown in the documentation example.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet specifies `JsonCssExtractionStrategy(article_schema)` as a parameter to `crawler.arun`. This strategy is used for structured extraction as detailed in the documentation.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for structured extraction with a schema, which aligns with the ground truth's explanation of how it implements structured extraction using CSS selectors and schema structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "Referred explicitly as `LLMExtractionStrategy` in the example with a specific provider and schema for extracting semantic analysis from the article content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that LLMExtractionStrategy is used for semantic analysis with custom providers and schemas as shown in the documentation example. Both descriptions align on the fundamental purpose and implementation context.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is invoked in the provided example to retrieve structured content using `JsonCssExtractionStrategy` and semantic analysis using `LLMExtractionStrategy`.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used to retrieve structured content using different extraction strategies (JsonCssExtractionStrategy and LLMExtractionStrategy), which aligns with the ground truth's description of arun() executing core crawling functionality with different extraction strategies for pattern-based and LLM-based content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation and usage of `AsyncWebCrawler` in the context of maintaining session state.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the use of AsyncWebCrawler and mentions sessions, but misses the key aspect that session_id is specifically used for state maintenance between requests and stored in CrawlResult objects.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is explicitly mentioned in the example usage within the snippet when making requests with session state.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that arun() is used with session management, it fails to explain how the session_id parameter is handled internally or its role in state persistence. The ground truth provides a more complete explanation of the session management implementation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session()` method is explicitly called in the example to terminate the session, indicating its role in session lifecycle management.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies kill_session's role in session termination but omits the crucial implementation details about closing both page and context objects and cleaning up the sessions dictionary that are core to understanding its functionality",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned, `AsyncPlaywrightCrawlerStrategy` implements features utilized by `AsyncWebCrawler`, evident from `kill_session()` usage.",
    "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements session management functionality used by AsyncWebCrawler, which aligns with the ground truth's explanation of session management through the sessions dictionary.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation demonstrates the use of the 'arun()' method of an instance of 'crawler'. This is explicitly shown in examples where JavaScript commands are passed as 'js_code' parameter to 'arun()'.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method being used but fails to mention how js_code is handled internally via kwargs and crawler_strategy.crawl(). It describes the interface but misses the implementation aspect.",
      "error_type": "missing_implementation_details"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, the 'AsyncPlaywrightCrawlerStrategy' class is the likely implementation of the strategy pattern used by 'AsyncWebCrawler'. It contains methods for handling Playwright interactions, such as executing JavaScript code on page objects.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the class as handling Playwright interactions and JavaScript execution, but incorrectly suggests it's likely an implementation of a strategy pattern without evidence. The ground truth focuses specifically on the JavaScript execution functionality through the crawl() method.",
      "error_type": "unsubstantiated_assumption"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncCrawlerStrategy' provides the abstract interface which 'AsyncPlaywrightCrawlerStrategy' extends. The 'arun()' method in 'AsyncWebCrawler' may rely on methods defined abstractly in 'AsyncCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies AsyncCrawlerStrategy as an abstract interface, but misses the key point about JavaScript execution capability shown in the ground truth",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet refers to the 'crawler' object invoking the 'arun' method. This aligns with 'AsyncWebCrawler', which is the class implementing such functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the crawler object uses the arun method of AsyncWebCrawler class to retrieve HTML content, which matches the core functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a usage example calling 'arun'. This method in 'AsyncWebCrawler' implements functionality to retrieve HTML, matching the documented example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the arun method implements HTML retrieval functionality matching the documented example, even though it doesn't detail all implementation specifics about unmodified content and CrawlResult.html property.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result of 'arun', as demonstrated in the documentation, involves accessing an '.html' attribute. 'CrawlResult' is the return type of the 'arun' method containing the 'html' field, thereby implicitly involved.",
    "ground_truth_relationship": "The CrawlResult class stores raw HTML content in its 'html' field, matching the documentation's description of preserving unmodified webpage content for analysis and debugging purposes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that CrawlResult stores raw HTML content through its html field, which matches the documentation's purpose of preserving unmodified webpage content",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The documentation outputs HTML using 'result.html', indicating usage of the 'html' attribute within the 'CrawlResult' class as part of its interface.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the html attribute/property provides access to HTML content through result.html. While it omits some details about the HTML being unmodified and used for processing/debugging, these are minor omissions that don't change the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates using the JsonCssExtractionStrategy by initializing it with a schema and using it in a call to a crawler's run method.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy uses a schema and is used with a crawler, but misses the key aspects of pattern-based extraction from repeating HTML elements and recursive nested data extraction using CSS selectors",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Although not explicitly mentioned, the method `arun` of AsyncWebCrawler is used in an example where it receives an instance of JsonCssExtractionStrategy and a URL for potential extraction of content.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun accepts JsonCssExtractionStrategy but omits the crucial detail that it actively processes the schema to extract structured data according to defined selectors",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions and demonstrates the use of the 'AsyncWebCrawler' class by instantiating it in the 'main' function for performing crawl operations.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions the AsyncWebCrawler class but misses the crucial caching functionality which is the main focus of the documentation and implementation. It only describes instantiation and crawling, without mentioning the cache behavior.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' of the 'AsyncWebCrawler' class is explicitly called in the example with the 'url' parameter to demonstrate caching and bypassing cache.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the key relationship that arun() handles URL crawling with caching functionality, which aligns with the ground truth showing caching implementation and demonstration",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The example uses 'result1' and 'result2' to hold the outputs from 'arun()', indicating that 'arun()' returns a CrawlResult object, which supports the 'markdown' attribute.",
    "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is returned by arun() and contains the markdown attribute shown in the example. While it doesn't mention caching explicitly like the ground truth, it captures the core relationship between the class and its usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Within the documented code, 'result1.markdown' and 'result2.markdown' illustrate that the 'CrawlResult' instance supports the 'markdown' attribute, which is accessed for its content.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that CrawlResult.markdown contains webpage content in markdown format and is accessed as an attribute/property. The predicted description captures the core relationship even though it doesn't mention the specific 100-character print example.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet describes creating complex nested and list objects using JSON-like syntax with selectors, which directly correlates to how the JsonCssExtractionStrategy is designed to parse and extract data using CSS selectors as per the provided fields and structure.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the JSON schema structure and the extraction strategy's use of CSS selectors to parse nested/list data, which aligns with the ground truth's explanation of the same functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly shows the creation of an instance of `AsyncWebCrawler`. It mentions importing this class and utilizing it in the `main` function within an async context manager.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship shown in the ground truth - that AsyncWebCrawler implements async context manager functionality to handle initialization and cleanup. While the predicted description is less detailed, it correctly identifies the key usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text directly mentions 'strategy = CosineStrategy(...)' which demonstrates that 'CosineStrategy' implements the functionality described, particularly focusing on parameters like 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k'.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core connection between the documentation and CosineStrategy class parameters, but misses the crucial mapping to implementation methods that the ground truth emphasizes",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "'CosineStrategy' is a derived class of 'ExtractionStrategy'. The documentation discusses various strategy-related parameters, implying a design pattern involving strategy classes, which often extend a base class.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that CosineStrategy is a derived class of ExtractionStrategy and recognizes the strategy pattern design. The ground truth provides more implementation details about parameters, but the fundamental relationship described is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly mentions the use of the `AsyncWebCrawler` class for performing web crawling with an advanced schema. The code example demonstrates creating an instance of `AsyncWebCrawler` to perform an asynchronous crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of AsyncWebCrawler being used for web crawling, but misses crucial functionality around caching and the JsonCssExtractionStrategy implementation that the ground truth emphasizes as core aspects",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is used implicitly in the example to perform the crawling task. The example calls `arun` to manage the crawl process according to the extraction strategy provided.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun is used for crawling with extraction strategies, but omits significant aspects like caching, structured result handling, and the asynchronous nature that are core to the documented functionality",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet initializes an `JsonCssExtractionStrategy` object, clearly indicating its role in the advanced schema extraction process. This highlights setting up the strategy with a schema and integrating it into the crawler task.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the initialization aspect but misses the core functionality of processing HTML with BeautifulSoup and extracting structured data according to schema definitions",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is directly used in the example code snippet. It is instantiated with `verbose=True` and utilized as a context manager, indicating its explicit involvement in managing the crawling session.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage as a context manager and instantiation, but omits crucial aspects about session handling and core methods described in the ground truth, particularly around session persistence and user interactions.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example concludes with a call to `crawler.crawler_strategy.kill_session(session_id)`. This indicates the implicit use of the `kill_session` method within the `crawler_strategy`, a component of the `AsyncWebCrawler`.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the kill_session usage but misses the core functionality of cleaning up resources by closing page/context objects and removing sessions from memory",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is explicitly called in the example to perform the crawl action, receiving parameters such as `url`, `session_id`, and others.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun method usage, but misses the crucial aspect of session management and state maintenance across multiple requests that is central to the ground truth's description",
      "error_type": "key_concept_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The `result` variable, obtained from `arun`, is used to access `result.extracted_content`, implying that `arun` returns an instance of `CrawlResult`, which is necessary for content extraction in the example.",
    "ground_truth_relationship": "The CrawlResult class defines the structure for storing crawling results that are referenced in the example code, particularly through the 'result' variable which captures extracted content, session IDs, and other crawl-related data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that CrawlResult is the return type from arun() and stores crawling results including extracted content. While it's less detailed than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet includes a usage example of the 'arun' method, showing its invocation to crawl GitHub commits.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that the code shows usage of arun for crawling GitHub commits, but misses the core architectural relationship - that arun is the main method enabling dynamic web crawling with session management features.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'crawl_dynamic_content' function instantiates and uses 'AsyncWebCrawler', indicating its role in managing crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role in managing crawling operations, but misses key aspects about dynamic content support, session-based browsing, and JavaScript execution capabilities that are central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The document snippet explicitly mentions calling 'kill_session' to clean up the session used for crawling GitHub commits.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that kill_session is used for cleanup but omits the crucial details about closing page/context objects and removing from sessions dictionary",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet defines 'extraction_strategy' as an instance of 'JsonCssExtractionStrategy', specifying how to extract data elements from the crawled HTML.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as handling data extraction, but misses the crucial aspect of it implementing schema-based extraction specifically for GitHub commit data using BeautifulSoup as shown in the documentation example",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet features a code example using the 'arun' method of the 'crawler' object. It explicitly mentions awaiting 'crawler.arun()', indicating direct use of this method.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct basic usage of arun() method, but misses the crucial functionality of HTML-to-markdown conversion which is a core aspect mentioned in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the text snippet, 'result.markdown' is used, indicating that the 'CrawlResult' object, returned by 'arun', includes a 'markdown' attribute, implicitly showing how one accesses the markdown content extracted from a webpage.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the markdown attribute on the result object provides access to the markdown content extracted from a webpage, which aligns with the ground truth's explanation of the markdown property storing HTML-to-Markdown converted text.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet illustrates how to use the `AsyncWebCrawler` class, explicitly mentioning its usage to process iframe content.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the AsyncWebCrawler class handles iframe content processing through its arun() method with relevant parameters, though the predicted version is more concise",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is explicitly mentioned in the code example within the documentation as the function used to start the crawl with specific options like `process_iframes`.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the key method but incorrectly states it's explicitly shown in the documentation code example. While the fundamental purpose is understood, it misses that iframe processing is implemented through kwargs parameter passing.",
      "error_type": "documentation_misrepresentation"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` uses `AsyncPlaywrightCrawlerStrategy` as its default strategy, which would be involved when processing iframes due to the functionality described. This connection is not shown explicitly in the text but is implicit through class usage.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions iframe processing functionality but incorrectly suggests this is through AsyncWebCrawler rather than directly describing AsyncPlaywrightCrawlerStrategy's process_iframes method and its implementation details",
      "error_type": "incorrect_class_attribution"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides examples showing how to instantiate the AsyncWebCrawler class with proxy settings, both simple and with authentication.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description shows how to use proxies with AsyncWebCrawler but doesn't explain that proxy config is passed through kwargs to AsyncPlaywrightCrawlerStrategy. It shows usage but misses the internal relationship.",
      "error_type": "missing_core_relationship"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy' as its default crawling strategy when initialized, handling proxy configurations necessary for web crawling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description incorrectly states that AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default strategy, while the ground truth focuses on how AsyncPlaywrightCrawlerStrategy implements proxy support through browser_args configuration and ProxySettings objects. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method is explicitly referenced in the code snippet for executing JavaScript on a webpage. This method allows the execution of JavaScript code specified through the 'js_code' parameter, as demonstrated in the example with commands like 'window.scrollTo'.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles JavaScript execution, but incorrectly states that js_code is explicitly referenced in the code snippet when it's actually handled through **kwargs and passed to crawler_strategy.crawl()",
      "error_type": "factual_inaccuracy"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class, which is typically assigned to the 'crawler_strategy' attribute of 'AsyncWebCrawler' during initialization, is implicitly involved in executing JavaScript. The 'arun()' method of AsyncWebCrawler likely delegates JavaScript execution tasks to AsyncPlaywrightCrawlerStrategy, which handles page interactions including JavaScript execution.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles JavaScript execution, but incorrectly implies it's only through AsyncWebCrawler's arun() method. The ground truth shows that JS execution is implemented directly in the crawl method via page.evaluate(), with both single and multiple command support.",
      "error_type": "implementation_misunderstanding"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides a usage example where `arun()` is directly called, illustrating its function to perform a crawl operation.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() performs crawling but misses the key focus of the relationship which is about error handling and status checking through CrawlResult fields",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result of the `arun()` method is assigned to the variable `result`, which is an instance of `CrawlResult`. This usage is implied as the method returns a `CrawlResult` object that contains attributes like `success`, `error_message`, and `status_code`.",
    "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core relationship - that CrawlResult provides fields for error handling (success, error_message, status_code) that match the usage shown in documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation checks `if not result.success:` hinting at the `success` attribute of `CrawlResult` to determine the crawl's completion status.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of the success boolean being used to check crawl completion status, which aligns with the ground truth's explanation of error handling via the success property",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The error message accessed by `result.error_message` in case of a failed crawl refers to the `error_message` attribute of the `CrawlResult` class.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly indicates that error_message is an attribute used to store error information when a crawl fails, which aligns with the ground truth's explanation of its use in error handling",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "Accessing `result.status_code` after `arun()` implies use of the `status_code` attribute to understand the HTTP status of the crawl operation.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea that status_code relates to HTTP status of crawl operations, but misses the crucial error handling context and diagnostic purpose emphasized in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly mentions 'AsyncWebCrawler' in the context of user simulation, indicating that this class is used to initiate the web crawling process with simulated user behavior.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for simulated user behavior in web crawling, which aligns with the ground truth's explanation that the class implements user simulation functionality through its parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is directly called in the provided code snippet, showing its explicit use in executing the crawling task with enhanced user simulation options.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on directly calling the arun method, while the ground truth describes how arun implements user simulation behaviors through kwargs parameters. The predicted description misses the core relationship between the documentation and implementation.",
      "error_type": "missed_core_relationship"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, this class is the default strategy used by 'AsyncWebCrawler' if no other strategy is provided, and it implements various functionalities including user simulation ('simulate_user' argument).",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the class being used by AsyncWebCrawler, but makes an unsupported assumption about it being the 'default' strategy. It also mentions user simulation but doesn't fully explain the implementation details around mouse movements, clicks, and navigator property overrides that are key aspects in the ground truth.",
      "error_type": "unsupported_assumption_and_incomplete_detail"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation text shows an example usage with 'crawler.arun()', indicating that 'crawler' is an instance of a class that supports this method. The 'AsyncWebCrawler' class initializes with default or specified crawler strategies, aligning with the snippet's functionalities.",
    "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and arun method but focuses on instance creation and general functionality, while missing the specific HTML cleaning functionality described in the ground truth regarding WebScrappingStrategy and cleaned_html output.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' is explicitly called in the example code provided in the documentation snippet. This method is responsible for performing the actual web crawl operation.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as being used for web crawling, but misses its core HTML cleaning functionality described in the ground truth. While crawling is part of it, the main focus according to the documentation is on HTML sanitization and cleaning.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The code example accesses 'result.cleaned_html', indicating that 'CrawlResult.cleaned_html' holds the sanitized HTML, fitting the documentation's description of processing and cleaning HTML content.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that cleaned_html contains sanitized HTML content after processing and removing unwanted elements",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the `JsonCssExtractionStrategy` class and demonstrates its usage in extracting data using CSS selectors. This is evident from the line `strategy = JsonCssExtractionStrategy(schema)`, where the strategy is instantiated with a schema.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses CSS selectors, but misses the key implementation detail about using BeautifulSoup's select() method and the extraction process for matched elements",
      "error_type": "missing_core_implementation"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text provides a usage example that implicitly includes the `arun` method of the `AsyncWebCrawler` class. This is indicated by the call `result = await crawler.arun(url=\"https://example.com/products\", extraction_strategy=strategy)`, which shows how the strategy is used alongside an async web crawling method.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship showing how arun() works with extraction strategies through an example, while the ground truth explains it more formally. Both convey that arun() integrates with extraction strategies to process web pages.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses an instance of `AsyncWebCrawler` to call its `arun` method, which is lined as `result = await crawler.arun(...)`. Though not explicitly detailed, AsyncWebCrawler serves as the context for `arun` usage, making its presence implicit.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its arun method usage, but misses the core relationship about CSS-based extraction through JsonCssExtractionStrategy implementation in aprocess_html. It focuses on API usage rather than the internal processing mechanism.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The text snippet explicitly shows creating an instance of 'CosineStrategy' with specific parameters for clustering and content filtering strategies.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic instantiation of CosineStrategy with parameters, but misses crucial aspects of its functionality like semantic filtering, hierarchical clustering, and cosine similarity that are core to how it works according to the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet includes a code example where 'AsyncWebCrawler' is instantiated and used to run the '@crawler.arun()' method, showing its direct usage in conjunction with 'CosineStrategy'.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler and its arun method usage, but fails to capture its broader role in providing core functionality for content filtering, customizable extraction strategies, and caching mechanisms as described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Although not explicitly mentioned, the 'arun' method is implied in the example usage of 'AsyncWebCrawler' for executing a crawl with an extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as the execution method for AsyncWebCrawler, but misses significant functionality like caching, HTML processing, and error handling mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not explicitly mentioned, 'CosineStrategy' implicitly relies on 'ExtractionStrategy' as it inherits from this abstract base class for extraction implementation.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, and the fundamental purpose of ExtractionStrategy as a base class for implementing extraction functionality. While it's more concise than the ground truth, it captures the essential relationship without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly mentions using the 'AsyncWebCrawler' with specific proxy settings. This indicates its use as the main class for asynchronous web crawling with a proxy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for asynchronous web crawling with proxy support, which aligns with the ground truth's explanation that the class implements proxy support through its crawler_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is called in examples within the documentation snippet to process the given URL, indicating its role in executing the crawling logic with proxy setup.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that arun() is used to execute crawling functionality with proxy setup, which aligns with the ground truth's description of it implementing core crawling functionality and handling proxy configurations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The document snippet provides an example using the `arun()` method to perform web crawling and modulate content cleaning by passing various parameters including `word_count_threshold`, `excluded_tags`, and `remove_overlay_elements`.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the arun() method and content cleaning functionality, mentioning key parameters that control the cleaning behavior. The minor omission of layout analysis doesn't change the fundamental understanding.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Though not directly mentioned, the code snippet shows an instance of `AsyncWebCrawler` being used to call the `arun()` method. This implies the use of the `AsyncWebCrawler` class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic existence of AsyncWebCrawler and its arun method, but misses the core functionality of content cleaning and noise removal that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet shows accessing the `cleaned_html` attribute from the `CrawlResult` object, indicating the use of this attribute to get cleaned HTML content from the web crawl operation.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that cleaned_html contains cleaned HTML content from web crawling, but misses explaining that it's specifically the result of Crawl4AI's multi-step cleaning process (basic cleaning, content relevance, layout analysis).",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The document snippet accesses the `markdown` attribute of a `CrawlResult` object, to display a clean markdown version of the crawled content, showing explicit use of this attribute.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct use of markdown attribute for displaying clean content, but misses the key aspect of noise removal and cleaning that is central to the ground truth relationship",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The text explicitly discusses the 'JsonCssExtractionStrategy', providing a context that it is capable of handling complex, nested HTML structures. This establishes a direct documentation relationship outlining its advanced usage scenarios.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy handles complex nested HTML structures, which aligns with the ground truth's description of recursive HTML processing using BeautifulSoup and schema-based selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The implicit relationship arises because 'JsonCssExtractionStrategy' extends 'ExtractionStrategy'. This is fundamental as 'JsonCssExtractionStrategy' adopts methods and behaviors from 'ExtractionStrategy'.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy providing the foundational architecture for JsonCssExtractionStrategy",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet mentions using `crawler.arun` with specific parameters for smart link filtering. The `AsyncWebCrawler.arun()` is mentioned explicitly as the method to call for achieving the functionality described.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that crawler.arun is used for link filtering, but misses the key implementation detail that the filtering parameters are passed through kwargs to crawler_strategy.crawl(). It suggests a more direct relationship than the indirect passing of parameters that actually occurs.",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While the snippet doesn't explicitly name `AsyncWebCrawler`, it's inferred as the class that holds the `arun` method being invoked in the example provided.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as having the arun method but misses the key relationship about link filtering functionality described in the ground truth. While not completely wrong, it omits the core functionality being documented.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is directly demonstrated in the Magic Mode example code block in the documentation snippet, which shows how to use this class to enable Magic Mode for anti-bot protection when crawling a URL.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in Magic Mode anti-bot protection, but misses that this functionality is implemented through the crawler_strategy parameter and AsyncPlaywrightCrawlerStrategy rather than directly in the class itself.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is invoked in the example code within the documentation to perform the crawling task with 'magic=True', enabling anti-bot features.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the arun method is used to perform crawling with magic mode for anti-bot features, which aligns with the ground truth's explanation that arun accepts a magic parameter to enable anti-bot protection features via the crawler_strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class implicitly relates to the documentation as it provides the underlying strategy likely used by 'AsyncWebCrawler'. Although not mentioned in the snippet, it is necessary for the described anti-detection features.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is related to the anti-detection features, but it describes this relationship too vaguely and fails to acknowledge that the code directly implements the Magic Mode functionality rather than just being 'likely used'",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The `CosineStrategy` class is directly mentioned in the text snippet as a strategy for extracting relevant content sections based on similarity clustering. The documentation provides a code example that initializes and uses `CosineStrategy`.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy as a similarity-based content extraction tool, but misses describing the specific configurable parameters and their role in controlling the clustering process, which is a key aspect highlighted in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is invoked in the example code using the `CosineStrategy` for extracting content from a URL. This provides implicit traceability to `AsyncWebCrawler.arun()`, demonstrating its role in utilizing the extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() method uses extraction strategies like CosineStrategy to process content from URLs. While less detailed than the ground truth, it captures the core relationship between arun() and extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class provides the context within which the `arun` method, implicitly linked in the documentation snippet, operates. This indicates that the high-level crawling process involves the use of `CosineStrategy`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler provides context for using CosineStrategy via arun(), but misses the crucial aspect that CosineStrategy specifically performs similarity-based content clustering and extraction, which is a core part of the relationship.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet for 'E-commerce Scraping' explicitly demonstrates the use of a CSS strategy involving the `baseSelector` and `fields` attributes, which aligns with the purpose of `JsonCssExtractionStrategy` as it uses a schema to extract fields from CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that JsonCssExtractionStrategy implements CSS-based scraping using a schema with base selectors and field mappings, which matches the e-commerce example in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "For 'News Article Extraction', the document describes using an 'LLM Strategy' with a provider and a schema, matching the functionality of `LLMExtractionStrategy`, which is initialized with a provider and schema for extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for article extraction and accurately notes the key components of provider and schema parameters in initialization, matching the ground truth's core relationship description.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The 'Content Analysis' section suggests a 'Cosine Strategy' with semantic filtering and topic selection, implying the use of `CosineStrategy` for analysis, as it extracts topics based on semantic similarity and top-k clustering.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy - using semantic filtering and clustering for content analysis with top-k selection, which aligns with the ground truth's description of using cosine similarity and semantic filtering for text content analysis.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation demonstrates the use of a 'crawler' object to call the 'arun' method, indicative of the 'AsyncWebCrawler' class, which is responsible for high-level crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in crawling but fails to mention its core metadata extraction functionality that's central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly calls the 'arun' method on the crawler object to demonstrate usage. This method is part of 'AsyncWebCrawler' and is central to the process of initiating a crawl to extract metadata.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the method for crawling but misses the key relationship that it returns metadata fields accessible through the .metadata property, which is the main focus of the documentation.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "The snippet accesses 'result.metadata' after calling 'arun', directly indicating that 'metadata' is a part of the 'CrawlResult'. It shows how metadata can be accessed post-crawl.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "Predicted describes basic metadata access correctly but omits crucial information about metadata being optional and the specific fields available in the dictionary",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document explicitly mentions the schema components structured for `JsonCssExtractionStrategy`, such as `name`, `baseSelector`, and `fields`. These elements are directly tied to the functionality of the `JsonCssExtractionStrategy` class.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the schema components but lacks the crucial implementation detail about how BeautifulSoup's select() method is used to actually extract the data according to the schema configuration",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet shows the creation and usage of an 'AsyncWebCrawler' instance, explicitly referred to in the 'async with AsyncWebCrawler' line, indicating its use in a simple async context as part of an example.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the essential relationship between AsyncWebCrawler and its async context manager functionality, noting its usage with 'async with' syntax which aligns with the ground truth's description of the __aenter__ and __aexit__ implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is explicitly used to perform a crawl in the example with 'result = await crawler.arun(...)'. This shows a direct usage example.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method being used for crawling, but misses crucial aspects like content extraction, caching, and error handling that are core to the implementation shown in the code",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Although not explicitly mentioned, 'result.markdown' is accessed in the snippet, indicating usage of 'CrawlResult' (by its attribute access) which 'arun()' presumably returns.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that result.markdown contains extracted content that can be accessed/printed, with the predicted version making a reasonable inference from the code example shown",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of `AsyncWebCrawler` in the example function `advanced_session_crawl_with_hooks()`. It showcases how an instance of this class is instantiated and used as the main tool for executing the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage in the example but misses the core relationship about custom execution hooks being implemented through the crawler_strategy attribute, which is the main focus of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is explicitly called in the code snippet within the documentation to perform the crawling process for a URL, indicating its role in executing crawling operations.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as being used for crawling URLs, but misses the crucial aspect of it enabling custom hooks through crawler_strategy during the crawling process",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The text references setting hooks via the `set_hook` method, which is a feature of `AsyncPlaywrightCrawlerStrategy`. The code sample shows using `set_hook()` to incorporate a custom function `on_execution_started`.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that set_hook() is used to register custom hook functions, and the example matches the code's functionality of storing hooks in a dictionary. While the predicted text doesn't explicitly mention the hook storage mechanism, it correctly describes the core relationship and usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example in the documentation uses the `kill_session()` method to terminate a browser session after crawling, illustrating its purpose in session management.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core purpose of kill_session() as a method for terminating browser sessions after crawling. While it doesn't detail the specific cleanup steps, it conveys the essential relationship between the method and session management.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides a schema for extracting complex HTML structures using a 'baseSelector' and various 'fields', which aligns with 'JsonCssExtractionStrategy' that processes similar CSS-based extraction tasks.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation - both describe a CSS selector-based extraction strategy that uses a schema structure to parse HTML elements, even though it doesn't mention all implementation details like BeautifulSoup's select() method.",
      "error_type": "none"
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Since 'JsonCssExtractionStrategy' is a specific strategy for CSS extraction, it implicitly extends the base class 'ExtractionStrategy', responsible for the generic structure of extracting meaningful content from HTML.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class, but focuses only on JsonCssExtractionStrategy which isn't mentioned in the code/docs. It misses the core purpose of providing schema-based extraction patterns.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the documentation, indicating its support for session-based crawling through the `session_id` parameter and related methods.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class supports session-based crawling through the session_id parameter, which aligns with the ground truth's description of maintaining persistent browser sessions across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation discusses maintaining a session with `session_id`, an implicit reference to `arun()` using this parameter during the crawling process.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that session_id is used within arun() for session management during crawling, even though it doesn't detail the specific implementation of storing it in CrawlResult.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, the `AsyncWebCrawler` likely relies on strategies like `AsyncCrawlerStrategy` due to its role in session-based crawling architecture.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncWebCrawler's likely reliance on AsyncCrawlerStrategy but is tentative ('likely') and misses explaining the concrete strategy interface role in defining session-based crawling capabilities.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` is linked to `AsyncWebCrawler` as a specific strategy that implements session-based functionality.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as implementing session-based functionality, which aligns with the ground truth's description of how it manages sessions through its sessions dictionary and associated methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly uses the `AsyncWebCrawler` class to perform asynchronous crawling with dynamic content handling. The class is mentioned explicitly in the code example provided.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for asynchronous crawling, but misses the crucial aspect of session-based dynamic content handling with pagination and JavaScript injection capabilities that the ground truth emphasizes.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` is extended by instances of `AsyncWebCrawler` through its default crawler strategy. The documentation demonstrates dynamic content handling, which involves webpage interactions facilitated by this strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is used for dynamic content handling, it incorrectly states that AsyncWebCrawler extends it. Rather, the strategy is used by AsyncWebCrawler as its crawler strategy.",
      "error_type": "incorrect_inheritance_relationship"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The JSON CSS extraction logic is demonstrated within the `extraction_strategy` parameter, wherein a schema is defined to extract commit titles from the webpage structure, showing explicit use of `JsonCssExtractionStrategy`.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that JsonCssExtractionStrategy implements schema-based data extraction using CSS selectors to parse structured data from web pages, with the main focus on extracting commit information based on the schema configuration.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `.kill_session()` method of `AsyncPlaywrightCrawlerStrategy` is used directly in the snippet after the crawls, explicitly showcasing session termination.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session is used to terminate sessions after crawling, which aligns with the ground truth's explanation of cleaning up browser resources by closing Playwright objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet demonstrates the usage of JsonCssExtractionStrategy with a complex HTML structure of a hypothetical e-commerce webpage. It illustrates how JsonCssExtractionStrategy can be used to extract various components such as product categories, product details, reviews, and related items.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately conveys that JsonCssExtractionStrategy handles complex HTML structures and extracts data from e-commerce product listings, which aligns with the ground truth's explanation of using BeautifulSoup to parse HTML and map CSS selectors to schema fields.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The document explicitly mentions the use of the CosineStrategy class for article content extraction, product review analysis, and technical documentation tasks. The strategy is configured with specific parameters to adapt to different use cases.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy being used for different use cases through parameter configuration, matching the ground truth's explanation of how the class handles different extraction scenarios by adjusting parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The AsyncWebCrawler.arun() method is used in code examples for executing the crawling with a specified extraction strategy. While not directly discussed, it is implied as a necessary step in the process outlined in the code snippets.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as necessary for crawling with extraction strategies, but misses the crucial relationship between arun() and the flexible parameter configurations shown in the use cases that enable different extraction behaviors.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation explicitly demonstrates the use of the kill_session() method to ensure resource management by cleaning up sessions after crawling activities, as shown in the code snippet 'await crawler.crawler_strategy.kill_session(session_id)'.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain how kill_session implements resource management by cleaning up/closing sessions, with the predicted description focusing on its documented usage and the ground truth focusing on its implementation details",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides code examples using the arun() method of AsyncWebCrawler to manage state transitions between pages using sessions, reflected by lines like 'result = await crawler.arun(...)'.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's role in session management but focuses on the external usage rather than the internal session_id storage mechanism described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The use of the arun() function within the documentation implies the utilization of the AsyncWebCrawler class, as arun() is a method associated with this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between arun() and AsyncWebCrawler, but misses the crucial session management functionality that's central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Since AsyncWebCrawler utilizes AsyncPlaywrightCrawlerStrategy as a strategy and given AsyncPlaywrightCrawlerStrategy is a subclass of AsyncCrawlerStrategy, the documentation indirectly uses AsyncCrawlerStrategy through inherited capabilities.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core methods needed to implement the session management behaviors described in the documentation, including crawl() for executing page actions and hook management for handling session state.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncCrawlerStrategy is used through inheritance, but doesn't capture its core purpose in providing session management and crawling capabilities described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The AsyncPlaywrightCrawlerStrategy is implicitly traced because it extends the AsyncCrawlerStrategy which is used by AsyncWebCrawler for session and state management.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship with AsyncCrawlerStrategy but misattributes the session management functionality as coming from the parent class when it is actually directly implemented in AsyncPlaywrightCrawlerStrategy.",
      "error_type": "incorrect_source_attribution"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet directly instantiates an 'AsyncWebCrawler' within an async with block, indicating explicit usage of this class for managing web crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler's basic usage pattern with async context management, but misses the crucial aspect of customizable extraction strategies and the specific purpose demonstrated in the documentation example (extracting tech-related content with LLMExtractionStrategy).",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is explicitly called within the snippet to perform the web crawling operation for a specified URL using the provided LLM extraction strategy.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method being used for web crawling but misses the crucial aspect of content filtering/extraction based on specific criteria using LLMExtractionStrategy, which is a core part of the relationship shown in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'LLMExtractionStrategy' is instantiated with parameters including a provider and API token, implementing the specific extraction strategy as per the documentation instructions to extract technology-related content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation - that LLMExtractionStrategy implements content extraction functionality with specific provider/API parameters to extract content based on instructions, matching the example shown in documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly uses the `AsyncWebCrawler` class within the `extract_openai_fees` function, establishing an instance of this class to perform crawling operations as part of the asynchronous context manager.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship by describing how the AsyncWebCrawler class is used for crawling operations through its async context manager functionality, which aligns with the ground truth's description of the class implementing web crawling functionality with async methods for URL crawling and data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly instantiated in the provided example with specific parameters like provider and API token, indicating its use for extraction tasks.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of LLMExtractionStrategy for extraction tasks but misses the crucial aspect of structured data extraction using Pydantic schema and HTML parsing functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `extract_openai_fees` function calls `arun()` method on the `AsyncWebCrawler` instance, which is implicitly indicated as a fundamental operation to perform web crawling.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship of using arun() for web crawling, but misses crucial aspects about the extraction strategy, data processing and error handling that are key parts of the relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet includes the usage of 'AsyncWebCrawler' in the function `capture_and_save_screenshot`. The class is explicitly mentioned as being initialized in the example provided.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct class and its usage in the example, but misses the core screenshot functionality implementation detail that the ground truth describes - specifically how the screenshot data is handled in base64 format through the arun method.",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the example, `arun()` is the method called on the 'AsyncWebCrawler' instance to perform the crawling operation, which includes taking a screenshot. The method is not explicitly discussed but is necessary for the functionality described.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method for crawling with screenshot capability, but misses explaining the core mechanics of how screenshots are handled through screenshot_data and caching that the ground truth describes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The example checks `result.screenshot` to determine if a screenshot was successfully captured. This involves accessing the 'screenshot' attribute of 'CrawlResult', which is an implicit trace from the method's output.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the code checks result.screenshot for successful capture, but misses the key detail that it contains base64-encoded screenshot data that gets decoded and saved",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation snippet explicitly discusses 'TopicSegmentationChunking', detailing its use of the TextTiling algorithm to segment text into topic-based chunks. It indicates parameters like 'num_keywords' which aligns with the parameters described in the artifact.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of using TextTiling for topic-based text segmentation, which aligns with the ground truth's explanation of the implementation using NLTK's TextTilingTokenizer",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The class `AsyncWebCrawler` is explicitly mentioned in the documentation snippet as part of a code example showing how to instantiate it with custom user agent and headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's presence in the documentation example, but misses the key functional relationship that the class implements identity management through user agent and header customization. It only describes the surface-level presence of the class rather than its identity management capabilities.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is used in the example provided in the documentation snippet. It is implicitly related through the code example which shows how this method is called as part of the `AsyncWebCrawler` instantiation and use.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the presence of arun() in the code and documentation examples, but misses the key functionality relationship that arun() implements identity management through user_agent and header customization.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This is the default strategy used by `AsyncWebCrawler` when no strategy is explicitly provided, as indicated in the code for `AsyncWebCrawler`. This relationship is inferred from the usage of `AsyncWebCrawler` with options like user agent and headers which are managed by strategies like `AsyncPlaywrightCrawlerStrategy`.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles identity management through user agents and headers, which aligns with the ground truth's explanation of how the class manages these identity aspects through its constructor and methods.",
      "error_type": null
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` extends `AsyncCrawlerStrategy`, which is a conceptual layer in the understanding of how strategies function within the `AsyncWebCrawler`, even if implicit in this specific document snippet.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies AsyncCrawlerStrategy as a strategy layer, but misses the core purpose of identity management and customization described in the ground truth. It also incorrectly mentions AsyncPlaywrightCrawlerStrategy which isn't in the code.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is implicitly demonstrated in the documentation snippet within the usage example for crawling a URL. The example code `result = await crawler.arun(url=\"https://example.com\")` suggests the use of this method to obtain web crawl results.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a method for crawling URLs but misses its crucial role in processing and generating the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'CrawlResult' class is explicitly discussed in the documentation snippet, indicating the different formats (attributes) available such as 'html', 'cleaned_html', 'markdown', and 'fit_markdown'. These are accessed from the result of 'arun' as demonstrated, using the snippet: `raw_html = result.html`.",
    "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the CrawlResult class and the documented output formats, noting how the class properties correspond to the various formats shown in the documentation examples.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The 'html' attribute is explicitly used in the documentation for accessing the 'original HTML' from a 'CrawlResult' instance, shown in the example: `raw_html = result.html`.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions indicate that the 'html' is used to store/access raw HTML content, with the predicted description referencing the actual usage shown in documentation and ground truth describing the type definition. The core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The 'cleaned_html' attribute is explicitly mentioned to access 'Sanitized HTML' from a 'CrawlResult' instance, evidenced by the line: `clean_html = result.cleaned_html`.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that cleaned_html provides access to sanitized HTML content from the result object, which aligns with the ground truth's core meaning",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The 'markdown' attribute of 'CrawlResult' is directly referenced in the documentation snippet to describe accessing markdown format: `markdown = result.markdown`.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that markdown is accessed via result.markdown, but misses the key functionality that it converts HTML content to standard markdown format",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet explicitly highlights the 'fit_markdown' attribute accessed for retrieving 'Most relevant content in markdown': `fit_md = result.fit_markdown`.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that fit_markdown contains the most relevant content in markdown format, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function 'arun' is directly called in the code snippet for handling the crawling process. It is used to attempt URL extraction with specific strategies.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that 'arun' is used for crawling, but misses the core focus on error handling and response states that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The documentation mentions the use of an 'extraction_strategy' that provides an interface for extracting meaningful data from HTML content. This aligns with the 'ExtractionStrategy' abstract class.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that ExtractionStrategy provides an interface for extraction, but misses the crucial aspects of parallel processing capabilities and the base class's role in implementing different extraction methods like Cosine Strategy",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The 'Cosine Strategy' is mentioned for its effectiveness with semantic understanding and content with inconsistent structures, linking it implicitly to the 'CosineStrategy' which extends 'ExtractionStrategy'.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core purpose of semantic understanding and handling inconsistent structures, but misses crucial functionality details about cosine similarity, hierarchical clustering, and the key methods that implement these features.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The 'extracted_content' attribute is accessed in the provided code snippet to process the extracted data if the crawl is successful. The attribute represents the specific output of the extraction process.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies extracted_content as storing extraction output, but misses crucial context about it being optional and the semantic matching/cosine strategy aspects mentioned in ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The 'error_message' attribute is accessed in the code snippet to display an error detail if the crawling process fails. This relates to error handling within the documented trial-except block.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that error_message is used for displaying error details during failed execution, which aligns with the ground truth's explanation of using it to store and show failure details via result.error_message",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is specifically used in the documentation snippet to execute crawling operations with extraction strategies, as demonstrated in the examples provided.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of arun() method being used with extraction strategies for crawling operations, which aligns with the ground truth's explanation of how it implements extraction strategies for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` class is instantiated and utilized in the example for structured extraction after page interactions, highlighting its usage explicitly.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions JsonCssExtractionStrategy's use for structured extraction, but misses the core implementation detail about using BeautifulSoup and CSS selectors to parse HTML elements according to the schema",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly mentioned and used in an example, showing how it can be configured to analyze dynamic content using language models.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as a way to analyze content using language models, which aligns with the ground truth's description of processing HTML content through LLM models with configurable settings.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly mentioned, `AsyncWebCrawler` is the context in which the `arun` method is used, implying its usage in the interaction and extraction process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncWebCrawler's role in providing arun() but fails to mention its key functionality with extraction strategies and JavaScript execution for dynamic content, which are central aspects in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly mentions using the `arun()` method of the `crawler` instance, which implicitly refers to the `AsyncWebCrawler.arun()` method when given the context of the available artifacts.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method implementation but misses the key relationship about html2text customization options being passed as kwargs, which is the main focus of the documentation",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The usage of `crawler` implicitly suggests the instance of `AsyncWebCrawler`, given that `arun()` is a method of the `AsyncWebCrawler` class. Thus, it implies that `crawler` is an instance of `AsyncWebCrawler`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the instance relationship of 'crawler' being an AsyncWebCrawler, while the ground truth describes the functional relationship of how AsyncWebCrawler handles HTML-to-text customization through its arun() method. While not entirely wrong, it misses the core functionality being described.",
      "error_type": "missed_core_functionality"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly mentions the use of 'arun' method of a 'crawler' which corresponds to 'AsyncWebCrawler.arun()'. The code snippet demonstrates calling this method with different 'extraction_strategy' parameters.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method accepts different extraction strategies as parameters, which aligns with the ground truth's explanation of how the method enables combining different strategies sequentially.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation mentions using 'css_strategy' which implicitly corresponds to 'JsonCssExtractionStrategy'. This class implements extraction using CSS selectors, as required in the example where a CSS strategy is first used.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as implementing CSS-based extraction that can be used as part of a combined strategy approach, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'llm_strategy' mentioned in the documentation implicitly corresponds to 'LLMExtractionStrategy', which implements extraction of semantic details, a step demonstrated in the example as a second phase following the CSS strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship of LLMExtractionStrategy being used for semantic analysis in combination with other strategies, which aligns with the ground truth's explanation of the class enabling combined crawling strategies with LLM-based semantic processing.",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation explicitly mentions the 'FixedLengthWordChunking' class and describes its functionality of splitting text into fixed-size word chunks. This directly corresponds to the class definition found in the provided artifacts.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of splitting text into fixed-size word chunks, which aligns with the ground truth's explanation of the implementation using word tokens and chunk_size parameter",
      "error_type": "none"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "While the specific mention of 'ChunkingStrategy' is not made, 'FixedLengthWordChunking' is likely extending from 'ChunkingStrategy' as part of its base class structure, which is a common design pattern for creating various chunking strategies.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that FixedLengthWordChunking extends/implements ChunkingStrategy as a base class, which aligns with the ground truth's description of their relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows a usage example of the 'arun' method of the 'AsyncWebCrawler' class, indicating it is the central interface for executing a crawl with various configurations like word_count_threshold and content exclusions.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the main interface for crawling with configurations, but misses the key point that the documented filtering parameters are specifically implemented as kwargs and processed during HTML extraction stages.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not explicitly mentioned, 'AsyncWebCrawler' is the class containing the 'arun' method. The usage of 'arun' inherently implies the use of 'AsyncWebCrawler' since methods are invoked on instances of the class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only mentions the structural relationship between AsyncWebCrawler and arun method, but misses the core functionality of content filtering described in the ground truth. While not incorrect, it omits the crucial aspect of how the class implements content filtering through various parameters.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation mentions accessing media elements from 'result.media', indicating direct usage of the CrawlResult class as it provides a 'media' attribute.",
    "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CrawlResult's media attribute is used, but misses the key detail about the hierarchical structure of video/audio metadata storage in the media dictionary and how it's accessed.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Explicitly shown in the documentation as being accessed to retrieve video and audio metadata, aligning with the 'CrawlResult' class's 'media' attribute.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that media is a dictionary containing video and audio metadata accessed through lists, with the predicted description focusing on the access pattern and the ground truth providing more implementation details",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions using the LLMExtractionStrategy class from crawl4ai.extraction_strategy for extracting entities and relationships.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for extraction and mentions some key parameters, but misses crucial aspects of its functionality like chunking, rate limiting, and parallel processing that are central to how it actually works with different LLM providers.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example in the text snippet shows calling the arun method of a crawler object to perform the extraction using LLMExtractionStrategy. Although AsyncWebCrawler is not explicitly described, its method arun is utilized in the code.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that arun() method is used with LLMExtractionStrategy for extraction, which aligns with the ground truth's explanation of how AsyncWebCrawler.arun() implements structured data extraction by accepting an extraction_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the example, the extracted content is accessed via result.extracted_content. This implies the use of the CrawlResult class, notably its extracted_content attribute, as part of the process.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed via result.extracted_content, but misses the crucial aspect that it specifically stores structured data as a JSON string from LLMExtractionStrategy that can be parsed into a Pydantic model",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation explicitly mentions `fit_markdown` as an advanced heuristic feature used to extract the main content from a webpage.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that fit_markdown is a feature for extracting main content using heuristics, which aligns with the ground truth's definition of it being a property storing extracted main content after applying heuristics.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation contrasts `fit_markdown` with `markdown`, showing how both are used to obtain different levels of content extraction. The text mentions regular markdown for comparison with fit markdown.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship between markdown and fit_markdown, noting they are used for different levels of content extraction and can be compared.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation features an example function `extract_article_content`, which utilizes `AsyncWebCrawler` to perform crawling and extract the main content.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role in content extraction but misses the key focus on fit_markdown's functionality for identifying and filtering main content while removing boilerplate elements. It describes more what the example code does rather than how the feature works.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows the usage of `arun` method within `AsyncWebCrawler` to perform the web crawling operation for a given URL, retrieving a `CrawlResult`.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's role in web crawling but omits the crucial functionality around content extraction and fit_markdown feature that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows usage of the 'arun' method of 'crawler', setting parameters such as 'url', 'page_timeout', 'delay_before_return_html', and 'wait_for', which directly relate to handling timeouts and waiting mechanisms during the crawling process.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method handles timeout and waiting parameters, which matches the ground truth's explanation that these controls are implemented through kwargs and passed to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun()' method belongs to the 'AsyncWebCrawler' class, which implicitly suggests the use of this class when configuring the crawling strategy, as the method depicted is a member function.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic ownership relation between arun() and AsyncWebCrawler, but misses the core functionality about timeout and waiting configuration implementation described in the ground truth. The predicted focuses only on class membership rather than the actual timeout functionality.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly uses 'AsyncWebCrawler' in the example for crawling a website asynchronously with a wait_for parameter. This usage aligns with the class being a primary interface for web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler is used for asynchronous web crawling and handles the wait_for parameter, which aligns with the ground truth's explanation that this functionality is supported through its arun method and crawler_strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class initializes with an 'AsyncPlaywrightCrawlerStrategy' unless specified otherwise. This example implicitly uses 'AsyncPlaywrightCrawlerStrategy' through 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the initialization relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, while the ground truth describes the wait_for functionality and smart_wait method implementation for handling dynamic content loading.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly called in the example to perform crawling on the given URL with specified parameters, indicating its role in executing the crawl operations.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method for crawling URLs, but misses the key relationship with the wait_for parameter functionality that is central to the ground truth description.",
      "error_type": "key_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' is explicitly instantiated and used for extracting data from the crawled content, highlighting its role in manipulating and processing extracted data.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in data extraction but misses the crucial schema-based implementation details using BeautifulSoup for HTML parsing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The method 'kill_session' is explicitly invoked to terminate a session identified by 'session_id', demonstrating its utility in managing session life cycles.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic function of kill_session for terminating sessions, but misses the crucial context that it's specifically used to clean up resources after wait_for-based crawling completes",
      "error_type": "incomplete_context"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The provided code snippet explicitly uses the AsyncWebCrawler class, which is evident from the statement 'async with AsyncWebCrawler(...)'. This shows the initialization and configuration of the crawler with various parameters.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its initialization, but misses describing many key features shown in the ground truth, such as the comprehensive configuration options for browser setup, timing controls, and anti-detection features that are central to the class's functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function AsyncWebCrawler.arun() is explicitly called in the usage of AsyncWebCrawler in the code snippet, indicating that arun() is used to perform the crawling task as configured by the AsyncWebCrawler instance.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling tasks with AsyncWebCrawler, but misses the crucial aspect of how it processes configurations through error handling, caching, and content extraction shown in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is likely used as a strategy pattern or implementation within AsyncWebCrawler based on the initialization in AsyncWebCrawler and its methods such as crawl(), which align with functionalities demonstrated in the documentation like handling dynamic content and taking screenshots.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the key relationship - that AsyncPlaywrightCrawlerStrategy implements core crawler functionality used by AsyncWebCrawler for browser automation, content handling, and various crawling features shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a code example where the 'arun' method of a 'crawler' object is used with several parameters like 'url', 'word_count_threshold', 'exclude_external_links', 'remove_overlay_elements', and 'process_iframes'. The use of the 'arun' method in the snippet suggests it is a part of the 'AsyncWebCrawler' class behavior.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that the arun method accepts the documented configuration options and is used for asynchronous web crawling. While it doesn't mention all implementation details, it correctly identifies the main functionality and parameter system.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While 'AsyncWebCrawler' is not directly mentioned in the snippet, 'crawler' is implicitly of this type given that its 'arun' method is used in the example, which connects the documentation to the 'AsyncWebCrawler' class itself.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class implements the documented arun method with the corresponding configuration options, just with less detail than the ground truth. The core relationship between the code and documentation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' is related to 'AsyncWebCrawler' implicitly as a likely strategy being used. 'AsyncWebCrawler' accepts various strategies and given the context, 'AsyncPlaywrightCrawlerStrategy' is a potential default implementation within 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncPlaywrightCrawlerStrategy is related to AsyncWebCrawler but mischaracterizes it as an implicit/default implementation. The ground truth shows it's explicitly implementing documented crawling options through specific methods.",
      "error_type": "incorrect_relationship_characterization"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet directly mentions using the 'AsyncWebCrawler' class with different browser types ('firefox', 'webkit', and default 'chromium').",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of browser selection in AsyncWebCrawler, mentioning all three browser types (firefox, webkit, chromium) supported by the class. While it doesn't detail the implementation via crawler_strategy, it correctly describes the main relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the provided code examples, the 'arun' method is called on an 'AsyncWebCrawler' instance to perform crawling, shown in different configurations of the web crawler setup.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted describes arun() being called for crawling but misses the key aspect of browser selection through browser_type parameter that the ground truth emphasizes",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Since 'AsyncWebCrawler' uses the 'AsyncPlaywrightCrawlerStrategy' by default for operating with the 'chromium' browser as inferred by reference to browser types in the strategy's implementation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that AsyncPlaywrightCrawlerStrategy supports multiple browser types with Chromium as default, which aligns with the ground truth's explanation of browser selection implementation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a usage example of calling the 'arun' method of the 'AsyncWebCrawler' class, specifically highlighting its ability to use CSS selectors to extract content from a webpage.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that the arun method supports CSS selector-based content extraction from webpages. While the ground truth provides more implementation details, the core functionality is correctly understood in the prediction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While the documentation explicitly demonstrates usage of the 'arun' method, the method itself is part of the 'AsyncWebCrawler' class, implying the use of this class as the main context in which the method operates.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the connection between AsyncWebCrawler and the arun method, but misses the key relationship involving CSS selectors for content extraction, which is the main focus of the ground truth",
      "error_type": "key_omission"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly discusses the 'NlpSentenceChunking' class. It explains its purpose and usage, noting that it uses NLP models to split text into sentences, ensuring accurate sentence boundaries. An example is provided that shows how to instantiate and use this class.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of NlpSentenceChunking using NLP models for sentence splitting, which aligns with the ground truth's explanation of using NLTK's sent_tokenize for sentence boundary detection.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'NlpSentenceChunking' class extends 'ChunkingStrategy', which is implicitly necessary to understand the architecture. This is inferred from the pattern that new chunking strategies extend a base strategy interface.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that NlpSentenceChunking extends/implements ChunkingStrategy as a base interface/class. While it doesn't detail the chunk() method, it gets the fundamental inheritance relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet demonstrates using the `AsyncWebCrawler` class to maintain session state using `session_id`. The usage of `AsyncWebCrawler` is through context management, as shown in the example with `async with AsyncWebCrawler() as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of session management in AsyncWebCrawler through session_id parameter and context management, which aligns with the ground truth's explanation of how session state is maintained between requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is explicitly invoked in the documentation to perform crawling using a specified `session_id`. This evidences its direct implementation in handling URL requests while maintaining session context.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that arun() is used for URL crawling with session management through session_id parameter, which aligns with the ground truth's explanation of session management implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation shows calling `kill_session()` on `crawler.crawler_strategy` to terminate the session identified by `session_id`. This is a direct use case where the method is utilized to clean up session resources when the operations are complete.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the high-level usage of kill_session for cleanup, but misses the important implementation details about closing page/context objects and removing from sessions dictionary",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet describes using the 'crawler.arun' function. This indicates the use of an instance of the 'AsyncWebCrawler' class, which directly provides the 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic relationship between AsyncWebCrawler and the arun method, but misses the crucial dynamic content handling functionality (scrolling, form interactions, wait conditions) that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method from the 'AsyncWebCrawler' class is directly referenced in the code snippet as the function being used to perform scrolling and form interaction on the webpage.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method, but misses the core functionality of how it handles dynamic content through js_code and wait_for parameters. It incorrectly implies direct involvement in scrolling/form interaction rather than providing the framework for it.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class likely uses 'AsyncPlaywrightCrawlerStrategy' under the hood to execute JavaScript and handle dynamic content on pages, as indicated by the ability to pass JS code and wait conditions in 'arun'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles JavaScript and dynamic content, but incorrectly suggests it's used by an 'AsyncWebCrawler' class which isn't mentioned in the code or documentation. The ground truth more accurately describes how the class directly implements dynamic content loading through its smart_wait method.",
      "error_type": "incorrect_class_relationship"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly uses `await crawler.arun(...)`, indicating the direct invocation of the AsyncWebCrawler.arun() method.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description only states that the method is called with await, while the ground truth explains the core functionality of content filtering and processing HTML content based on parameters.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example uses an instance of 'crawler', presumed to be of type AsyncWebCrawler, hence implicating this class. It forms the object context for calling `arun`.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and arun method, but misses the core relationship about how the filtering parameters are implemented in the arun method, which is the main focus of the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses the `AsyncWebCrawler` class directly in the example code to initiate a crawling process, suggesting that this class is responsible for handling web crawling tasks, including capturing screenshots.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class handles web crawling tasks including screenshot functionality, which aligns with the ground truth's explanation of the class implementing screenshot capabilities through its arun() method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is explicitly used in the snippet as the main method for executing the crawl with the added functionality of taking a screenshot, as depicted by the inclusion of `screenshot=True` and `screenshot_wait_for=2.0` in the function call.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that the arun method handles screenshot capabilities through a screenshot parameter and includes the screenshot data in its results. The predicted description gives a usage example while the ground truth explains the implementation, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The snippet refers to `result.screenshot`, indicating that the `CrawlResult` class holds the resultant state of a crawl, including screenshot data if it was captured. This indicates that the `CrawlResult` class implicitly carries the screenshot functionality as part of its attributes.",
    "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult stores screenshot data as an attribute after crawling, which aligns with the ground truth's explanation of screenshot functionality being enabled through the screenshot field storing Base64 encoded image data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The `screenshot` attribute in `CrawlResult` is used in the snippet where it is checked and decoded to write the image to a file, showcasing its role in handling the results of a screenshot capture.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage image as a base64-encoded string that can be decoded into a PNG file as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that screenshot is used to store and decode captured images, matching the ground truth's explanation of it being a base64-encoded string for storing webpage screenshots.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document explicitly mentions `JsonCssExtractionStrategy` as a strategy that involves using CSS selectors, which aligns with its purpose as described in the code artifact.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between JsonCssExtractionStrategy and CSS selectors, but misses the crucial relationship between the code's implementation and the documentation's specific tips about selector testing and inspection",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation describes the 'Cosine Strategy,' which matches the name of the class 'CosineStrategy' and explains its purpose: using similarity-based clustering to identify and extract relevant content sections. This corresponds to the class implementation of a strategy for extracting content by leveraging cosine similarity.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy uses similarity-based clustering for content extraction, but fails to describe the specific 5-step workflow that is central to the ground truth description. The ground truth emphasizes the concrete implementation of these steps through specific methods.",
      "error_type": "incomplete_core_workflow"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' class inherits from the 'ExtractionStrategy' abstract base class. This association is inferred because the documentation mentions 'Cosine Strategy in Crawl4AI,' which generally implies extending from a common interface or abstract class like 'ExtractionStrategy.'",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, but misses the crucial aspect of the abstract class providing the foundational structure and interface for content extraction and parallel processing methods.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet shows an example using the `AsyncWebCrawler` class to manage the instantiation and execution of the web crawler, as evidenced by `async with AsyncWebCrawler() as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's basic instantiation pattern but misses its core functionality for handling complex page interactions, JavaScript execution, and session management described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Explicitly mentioned in the example with `await crawler.arun(...)`, which is used for loading content from a URL.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its basic URL loading functionality, but misses crucial aspects about dynamic page handling, JavaScript execution, session management, and wait conditions that are central to the complex interactions described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Implicitly related as it is a typical strategy instantiated by `AsyncWebCrawler` if no other strategy is provided. This assumption is based on the default behavior described in the `AsyncWebCrawler` constructor logic.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies a relationship with AsyncWebCrawler but focuses only on it being a default strategy. It misses the core relationship shown in the ground truth - that this class specifically implements the complex interaction capabilities demonstrated in the example code.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The method is called for cleaning up sessions, as shown in the snippet `await crawler.crawler_strategy.kill_session(session_id)`.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic purpose of cleaning up sessions but omits the crucial detail about closing browser resources (page and context objects) to prevent memory leaks during multi-page crawling",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The attribute is accessed for processing content after each load in the example, denoted as `result.cleaned_html`.",
    "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that cleaned_html is used to store/access processed HTML content after dynamic page loads, with the main functionality accurately captured",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is directly used in the example to create a context manager, as evidenced by `async with AsyncWebCrawler(verbose=True) as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies AsyncWebCrawler's context manager usage, it misses the core functionality of JavaScript execution and content extraction described in the ground truth. The ground truth emphasizes the class's role in dynamic content handling through js_code and wait_for parameters.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method AsyncWebCrawler.arun() is explicitly called in the example through `result = await crawler.arun(...)`, indicating its use in initiating a web crawl.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called for web crawling, but misses the crucial aspect of how it handles dynamic content through JavaScript execution and LLM extraction capabilities described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The LLMExtractionStrategy is specified as the extraction strategy for the crawling task in the example with `extraction_strategy=LLMExtractionStrategy(...).`",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy is used as an extraction strategy, but fails to capture that this is a class implementation that provides the core LLM-based extraction functionality with configurable providers, tokens, and instructions",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as a tool for extracting highly structured data from complex web pages. This corresponds to the class 'JsonCssExtractionStrategy' that uses schemas for web scraping tasks.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies JsonCssExtractionStrategy as a tool for structured data extraction, but misses the key connection to the tips in the documentation, especially regarding schema-based incremental development",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation explicitly mentions 'SlidingWindowChunking' as a class that uses a sliding window approach to create overlapping chunks. It describes its purpose, parameters, and a usage example, indicating that this class implements the described functionality.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of creating overlapping chunks using a sliding window approach and preserving context, which aligns with the ground truth's explanation of the implementation using window_size and step parameters.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'SlidingWindowChunking' class is a specific type of chunking strategy, suggesting it may extend a base class like 'ChunkingStrategy'. This is common design where specific strategies inherit from a more general strategy interface or abstract class.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted text correctly identifies that SlidingWindowChunking is a specific implementation of the ChunkingStrategy abstract base class, capturing the inheritance relationship. While it doesn't detail the sliding window functionality, it accurately describes the core class relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The class `CosineStrategy` is directly referenced in the documentation snippet as it describes its initialization parameters like `semantic_filter`, `word_count_threshold`, `sim_threshold`, etc. This indicates that the documentation is explicitly detailing how to configure an instance of the `CosineStrategy` class.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the documentation directly describes the CosineStrategy class configuration parameters, matching the ground truth's observation that the code implements these exact parameters with the same default values.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `CosineStrategy` is a specific implementation of `ExtractionStrategy`. This class isn't mentioned directly in the text, but since `CosineStrategy` extends `ExtractionStrategy`, it is necessary to understand the full functionality and structure `CosineStrategy` builds upon.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions establish that ExtractionStrategy is a base class that CosineStrategy implements/extends, with configuration happening through the constructor parameters. The predicted description captures this core relationship accurately, even if it's less detailed about the parameter passing mechanism.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet provided in the documentation directly references the 'arun()' method, demonstrating its use in configuring a crawler with specific timing controls like 'page_timeout' and 'delay_before_return_html'.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the high-level relationship that the arun() method handles timing controls through its parameters, even though it doesn't explicitly mention the **kwargs implementation detail.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun()' method is a part of the 'AsyncWebCrawler' class. While the documentation snippet doesn't explicitly mention the class, it is implicitly referenced through the method example, as 'arun()' is a method of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as part of AsyncWebCrawler but misses the core timing control functionality described in the ground truth. The prediction focuses only on the class-method relationship while omitting the key timing control parameters and their purpose.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'Use CSS Strategy' when the content structure is well-defined, which aligns with the purpose described for the JsonCssExtractionStrategy class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship between the CSS Strategy and well-structured HTML content as described in the ground truth, even though it doesn't elaborate on the implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation's recommendation to use 'LLM Strategy' for natural language text explicitly points to the LLMExtractionStrategy class, designed for handling and extracting information from unstructured text using language models.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that LLMExtractionStrategy implements the LLM Strategy option for handling natural language text processing, which aligns with the ground truth's main points about the class's purpose and functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation suggests 'Use Cosine Strategy' for mixed and complex content due to its relevance-based extraction capabilities, which match the purpose of the CosineStrategy class that focuses on content relevance using cosine similarity.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that CosineStrategy is designed for mixed/complex content and focuses on relevance-based extraction using cosine similarity. While it doesn't mention hierarchical clustering, this omission doesn't change the fundamental understanding of the relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly instantiated in the example with an asynchronous context manager, indicating it is a core part of the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's async context manager aspect but misses its core functionality of supporting multiple extraction strategies and processing HTML content into various formats (markdown, structured data, media). The context manager is just one implementation detail, not the main relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is directly called on an instance of `AsyncWebCrawler` multiple times in the example, indicating it performs the crawling action.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling but misses the key point about how it handles different extraction strategies, which is a crucial aspect highlighted in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is instantiated as an argument in the example within the `arun` method call, suggesting it modifies the extraction behavior during crawling.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy is used as an argument to modify extraction behavior, but misses the crucial aspect that it specifically implements language model-based structured data extraction with custom providers, schemas, and instructions",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "Similar to `LLMExtractionStrategy`, `JsonCssExtractionStrategy` is used within the `arun` method call to specify a json/css-based extraction logic.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as being used within arun() for extraction, but misses the key aspect of its specific purpose - extracting structured data from repeated HTML patterns using CSS selectors",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The output of the first `arun` call accesses `fit_markdown`, denoting its role in returning processed markdown from a crawl result.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used to return processed markdown from crawl results, which aligns with the ground truth showing it being used to store the main extracted content from webpage crawls",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "This attribute is accessed to retrieve structured and pattern data, which reflects its use for storing extracted data.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content stores extracted data, but misses the crucial detail that it specifically stores string data that needs to be parsed as JSON, as shown in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Although `media` is returned from the result object, it is used to provide media information from the crawl result.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that media is from the result object and contains media information from the crawl, which aligns with the ground truth's explanation that CrawlResult.media stores media items returned in the output",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation directly mentions the initialization of an `LLMExtractionStrategy` instance with specific parameters such as `provider` and `schema`, which closely matches the code structure in the `LLMExtractionStrategy` artifact.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the initialization aspects and parameters of LLMExtractionStrategy but misses the crucial functionality of using LLM completions to parse web content into structured format as described in the ground truth",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `arun` function is called on an `AsyncWebCrawler` instance in the documentation snippet to perform content extraction using the `LLMExtractionStrategy`, indirectly referencing the `AsyncWebCrawler` class that supports this asynchronous operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the structured content selection described in the documentation through its arun() method, which accepts an extraction_strategy parameter that can be configured with an LLMExtractionStrategy to extract specific content types using LLMs.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler's arun method is used to perform content extraction using LLMExtractionStrategy, which aligns with the ground truth's explanation of how the class implements structured content selection through the arun() method with extraction_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documented usage of `arun()` implies its implementation is within `AsyncWebCrawler`, demonstrating the method's role in executing the strategy and obtaining results.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an implementation method, but misses the key functionality of LLM-based content extraction which is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result from `await crawler.arun(...)` is expected to be in a format compatible with `CrawlResult`, from which `extracted_content` is later accessed to retrieve the extracted JSON.",
    "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that CrawlResult stores the extracted_content which comes from crawler.arun() execution and contains JSON data. While it's less detailed than the ground truth, it conveys the same core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation states that the JSON data is obtained from `result.extracted_content`, indicating this attribute provides access to the extracted article content.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content contains the extracted article content, but misses the crucial aspects that it's specifically stored as a JSON string matching a Pydantic schema for typed parsing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example directly uses AsyncWebCrawler in a context manager, explicitly mentioning it to demonstrate enabling verbose logging functionality.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions focus on the key aspect of verbose mode enabling logging functionality in AsyncWebCrawler. While the predicted description focuses on the example usage, it correctly captures the core relationship between verbose mode and logging capability.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method arun is invoked on an AsyncWebCrawler instance in the example provided. Although not explicitly discussed, it shows typical usage of the method.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun is used with AsyncWebCrawler but misses the core focus on verbose logging functionality that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, through its initialization, defaults to using AsyncPlaywrightCrawlerStrategy when no strategy is provided. This implicit usage is crucial for its functionality.",
    "ground_truth_relationship": "The code implements verbose logging functionality through the 'verbose' parameter in the AsyncPlaywrightCrawlerStrategy class, which when set to True prints crawling progress messages using print statements prefixed with '[LOG]'.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncWebCrawler's default strategy initialization, while the ground truth describes the verbose logging functionality implementation in AsyncPlaywrightCrawlerStrategy. These are completely different aspects of the code.",
      "error_type": "wrong_functionality"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet mentions the use of a 'crawler.arun' method, and this suggests the usage of the AsyncWebCrawler class, which has the 'arun' method for crawling given URLs. The explicit example of 'await crawler.arun(url=\"https://example.com\", ...)' directly ties to the AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between the documentation's crawler.arun method and the AsyncWebCrawler class, but misses the crucial aspect of dynamic content handling capabilities which is the main focus of the ground truth relationship.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet demonstrates using 'await crawler.arun(...)' indicating the arun method is used for dynamic content handling as detailed in the doc. The method 'arun()' within the AsyncWebCrawler class is responsible for processing the URL with attributes like 'wait_for', 'process_iframes', and 'js_code', aligning with the example provided.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles dynamic content through parameters passed via kwargs, which aligns with the ground truth's explanation of supporting wait conditions and JavaScript code execution",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy implicitly to achieve asynchronous crawling behavior as described. The mention of handling dynamic content relates to browser automation, which this class provides.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the async nature and browser automation capabilities, but misses the core focus on the specific dynamic content handling mechanisms (smart_wait, iframe processing, configurable delays) that are central to the ground truth relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly mentioned in the documentation snippet as it is shown being imported from `crawl4ai.extraction_strategy`. The snippet further uses this strategy to exemplify how different large language model (LLM) providers can be employed for data extraction, making it a central class in the context of this documentation piece.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy and its role in working with different LLM providers, but misses the key architectural relationship of how it implements this flexibility through the constructor parameters (provider and api_token) which is central to the ground truth description.",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the class `LLMExtractionStrategy`, indicating its use for employing various LLM providers by passing parameters such as `provider`, `api_token`, and `instruction`. This corresponds to the stated functionality of integrating with different providers and customizing the extraction process.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship between the documentation and code, correctly identifying that LLMExtractionStrategy allows customization of LLM providers through provider and API token parameters",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is a subclass that extends from `ExtractionStrategy`. This is implied as the `LLMExtractionStrategy` provides extraction functionalities that are outlined in the documentation. The base class establishes the common interface or structure for all extraction strategies, including the LLM-based one.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core inheritance relationship between ExtractionStrategy and LLMExtractionStrategy, as well as their purpose in the extraction framework. While it's more concise than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'CosineStrategy' as a strategy to be used for adjusting thresholds iteratively, choosing word count thresholds, optimizing performance, and handling different content types. The specific code examples shown indicate how parameters of the 'CosineStrategy' class should be configured.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy implements configurable parameters for threshold adjustment and content handling as described in the documentation. Both descriptions emphasize how the class parameters align with the documented best practices for different content types and use cases.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "'CosineStrategy' extends 'ExtractionStrategy', and this relationship is implicitly understood because 'CosineStrategy' is being used as a strategy within the context of content extraction as shown in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on CosineStrategy inheritance which isn't mentioned in ground truth. However, it correctly implies the strategy pattern for content extraction, though misses the key point about configurable parameters through kwargs.",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet directly describes the 'RegexChunking' class and details its functionality, including splitting text using regular expressions and when to use it, making it an explicit mention.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship by stating that RegexChunking splits text using regular expressions, which aligns with the ground truth's explanation of using re.split() with patterns for text splitting.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of the `AsyncWebCrawler` class in the example code as part of the context manager 'async with AsyncWebCrawler(verbose=True) as crawler:', showcasing how to initialize and use it to perform crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler class usage but focuses on initialization rather than the key error handling functionality described in the ground truth. While not contradictory, it misses the central relationship between the class and error handling pattern.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly instantiated within the example provided in the documentation. It sets configuration parameters like provider and api_token to integrate with LLM APIs, showcasing how to use this strategy.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy as a class for LLM API integration, but misses the core focus on error handling and parallel processing with ThreadPoolExecutor that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` is invoked when performing retry-based extraction, though not directly named in the snippet, it's part of the crawl process shown in the example with: 'await crawler.arun(...)'.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's involvement in the retry process but misses the core focus on its error handling implementation, which is the main relationship highlighted in the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet in the documentation explicitly shows the usage of the 'AsyncWebCrawler' class to configure and launch a web crawler with specific options like 'headless', 'verbose', and 'sleep_on_close'. The class is directly instantiated, making its usage explicit in the context of browser configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler supports browser configuration options through instantiation, while the ground truth explains the implementation details of how these options are handled internally. The core relationship of configuration option support is consistent between both descriptions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the 'AsyncWebCrawler' class is invoked within the snippet to perform a crawl operation on 'https://example.com'. While the method itself is not discussed in detail, its usage is demonstrated implicitly in the context of the example. This indicates that 'arun' is a key function of the 'AsyncWebCrawler' class.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies 'arun' as a key method of AsyncWebCrawler used for crawling, but misses the crucial relationship between the configuration parameters and how they are handled through the constructor and method parameters",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' is noted to either take a 'crawler_strategy' or default to using 'AsyncPlaywrightCrawlerStrategy'. This relationship is implied, as the crawler setup naturally cascades to use this strategy for operations, although 'AsyncPlaywrightCrawlerStrategy' is not directly mentioned in the text snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on a different relationship (AsyncWebCrawler using AsyncPlaywrightCrawlerStrategy as default), while the ground truth describes how AsyncPlaywrightCrawlerStrategy implements specific browser configuration options (headless, verbose, sleep_on_close) from the documentation.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet demonstrates the creation of an instance of 'AsyncWebCrawler' class.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies creation of AsyncWebCrawler but misses the core functionality shown in the documentation - using it as an async context manager with arun() method for web crawling",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the 'AsyncWebCrawler' is directly called in the code example to perform the crawl operation.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() method but misses the crucial aspects of what the method actually does (HTML extraction, caching, processing) and what it returns (CrawlResult object) as described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet accesses 'markdown' from the 'CrawlResult' object returned by 'arun()', demonstrating how results are accessed.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the markdown attribute stores/contains the output text content from the crawling result which can be accessed and printed",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides a code example where an instance of the `AsyncWebCrawler` class is created and used in an asynchronous context manager to manage web crawling operations. Evidence can be found in the line `async with AsyncWebCrawler() as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's async context manager capability but misses significant aspects described in the ground truth, notably its support for both structured JsonCssExtractionStrategy and LLM-based content extraction functionality",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is explicitly called in the example to perform both structured and semantic content extraction. This is shown through lines like `pattern_result = await crawler.arun(...)` and `analysis_result = await crawler.arun(...)`.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used for content extraction and crawling functionality as shown in the documentation example. While it doesn't mention all implementation details, it captures the core relationship between the method and its usage.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation directly indicates the usage of the `JsonCssExtractionStrategy` class as an extraction strategy when calling `arun()`, as seen in `extraction_strategy=JsonCssExtractionStrategy(article_schema)`.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct usage of JsonCssExtractionStrategy in the documentation example but misses explaining the core functionality of schema-based CSS selector extraction that the ground truth emphasizes",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The example uses `LLMExtractionStrategy` as a semantic analysis tool, which is clearly shown in the block `extraction_strategy=LLMExtractionStrategy(...)`, highlighting its role in semantic content extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's core role as a semantic analysis tool, which aligns with the ground truth's explanation of it implementing semantic analysis functionality with custom providers and schemas. While the predicted description is less detailed, it captures the essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The data attributes of `CrawlResult`, specifically `extracted_content` and `media`, are used implicitly through the `pattern_result` and `analysis_result` variables. They derive from the `arun` method, which likely returns `CrawlResult` instances.",
    "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how CrawlResult stores extraction outputs and media assets from the crawler's arun method. While more concise, it conveys the same core relationship shown in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the provided text snippet, the class 'AsyncWebCrawler' is explicitly mentioned in the context of initializing a web crawler instance with specific parameters such as headless mode and bypassing cache.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class but only describes initialization parameters, missing the core functionality of overlay removal described in the ground truth",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is implicitly invoked in the usage example to perform the crawling operation. The text depicts it handling specific parameters like remove_overlay_elements and screenshot to fit and adjust content.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that arun() method handles overlay removal and content fitting through its parameters, matching the ground truth's explanation of the relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The document example indirectly uses 'AsyncPlaywrightCrawlerStrategy' through 'AsyncWebCrawler'. The latter initializes a strategy to handle crawling, which by default employs the 'AsyncPlaywrightCrawlerStrategy'.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, while the ground truth describes the specific overlay removal functionality implemented in the remove_overlay_elements method. These are completely different aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides a usage example with the 'AsyncWebCrawler' class, explicitly showing how to use this class within an asynchronous context manager (`async with`). It indicates that 'AsyncWebCrawler' is utilized for performing web crawls.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as a class used for web crawling with async context manager support, but misses the crucial relationship with proxy and magic mode features passed through kwargs to AsyncPlaywrightCrawlerStrategy",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes an example that calls the 'arun' method of the 'AsyncWebCrawler' instance (`await crawler.arun(..)`). This implies that 'arun' is a method used in conjunction with 'AsyncWebCrawler' to execute web crawling operations.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as a method used with AsyncWebCrawler for web crawling, but misses the crucial aspect that it specifically implements proxy and magic mode functionality through crawler settings and anti-detection features.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example illustrates the use of the `AsyncWebCrawler` class to handle asynchronous crawling while setting custom headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of AsyncWebCrawler handling asynchronous crawling with headers, but misses the crucial detail about how headers are passed through kwargs to AsyncPlaywrightCrawlerStrategy",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation example shows the invocation of the `arun` method on an instance of `AsyncWebCrawler` to execute a crawl operation on a specified URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic functionality of arun() for crawling URLs, but misses the key aspect about custom header support that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses an instance of `AsyncWebCrawler` within an `async with` context manager. This class is responsible for handling the web crawling operations, as evidenced by its usage in the `async with AsyncWebCrawler()` line.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used with an async context manager pattern and handles web crawling operations, which aligns with the ground truth's explanation of the class providing async context manager functionality through __aenter__ and supporting proxy updates.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is explicitly called in the snippet (`result = await crawler.arun(url=url)`), indicating its role in executing a crawling operation over a URL.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method for crawling URLs, but misses the significant detail about its integration with proxy configuration mentioned in the ground truth",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet provides usage examples with AsyncWebCrawler, demonstrating how it can be configured with different browser types like 'chromium', 'firefox', and 'webkit'. This indicates its role in defining browser configurations directly.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports different browser types, but mischaracterizes how this is implemented - it's not configured directly but through the crawler_strategy parameter",
      "error_type": "implementation_misunderstanding"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example includes calling 'arun' method to perform crawling with different browser configurations. The method is not explicitly discussed, but it is implied through its usage in executing crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes arun() as a method used for crawling, but misses the key relationship about browser_type selection documented in the configuration guide. It fails to mention the core functionality of selecting different browser engines.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy by default unless another strategy is provided. This underlying strategy is responsible for implementing browser-specific features.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description states that AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy by default, while the ground truth describes how AsyncPlaywrightCrawlerStrategy implements browser type selection internally. These are different relationships - one about default strategy usage, the other about browser implementation details.",
      "error_type": "different_relationship_focus"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The document text provides an example that uses the 'arun' method from an instance of 'AsyncWebCrawler'. This indicates the use of the 'AsyncWebCrawler' class, which is instantiated internally and used to perform crawling operations with anti-detection features.",
    "ground_truth_relationship": "The documentation describes anti-detection features which are implemented through optional kwargs parameters passed to the AsyncWebCrawler's constructor and arun method, which are then forwarded to the crawler_strategy for handling stealth behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class handles crawling operations with anti-detection features, which aligns with the ground truth's explanation that these features are implemented through parameters passed to the crawler's methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is directly called in the example provided in the documentation. It implements crawling functionality with options to simulate user behavior and apply anti-detection techniques.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and anti-detection, but misses that the stealth features are implemented through specific keyword arguments that get passed to crawler_strategy.crawl()",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly references the `arun` method as the function to invoke for CSS and JavaScript-based waiting. The examples demonstrate its usage with the 'wait_for' parameter specifying conditions.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is used for CSS and JavaScript-based waiting functionality, which aligns with the ground truth's explanation that arun() implements waiting through its kwargs parameter for both CSS and JavaScript wait conditions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler.arun()` method's functionality for waiting conditions implies it uses strategies defined in `AsyncCrawlerStrategy`, albeit not explicitly stated. This provides the protocol for implementing different strategies like awaiting condition.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy enables waiting functionality through its crawl method, with the predicted version correctly noting this happens through strategy implementation. Minor differences in specifics don't change the fundamental understanding.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "`AsyncPlaywrightCrawlerStrategy` extends `AsyncCrawlerStrategy` and likely provides concrete implementations relevant to the waiting strategies like CSS and JavaScript waiting mentioned in the documentation. It serves as a practical example of an active strategy employed by `arun()`.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy and implements the waiting functionality described in the documentation. While it's more general than the ground truth, it captures the core relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation directly shows the usage of 'await crawler.arun(url=\"https://example.com\")' to initiate a crawl, which ties into the AsyncWebCrawler.arun() method responsible for processing the given URL using crawling strategies.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling URLs, but misses the crucial aspect that it specifically enables link analysis functionality and returns categorized link data as described in the documentation",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The text snippet describes accessing the 'links' attribute of the CrawlResult, categorizing them into internal and external links, aligning directly with CrawlResult.links functionality of storing and organizing link data.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that CrawlResult.links stores and organizes link data into categories. While it doesn't detail all link types and properties mentioned in the ground truth, it correctly identifies the main functionality of categorizing and storing link data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Documentation implies the use of a result object when analyzing and categorizing links after a crawl, which is implicitly handed as a CrawlResult object that stores crawled page data, including link classifications.",
    "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that CrawlResult stores crawled data including categorized links, which enables the link analysis functionality described in the documentation. While it's less detailed than the ground truth, it conveys the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet in the documentation directly shows the usage of the `arun` method from the `AsyncWebCrawler` class by calling `await crawler.arun(url=\"https://example.com\")`. This indicates that the documentation is explaining how to use this method to perform actions within the web crawler.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for web crawling, but fails to mention its crucial role in media selection and structuring media data into the CrawlResult object, which is the main focus of the documented functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "While not explicitly mentioned by name, the `result` variable in the documentation example is of type `CrawlResult`. This is evidenced by accessing properties like `media`, which aligns with attributes defined in the `CrawlResult` class. This class encapsulates the results of the `arun` method, indicating an implicit return relationship.",
    "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies CrawlResult as storing the result of arun(), but misses the main focus of the ground truth which is about media selection functionality through the media dictionary field",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation explicitly mentions accessing `media` properties such as images, videos, and audios from the `result`. This matches `CrawlResult.media`, indicating that `media` is an attribute of the `CrawlResult` class, which provides a structured way to access different media types.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies media as an attribute of CrawlResult that provides structured access to different media types (images, videos, audios), matching the ground truth's explanation of the media dictionary property's organization and usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation directly uses this class by instantiating an object and performing a web crawling task using its `arun` method.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic use of the class through instantiation and arun() method, but misses the significant implementation details about content filtering, processing, and cache control functionality that are key aspects mentioned in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet shows a direct call to the `arun` method of the `AsyncWebCrawler` class to initiate the crawling process.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text identifies the basic relationship of arun() being called, but misses the crucial connection that the code is actually showing how all the documented parameters are implemented in the method definition",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "`AsyncWebCrawler` utilizes a `crawler_strategy` which defaults to `AsyncPlaywrightCrawlerStrategy`, an implementation of `AsyncCrawlerStrategy`. This is implicit as the snippet does not discuss the abstract strategy directly but relies on its interface via the strategy pattern.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the comprehensive crawling functionality demonstrated in the documentation example, including URL processing, screenshot capture, and hook management.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the strategy pattern usage but focuses too narrowly on implementation details about AsyncPlaywrightCrawlerStrategy rather than the core interface definition that the ground truth emphasizes",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` is used as the default strategy for crawling when instantiated by the `AsyncWebCrawler` in the absence of a specified strategy. This provides concrete behavior for the abstract `AsyncCrawlerStrategy` interface.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class directly implements all the crawler features shown in the example documentation, including content filtering, iframe processing, and cache control through its comprehensive crawl() method that handles parameters like process_iframes, bypass_cache, and excluded_tags.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as implementing crawler functionality, but mischaracterizes it as being the 'default' strategy when not specified. The ground truth focuses on its direct implementation of comprehensive crawling features without mentioning default status.",
      "error_type": "incorrect_assumption"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation processes the result of the `arun` method call, which returns a `CrawlResult` object. The text accesses various properties of this object to check for success and print results.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that CrawlResult stores crawling outputs that are accessed in the example code, mentioning key aspects like success status and result processing. While it doesn't list all properties, it gets the core relationship right.",
      "error_type": null
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Inside a successful crawl check, the documentation example appears to use the `markdown` attribute to output a portion of the content extracted during the crawl.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core concept that the markdown attribute is used to output crawled content from a successful crawl, which aligns with the ground truth's explanation. The omission of specific details like the 500 character limit doesn't impact the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation processes images by iterating over the `media['images']` list from the `CrawlResult` demonstrating an explicit usage.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that images are processed by iterating over the media['images'] list from the CrawlResult, which aligns with the ground truth's explanation of how media elements are stored and accessed",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation specifies accessing and printing internal links from `CrawlResult` using its `links['internal']` attribute.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that internal links can be accessed via links['internal'], but misses that the links dictionary stores both internal and external categorized links",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In case of a failed crawl, the `error_message` attribute of the `CrawlResult` is accessed to print error details, as shown in the example.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that error_message is used to communicate failure reasons when the crawl fails (result.success is False). The predicted description focuses on the same error handling relationship shown in the example code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' from 'AsyncWebCrawler' is explicitly mentioned in the documentation example where it's used to perform a crawling operation and the result is accessed for its 'fit_markdown' attribute.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() functionality but misses the key focus on markdown extraction and content processing. It only mentions accessing fit_markdown attribute without explaining the core transformation process.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation highlights 'fit_markdown' as a key result of using 'arun()', indicating how 'arun()' provides this processed markdown content.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies fit_markdown as output from arun(), but misses the key purpose of storing cleaned/extracted main content in markdown format with boilerplate removed",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation mentions the usage of 'AsyncWebCrawler' as part of the 'integrated_js_and_wait_crawl' function, emphasizing its role as the crawler context manager.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's usage in the context manager pattern but misses crucial aspects about its integrated JavaScript execution and content extraction capabilities mentioned in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' is explicitly mentioned in the documentation as the extraction strategy used for the crawler session.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the use of JsonCssExtractionStrategy but misses the core functionality of how it implements structured data extraction based on the schema. It focuses only on its presence in the documentation rather than its actual role.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' of 'AsyncWebCrawler' is explicitly used within the provided code snippet to run the crawling operation with specified parameters.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling but misses the crucial aspect of integrated JavaScript execution and waiting functionality described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "Although not directly discussed, 'kill_session()' of 'AsyncPlaywrightCrawlerStrategy' is invoked in the example to terminate the crawling session.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core function of kill_session as being used to terminate the crawling session, which aligns with the ground truth's explanation of cleaning up browser resources. While the ground truth provides more detail about closing page and context objects, the high-level relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` class is explicitly mentioned in the provided snippet as a strategy for pattern-based extraction. The snippet provides an example of its instantiation using a JSON schema configuration, demonstrating its role in the extraction process.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as a pattern-based extraction strategy that uses JSON schema configuration, which aligns with the ground truth's explanation of it implementing pattern-based scraping using schema-defined selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` from `AsyncWebCrawler` is shown being used in the snippet to run the extraction strategy, even though it is not mentioned by name in the text. The `arun` method is implicitly used in the context of executing an extraction strategy passed as an argument in the snippet.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that arun() is used to execute extraction strategies passed as arguments. While it's less detailed than the ground truth, it doesn't contradict or misunderstand the fundamental relationship between arun() and extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The `extracted_content` attribute of `CrawlResult` is used to access the results of the extraction performed by `arun`. It is not explicitly named in the snippet, but it is inferred that the value returned by `arun` with the extraction strategy would involve accessing this attribute.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is used to access extraction results, but misses the crucial aspect that it specifically stores JSON-formatted pattern matches based on the schema structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is directly invoked in the provided code snippet within the documentation, evidencing its usage to obtain the raw HTML from a URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() being used to get HTML content, but presents it as directly showing code usage rather than describing the implementation relationship between arun() and raw HTML retrieval functionality as stated in the ground truth",
      "error_type": "incomplete_relationship_characterization"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The `html` attribute of `CrawlResult` is accessed in the example `print(result.html)` to retrieve and print the complete HTML content crawled by `arun()`.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted text correctly captures the core relationship that html is an attribute/property that stores and provides access to the complete HTML content from a crawled page. While it doesn't mention all details about preservation and custom processing, it accurately describes the basic functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the `JsonCssExtractionStrategy` as a feature for extracting structured data from web pages using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states the strategy uses CSS selectors to extract structured data, but omits the crucial implementation details about using BeautifulSoup and the schema structure with baseSelector and fields that are central to how it works",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text explicitly discusses the usage of `JsonCssExtractionStrategy` with `AsyncWebCrawler`, indicating that the latter employs the former to perform extraction tasks based on CSS selectors.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler uses JsonCssExtractionStrategy for CSS selector-based extraction, which aligns with the ground truth's explanation of how the code implements and executes JsonCssExtractionStrategy in the aprocess_html method.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While not mentioned explicitly, `JsonCssExtractionStrategy` is a subclass of `ExtractionStrategy`. This base class provides the abstract interface for extraction strategies, enabling the functionality described.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of the base class's role in providing structured data extraction and parallel processing functionality that the ground truth emphasizes",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly mentions using the JsonCssExtractionStrategy with a schema for structured data extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core concept of JsonCssExtractionStrategy using a schema, but omits the crucial specifics about extracting cryptocurrency data and the structure of the schema (baseSelector for rows, fields for crypto name/symbol/price)",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet illustrates using AsyncWebCrawler to perform the crawling task with a specified extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic concept of AsyncWebCrawler performing crawling with an extraction strategy, but misses the crucial context that this is specifically implementing cryptocurrency price extraction functionality from Coinbase using JsonCssExtractionStrategy schema",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method arun of AsyncWebCrawler is being called to perform the crawling with the set extraction strategy. This is not mentioned explicitly but is clearly demonstrated in the example.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that arun() is the method used for performing crawling with extraction strategy, which aligns with the ground truth's description of it implementing core crawling functionality with strategy application. While the predicted is less detailed, it doesn't contradict the main functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example accesses the extracted_content attribute of the result obtained from the AsyncWebCrawler arun method to parse the extracted cryptocurrency prices.",
    "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that extracted_content from the AsyncWebCrawler result contains cryptocurrency data that gets parsed, which aligns with the ground truth's explanation of storing scraped crypto data that gets parsed into JSON.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'CosineStrategy' as the extraction strategy being used. It details initializing a CosineStrategy with specific parameters such as 'semantic_filter', 'word_count_threshold', and 'sim_threshold'.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that CosineStrategy is a configurable extraction strategy with parameters like semantic_filter, word_count_threshold, and sim_threshold, matching the implementation shown in the code and usage example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows the 'arun' method being called on an 'AsyncWebCrawler' instance within an 'async with' context block. This demonstrates how the 'arun' method of the 'AsyncWebCrawler' class is utilized in conjunction with the 'CosineStrategy'.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the async nature and basic usage pattern but misses the core functionality of URL processing, extraction, and CrawlResult return which are central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly named in the snippet, the method call 'arun' is shown as part of an 'AsyncWebCrawler' class, implying the usage of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun method and AsyncWebCrawler class relationship, but misses crucial aspects like the async context manager functionality and the extraction strategy parameter usage that are key parts of the documented API pattern.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions the `AsyncWebCrawler` as a tool to use Language Models (LLMs) for extracting structured data asynchronously.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of AsyncWebCrawler as a tool for LLM-based structured data extraction from web pages asynchronously, which aligns with the ground truth's description of its functionality and LLM extraction support.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly mentioned as a specific strategy used with the `AsyncWebCrawler` to demonstrate the use of LLMs for extraction purposes.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between LLMExtractionStrategy and AsyncWebCrawler for using LLMs to extract web content, matching the ground truth's explanation of implementing asynchronous web content extraction using Language Models.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses the `AsyncWebCrawler` class to initialize an asynchronous web crawler within the example code.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the AsyncWebCrawler class and its usage, it fails to mention the key async context manager pattern (__aenter__/__aexit__) and the core arun method functionality that are central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code snippet, the `arun` method of the `AsyncWebCrawler` class is invoked to crawl the specified URL. This demonstrates the method's use explicitly.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship between the code and documentation (arun method being used for crawling URLs), but misses crucial aspects like content extraction, caching, and return of CrawlResult object that are key parts of the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` class uses the `AsyncPlaywrightCrawlerStrategy` as its default strategy, implicitly involving it when crawling functions are executed.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the core implementation for the documented AsyncWebCrawler usage example by managing browser automation, page navigation, and content extraction through the crawl() method that processes the URL parameter shown in the basic usage documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly notes the connection between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, it oversimplifies by only mentioning implicit usage. The ground truth explains that this strategy class provides the core implementation for browser automation, page navigation, and content extraction - crucial functional aspects that are missing from the prediction.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation text provides an example usage of the `arun` method from the `AsyncWebCrawler` class to execute JavaScript code (both single and multiple commands). This aligns directly with the usage of `arun` for implementing the described functionality.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for JavaScript execution, but misses explaining that it's implemented through kwargs and crawler_strategy.crawl(), instead just stating it 'aligns directly' with the functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While the `arun` method is explicitly demonstrated, it is part of the `AsyncWebCrawler` class, which is implicitly involved in executing the JavaScript crawling strategy as shown in the examples.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented JavaScript execution functionality through its arun method, which accepts a 'js_code' parameter and passes it to the underlying crawler_strategy for execution before webpage scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method in AsyncWebCrawler handles JavaScript execution functionality, even though it doesn't explicitly mention the js_code parameter. The core relationship and functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` class instances often use `AsyncPlaywrightCrawlerStrategy` by default for handling the actual crawling process, which may include executing JavaScript code as configured in `arun`.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly indicates JavaScript execution capability but incorrectly suggests it's specifically tied to AsyncWebCrawler defaults rather than being a feature implemented in the crawl() method that accepts js_code parameter",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions using `JsonCssExtractionStrategy` to efficiently extract structured data from web pages. It demonstrates defining a schema for extraction and setting it up with the `JsonCssExtractionStrategy` class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core purpose of JsonCssExtractionStrategy for structured data extraction using schemas and CSS selectors, which aligns with the ground truth's explanation of how it processes schemas for dynamic web page extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet provides an example using `AsyncWebCrawler` in combination with the extraction strategy to crawl a webpage and extract data. It establishes a context where this class is utilized.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and mentions extraction strategy, but misses the key functionality of JavaScript execution, waiting conditions, and dynamic data extraction that are central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example script in the documentation calls the `arun` method on an `AsyncWebCrawler` instance to perform the web crawling and data extraction task.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function of arun() for web crawling but misses the key aspect about its support for dynamic content extraction and js_code execution emphasized in the ground truth",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is explicitly referenced in the documentation as the method used to perform crawling and is shown in an example where it returns a `CrawlResult`.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() returns a CrawlResult but misses discussing the core functionality of processing webpage content through extraction/chunking strategies. It describes the method more superficially based just on the docs.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation mentions that the `arun()` method returns a `CrawlResult` object, which is used to access different properties like `html`, `cleaned_html`, and others.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the CrawlResult object returned by arun() provides access to various properties, which aligns with the ground truth showing these properties are defined in the class. While it doesn't list every property, it captures the essential relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "Via the example in the documentation, `result.html` is used to access the raw HTML, but not directly discussed.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that html is accessed through result.html and represents raw HTML content, which aligns with the ground truth's description of html being a string property storing raw HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The `cleaned_html` attribute is accessed in the documentation example to demonstrate another format of the crawled content.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies cleaned_html as a format of crawled content, which is true, but misses the crucial aspect that it's an optional string property specifically containing sanitized HTML content",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation example accesses `result.markdown` indicating its use to get a markdown version of the crawled content.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that result.markdown provides a markdown version of the crawled content, which aligns with the ground truth. While the ground truth provides more detail, the predicted captures the essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The `fit_markdown` is shown being accessed in the example, suggesting its use to obtain the most relevant content in markdown form.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that fit_markdown contains content in markdown form and that it's accessed as shown in the example. While it doesn't explicitly mention it's optional or that it contains 'most relevant' content, it does imply the latter, and these omissions don't change the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation uses `result.success` in a code example to check if the crawl operation was successful.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that result.success is used to check crawl operation success, which aligns with the ground truth's description of it being a boolean field indicating successful completion.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The documentation example references `result.status_code` to demonstrate how one would check the HTTP status of a crawl.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that status_code is used to check the HTTP status of a crawl, which aligns with the ground truth's explanation of it storing the HTTP status code from web requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The `media` property is shown in the documentation as a way of accessing media discovered during the crawl.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the media property shown in the documentation is for accessing media from crawl results, which aligns with the ground truth's explanation that the media dictionary stores media elements discovered during crawling",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The example in the documentation accesses `result.links` to retrieve the links gathered during the crawl process.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that links are accessible via result.links, but misses that it's specifically a dictionary containing both internal and external links",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation suggests using Cosine for content relevance. This indicates a usage relationship with the 'CosineStrategy', which is intended for clustering and filtering content based on semantic similarity.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for content relevance and semantic similarity, which aligns with the ground truth's explanation of how the class implements cosine similarity metrics for content filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The mention of using CSS for structured data aligns with 'JsonCssExtractionStrategy', which focuses on extracting data using JSON and CSS patterns.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements CSS-based extraction for structured data, which aligns with the ground truth's explanation of using BeautifulSoup selectors for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation mentions using LLM for complex interpretation, aligning with the functionality of 'LLMExtractionStrategy' which extracts data using language models.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on LLM's ability to handle complex interpretation, which is present but not the main relationship shown in the ground truth. The ground truth specifically highlights the error handling functionality, which is a distinct aspect.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example in the documentation demonstrates using the 'AsyncWebCrawler.arun()' method to run extraction strategies and handle results, indicating its role in orchestrating the crawl operations with given strategies.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic role of arun() method, but misses the key focus on error handling patterns and caching that are central to the ground truth relationship with the documentation",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The example checks 'result.error_message' implying its usage in providing error details from the 'CrawlResult'.",
    "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies error_message's role in error reporting but misses its key connection to the success flag and the best practice pattern shown in the documentation",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example uses 'result.extracted_content' to access JSON data, implying reliance on this attribute for parsing extracted data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that extracted_content is used to access extracted data that can be parsed as JSON, which aligns with the ground truth's description of how the field stores and handles crawled data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly provides a code example using the `arun` method of the `crawler` object to configure anti-detection features with parameters `simulate_user` and `override_navigator`.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture that the arun() method supports manual anti-bot configuration options through parameters like simulate_user and override_navigator, with the predicted description providing a concrete code example that aligns with the documented functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` utilizes `AsyncPlaywrightCrawlerStrategy` as its default `crawler_strategy`, which includes functionality for various anti-detection settings. It implicitly supports the parameters demonstrated in the documentation example.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly indicates the existence of anti-detection settings, it incorrectly states these are implicitly supported by default, when in fact they must be explicitly enabled through parameters like simulate_user and override_navigator",
      "error_type": "incorrect_assumption_of_default_behavior"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions 'crawler.arun()', indicating the use of the 'AsyncWebCrawler' class for executing the 'arun' method as part of media processing and handling lazy-loaded content.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the use of AsyncWebCrawler's arun method, but misses the crucial detail that media processing is specifically handled through the aprocess_html method and WebScrappingStrategy, which is the core mechanism described in the ground truth",
      "error_type": "omission_of_key_mechanism"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a sample code using 'result = await crawler.arun(url=\"https://example.com\")', explicitly demonstrating the use of the 'arun()' method.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method as demonstrated in the documentation, but misses crucial functionality aspects like media processing, lazy-loading handling, caching and screenshot capabilities that are core to the relationship between code and documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation example accesses 'result.media[\"images\"]' to retrieve image details, indicating that 'CrawlResult.media' is used to store media-related results.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult.media is used to store media-related results and shows correct access pattern for images. While it doesn't detail all the metadata fields, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly shown in the provided code example as the context manager for crawling a protected site.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as a context manager but omits the crucial functionality around protected site crawling with specific parameters that the ground truth describes",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun method is explicitly called in the example, demonstrating how the AsyncWebCrawler is used to execute the crawling operation with specific parameters for a protected site.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses crucial aspects about its error handling, caching, and configurable parameters that enable protected site crawling. While it correctly notes the method usage, it oversimplifies the relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is the default strategy used by AsyncWebCrawler if no strategy is provided during instantiation, as inferred from the AsyncWebCrawler constructor.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description incorrectly states that AsyncPlaywrightCrawlerStrategy is the default strategy of AsyncWebCrawler, while the ground truth explains that it's a class implementing protected site crawling functionality using headless browser automation with specific features like popup removal and timeouts.",
      "error_type": "wrong_functionality"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows creating an instance of `AsyncWebCrawler` using an authenticated proxy configuration, demonstrating its capability to work with proxies.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that AsyncWebCrawler supports authenticated proxy configuration, as demonstrated in the example. While it doesn't mention the internal details about AsyncPlaywrightCrawlerStrategy, this omission doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the explicit use of `AsyncWebCrawler`, the method `arun` is invoked as part of the crawling process, implicitly highlighted through the example which issues a web crawl using an authenticated proxy.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun method is used for web crawling with proxy support through AsyncWebCrawler, which matches the ground truth's explanation of proxy configuration and implementation through the crawler strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Given that `AsyncWebCrawler` uses an instance of `AsyncPlaywrightCrawlerStrategy` by default if no strategy is provided, it is implicitly used in the context of the proxy configuration and crawl operation.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the general context of proxy configuration but misses the key implementation detail that it's specifically about authenticated proxy support through ProxySettings with server/username/password credentials in the start() method.",
      "error_type": "incomplete_mechanics"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides sample code that directly calls the method 'arun()' on an instance of the 'AsyncWebCrawler' class, using a crawler to perform a web crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() method usage correctly but misses the key functionality of domain-based filtering described in the ground truth, which is a crucial aspect of the relationship being documented.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'arun()' method to perform a web crawl with specific domain exclusion parameters, showing its use case.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as performing web crawling but fails to mention the core domain filtering functionality described in the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'LLMExtractionStrategy' class is explicitly mentioned in the provided text snippet as the strategy instantiated for extracting product details using language models.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the LLMExtractionStrategy class and its basic purpose for extraction, but fails to capture the core implementation details about processing HTML content through language models and extracting structured data according to schemas/instructions.",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is used in the provided example code to execute the extraction process with the `LLMExtractionStrategy`, indicating implicit usage.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() works with LLMExtractionStrategy but fails to capture its core role in handling URL fetching, caching, and coordinating the extraction process",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Given 'LLMExtractionStrategy' is traced explicitly and extends 'ExtractionStrategy', it implicitly forms part of the type hierarchy for handling extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy providing the base infrastructure for LLMExtractionStrategy's implementation. While the ground truth provides more detail about the specific methods and functionality, the predicted description captures the essential class relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The text explicitly mentions the 'RegexChunking' strategy for splitting text using a regex pattern, demonstrating its usage in the example code.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that RegexChunking uses regex patterns to split text, but misses mentioning the default pattern and the key functionality of iteratively splitting across multiple patterns",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example code implicitly utilizes the 'AsyncWebCrawler' class. It is instantiated with a 'chunking_strategy' argument, showcasing how the RegexChunking strategy is applied within its 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the AsyncWebCrawler class implements RegexChunking functionality through its arun() method where chunking_strategy can be specified. The predicted description captures the core relationship between the class and the chunking strategy feature.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function call to 'arun' reflects the use of 'RegexChunking' as a parameter, which is part of the 'AsyncWebCrawler' interface.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that RegexChunking is used as a parameter in arun(), but misses the key aspects that it's a default value and that there's type validation against ChunkingStrategy",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions the `AsyncWebCrawler` class as supporting session-based crawling through the `session_id` parameter and related methods. This aligns with the class being responsible for managing crawling sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncWebCrawler supports session-based crawling through the session_id parameter, which matches the ground truth's explanation of maintaining persistent browser sessions across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is implicit in the documentation as it discusses maintaining state across multiple `arun` calls through a `session_id`. The method is crucial for making crawling requests that maintain session state.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that arun() supports session-based crawling through session_id, enabling state maintenance across requests. The predicted description correctly identifies the main functionality, even if it's more general.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, `AsyncCrawlerStrategy` is significant as the base class for strategies that enable session-based features mentioned in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class for session-based features, but omits the crucial aspect that it specifically defines the core interface through abstract methods that implement the session capabilities.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This implementation of `AsyncCrawlerStrategy` likely extends the session management capabilities necessary for interaction mentioned in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the implementation extends session management capabilities for interaction as mentioned in the documentation. While it's less detailed than the ground truth, it captures the core relationship between the code and documentation - implementing session-based crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly instantiates the AsyncWebCrawler class to demonstrate how to combine proxy settings with magic mode. This class represents the primary component for initiating and managing a crawl session.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies AsyncWebCrawler as the main class for crawling, it misses the key relationship about how proxy and magic mode parameters are passed through kwargs to AsyncPlaywrightCrawlerStrategy. The description focuses only on instantiation without explaining the parameter handling mechanism.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet implicitly uses the arun method when demonstrating the functionality of crawling a URL while applying settings such as 'magic mode'. This method is invoked on the AsyncWebCrawler instance to perform the crawl.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is used for URL crawling, but misses the key relationship between arun() and the proxy/magic mode functionality described in the documentation. The ground truth explains how arun() implements these features through kwargs and crawler_strategy.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as a default crawler strategy unless specified otherwise, allowing functionalities such as utilizing a proxy and enabling 'magic mode'. The relationship with the strategy is thus indirect but essential as the strategy pattern interface facilitates the examples given.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but misses the main focus of the documented relationship which is specifically about proxy configuration and magic mode implementation in the strategy class.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions using the 'AsyncWebCrawler' class to demonstrate browser configuration and execution of web crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncWebCrawler's role in web crawling but misses the key aspect of browser type configuration through crawler_strategy, which is the main focus of the ground truth relationship",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides usage examples of the 'AsyncWebCrawler' class, demonstrating its 'arun' method to execute a crawl operation asynchronously with different browser configurations.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its arun method for crawling, but misses the key relationship about how browser_type parameter determines the engine selection (Chromium, Firefox, or WebKit) which is central to the ground truth.",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, the implementation of 'AsyncWebCrawler' involves 'AsyncPlaywrightCrawlerStrategy' as the default crawling strategy initialized with different browser types.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship between browser type selection and the AsyncPlaywrightCrawlerStrategy class, with the predicted description correctly identifying the basic functionality of browser type selection, even though it's less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of `AsyncWebCrawler` in an example to demonstrate how to remove overlay elements with the parameter `remove_overlay_elements=True`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles overlay removal through the remove_overlay_elements parameter, which matches the core functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The call `arun(url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True)` in the documentation is a direct use of the `arun()` method of `AsyncWebCrawler`.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but fails to acknowledge the key relationship about overlay removal functionality implementation that is central to the ground truth",
      "error_type": "omitted_key_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "`AsyncWebCrawler` uses a strategy pattern by employing `AsyncCrawlerStrategy`, although not explicitly mentioned in the documentation, it operates as the base strategy that `AsyncWebCrawler` implements.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While both descriptions identify the strategic relationship between AsyncCrawlerStrategy and AsyncWebCrawler, the predicted description suggests AsyncWebCrawler 'implements' the strategy when it actually uses/contains it. The core strategy pattern concept is correct but the implementation relationship is mischaracterized.",
      "error_type": "minor_mischaracterization"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation references 'AsyncWebCrawler' explicitly as it is used in the example of session-based crawling: `async with AsyncWebCrawler(verbose=True) as crawler:`.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the AsyncWebCrawler class usage but fails to mention the important aspects of session handling and core methods (__aenter__, arun, kill_session) that are central to the ground truth's description of session-based crawling functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly called within the provided code snippet: `result = await crawler.arun(...)`. It shows usage to run the web crawling operation.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for web crawling but misses the crucial aspect of session-based functionality and state maintenance across requests that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The 'kill_session' method is explicitly mentioned and used in the code snippet: `await crawler.crawler_strategy.kill_session(session_id)`. It demonstrates closing the session after crawling.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct presence and usage of kill_session but omits the crucial implementation details about closing both page and context objects and cleaning up session memory that the ground truth emphasizes",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the code snippet, 'result.extracted_content.count('.content-item')' implicitly refers to a property of the result which is an instance of 'CrawlResult'. The documentation doesn't explicitly mention 'extracted_content' but uses its functionality.",
    "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that extracted_content is a property containing the crawled content, though it's more implicit in its explanation. The core relationship of extracted_content storing HTML content from CSS selector matches is captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly shows the use of the AsyncWebCrawler class with a proxy configuration passed to it. Example: `async with AsyncWebCrawler(proxy_config=proxy_config) as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler accepts proxy configuration, but misses the key detail that this config is passed to the underlying AsyncPlaywrightCrawlerStrategy which actually handles the proxy connection",
      "error_type": "omitted_crucial_detail"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a call to the arun method of the AsyncWebCrawler class to execute a web crawl. Example: `result = await crawler.arun(url=\"https://example.com\")`.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method usage correctly but completely omits the core proxy functionality relationship that is the focus of the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "While not explicitly shown, AsyncWebCrawler's functionality hinges on crawler strategies defined by the base class AsyncCrawlerStrategy, indicating an intrinsic use relationship.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler relies on AsyncCrawlerStrategy for its core functionality, which aligns with the ground truth's explanation of how the strategy class enables proxy-authenticated crawling through its abstract methods",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The proxy configuration is likely used by the AsyncPlaywrightCrawlerStrategy (a subclass of AsyncCrawlerStrategy), which provides detailed browser initialization and proxy handling used by AsyncWebCrawler.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that proxy configuration is handled by AsyncPlaywrightCrawlerStrategy for browser initialization, but misses key details about the authenticated proxy support with username/password credentials via ProxySettings.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions the 'CosineStrategy' class being instantiated to define semantic extraction strategies specifically for cases like Article Content Extraction, Product Review Analysis, and Technical Documentation.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that CosineStrategy is used for different use cases, but fails to mention the core functionality of cosine similarity matching which is central to how the strategy actually works. It describes the configuration aspect but misses the key implementation detail.",
      "error_type": "missing_core_mechanism"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example demonstrates invoking the 'arun' method on a 'crawler' object which is likely an 'AsyncWebCrawler' instance. This method is used to execute crawling operations with the specified 'CosineStrategy'.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and its basic crawling functionality, but misses the key relationship with the documented use cases showing how extraction strategies parameterize the content filtering behavior",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While 'ExtractionStrategy' is not directly mentioned, it serves as the base class of 'CosineStrategy', which inherits extraction methodologies from it. The examples rely on the functionality provided by 'CosineStrategy', implicating its inheritance from 'ExtractionStrategy'.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and CosineStrategy, but misses explaining the crucial foundational functionality (extract() and run() methods) that enables the various use cases shown in the documentation",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The `CosineStrategy` class is explicitly mentioned in the documentation snippet as the implementation for similarity-based clustering used to extract relevant content sections. The snippet shows how to import and instantiate this strategy, demonstrating the practical application of the class.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core purpose of CosineStrategy for similarity-based clustering and content extraction, but omits the crucial aspect of how the configurable parameters control the clustering process, which is a key part of the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The snippet uses `CosineStrategy` within an `arun` function call. `arun` belongs to `AsyncWebCrawler`, which itself utilizes the concept of a strategy pattern involving `AsyncCrawlerStrategy` as a base class for various strategies like `CosineStrategy`. Therefore, `CosineStrategy` indirectly extends `AsyncCrawlerStrategy` via `AsyncPlaywrightCrawlerStrategy`.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods needed to implement concrete strategies like CosineStrategy by requiring async crawl operations, screenshot capabilities, and hook management that extraction strategies build upon.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that CosineStrategy relates to AsyncCrawlerStrategy through an implementation relationship and involves the crawler's operations, even if it adds some extra implementation details. The core relationship of AsyncCrawlerStrategy defining the interface that strategies like CosineStrategy build upon is preserved.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The provided code example uses `cosine.md` to instruct the execution of a crawl utilizing `CosineStrategy` via an `arun` method call on an instance, linking `CosineStrategy` execution with `AsyncWebCrawler` functions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the cosine strategy through its arun() method, which accepts an extraction_strategy parameter that can be configured with CosineStrategy to perform similarity-based content clustering as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that AsyncWebCrawler implements cosine strategy through the arun() method using an extraction_strategy parameter that can be set to CosineStrategy. While the predicted description is less detailed, it captures the core relationship between the components.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation explicitly mentions the use of `RegexChunking` as a chunking strategy for splitting text based on a regex pattern. The example code imports `RegexChunking` from the `crawl4ai.chunking_strategy` module, clearly showing its implementation.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of RegexChunking as a text splitting strategy using regex patterns, which aligns with the ground truth's explanation of its purpose and default behavior",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the code example. It is used to perform a web crawl operation while passing the `RegexChunking` as the `chunking_strategy`. This demonstrates the usage of this class for asynchronous web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncWebCrawler uses RegexChunking as a chunking strategy for text processing. The predicted description correctly identifies the class and its chunking functionality, even if it doesn't detail all implementation specifics mentioned in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is invoked within the example code provided in the documentation. This method is implicitly used to initiate a crawl with the `RegexChunking` strategy, as indicated by the call `await crawler.arun(...)`.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() uses RegexChunking, but misses the key point that the method implements support for chunking strategies by validating and accepting a chunking_strategy parameter with RegexChunking as the default.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation mentions using `AsyncWebCrawler` for extracting the main article content. The example code snippet demonstrates creating an instance of `AsyncWebCrawler` to perform web crawling operations, indicating its direct involvement in facilitating these operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling but misses the key relationship with fit_markdown feature implementation, which is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the documentation, `arun()` is implicitly referenced in the context of calling methods on `AsyncWebCrawler` to execute web crawling operations. The example snippet shows `arun` being called to fetch a web page's content.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as being used for web crawling but misses its crucial role in content extraction and processing via fit_markdown. It describes only the high-level crawling aspect without mentioning the core extraction functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The document explicitly discusses the `fit_markdown` attribute of `CrawlResult`, describing its purpose and behavior in extracting the main content of a page. The example highlights accessing `fit_markdown` to differentiate between extracted main content and all content from a web page.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used for extracting main content while removing boilerplate elements, which aligns with the ground truth's description of it being an optional string property for storing extracted main content after applying content extraction heuristics.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The text implicitly references the `markdown` attribute by contrasting it with `fit_markdown`, illustrating the difference in content extraction scope.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the key contrast between markdown and fit_markdown, explaining that markdown is the basic content extraction while fit_markdown is the filtered/focused version.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses 'AsyncWebCrawler' in the code example to demonstrate the session-based crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's use in the example but misses crucial functionality around dynamic content handling, pagination control, and JavaScript injection capabilities that are central to the class's purpose.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "'arun()' method of 'AsyncWebCrawler' is explicitly called in the example code to execute the crawling process, demonstrating how to manage dynamic content and pagination.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method for crawling, but misses the crucial aspect of session-based dynamic crawling with JavaScript execution and content update handling, which is the core functionality described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The text creates an instance of 'JsonCssExtractionStrategy' with a schema to extract specific elements from the HTML, explicitly showing its use for extracting commit details.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of JsonCssExtractionStrategy as a schema-based HTML element extractor, which aligns with the ground truth's explanation of how it implements structured data extraction using baseSelector and fields for GitHub commits",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The method 'kill_session()' of 'AsyncPlaywrightCrawlerStrategy' is clearly used to end a crawling session, showcasing resource management in dynamic crawling.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core purpose of the kill_session method - ending/cleaning up a crawling session. While it doesn't mention specific details about closing Playwright page/context objects, it conveys the same high-level relationship of resource management and session cleanup.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly describes the use of the 'CosineStrategy' class with parameters such as 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k'. It provides concrete examples of how objects of this class might be instantiated.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how the documentation's parameter details directly correspond to the CosineStrategy class's initialization parameters and their implementation. Both descriptions emphasize the mapping between the documented parameters and their actual usage in the class.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' is an implementation of the 'ExtractionStrategy', as 'CosineStrategy' extends functionality from 'ExtractionStrategy', which is a common pattern for strategy-based design implied in the documentation by reference to specific strategy parameters.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture the core relationship where ExtractionStrategy serves as a base/template class that defines the structure for implementing specific extraction strategies with various parameters. The predicted description accurately identifies the inheritance pattern, even if it doesn't list all the specific parameters.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses the `AsyncWebCrawler` class in the example, indicating its role in managing web crawling sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling sessions but misses the key point about session_id parameter management and state persistence between requests that is central to the ground truth explanation.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet uses the `arun` method twice within the example to perform web requests while maintaining the same session, showing how this method facilitates session-based crawling.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that the code enables session management across multiple requests, with the predicted description accurately capturing the core functionality of using arun() to maintain session state, even though it doesn't detail the specific implementation of session_id in kwargs and result fields.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet explicitly calls `kill_session` on the `crawler.crawler_strategy` attribute to terminate the session, indicating this method's role in session cleanup.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that kill_session is used for cleanup, but misses the key implementation details about closing both page and context objects and removing from sessions dictionary that are central to understanding how it works",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, the `AsyncWebCrawler` uses a `crawler_strategy`, which defaults to `AsyncPlaywrightCrawlerStrategy` if none is provided, implicating this class through inheritance and strategy pattern use.",
    "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core connection between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy through the strategy pattern, but misses the main focus of the relationship which is the session management functionality described in the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Implicitly related through the inheritance structure, as `AsyncPlaywrightCrawlerStrategy` is a subclass of `AsyncCrawlerStrategy`, showing the hierarchical structure.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core methods that enable session-based crawling functionality shown in the documentation, including the crawl method that processes individual session-based requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance/abstract class structure, but misses the core purpose of enabling session-based crawling functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly mentions the use of the 'arun' method within its examples, highlighting how to pass 'wait_for' parameters for CSS-based and JavaScript-based waiting.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the arun() method and wait conditions, noting that it handles both CSS and JavaScript-based waiting parameters via examples. While it doesn't explicitly mention the **kwargs implementation detail, this is a minor omission that doesn't affect the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned in the documentation, 'AsyncPlaywrightCrawlerStrategy' implements the functionality that would support the wait operations described, as it is a strategy used by 'AsyncWebCrawler' through the 'arun' method to actually perform crawling activities.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncPlaywrightCrawlerStrategy implements wait functionality but is vague about how it's implemented. The ground truth specifically explains that it uses smart_wait method with page.wait_for_selector for CSS and page.evaluate for JavaScript conditions.",
      "error_type": "missing_key_implementation_details"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "This is the abstract base class for 'AsyncPlaywrightCrawlerStrategy', providing the abstract method 'crawl' that 'arun' eventually relies upon. The strategy pattern implies use of defined methods in strategies that implement the base.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies this as an abstract base class and mentions the crawl method, but misses the key relationship about how the class enables waiting functionality through kwargs parameter",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls the 'arun' method as part of the media extraction process using the AsyncWebCrawler instance. This method is involved in crawling a given URL and appears to handle various configurations such as 'wait_for' and 'delay_before_return_html', which are mentioned in the examples for handling lazy-loaded content.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is central to the crawling process and handles configuration options mentioned in the documentation, including lazy-loaded content. While it doesn't mention all capabilities like caching and screenshots, it captures the core relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation example iterating over 'result.media[\"images\"]' explicitly shows how the media extraction functionality of 'arun' returns detailed image metadata via the CrawlResult's 'media' attribute.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the media dictionary property stores image metadata accessed through result.media['images'], which aligns with the ground truth's explanation of the CrawlResult.media dictionary storing media-related data and metadata.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The provided text outlines a JSON schema for extracting data from HTML structures. The JsonCssExtractionStrategy class aligns with this by providing a method for defining and extracting HTML elements based on a schema. Elements in the text, such as 'nested', 'list', and 'fields', are part of this Json-CSS schema processing.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of JSON schema and HTML extraction but misses crucial aspects of BeautifulSoup implementation and the hierarchical processing of different field types (nested, list, nested_list) that are central to the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly refers to the `LLMExtractionStrategy` as the tool used to extract structured data from the OpenAI pricing page. This strategy is instantiated with specific parameters such as the provider, token, and extraction schema, demonstrating its use in the example code.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for structured data extraction and mentions its key implementation aspects (provider-based, using parameters like token and schema). While it doesn't mention Pydantic specifically, it captures the core relationship and functionality shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned and used in the example to create an instance that carries out the web crawling process, integrated with the `LLMExtractionStrategy` to extract data.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship, noting that AsyncWebCrawler is used for web crawling and works with LLMExtractionStrategy for data extraction, which aligns with the ground truth's description of its functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is invoked within the `AsyncWebCrawler` context to handle the extraction process with the specified strategy and settings. It is explicitly referenced in the example as part of performing the crawling and extraction.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic relationship showing arun() is used for crawling and extraction, but misses significant functionality around error handling, caching, HTML processing and returning CrawlResult that is core to how the method works according to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is explicitly called in the documentation to perform page interaction and extraction operations using specified extraction strategies.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun method is used for page interaction and extraction with extraction strategies, which aligns with the ground truth's explanation of how arun() implements crawling functionality with extraction strategies and JavaScript interactions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' class is explicitly mentioned and used as an extraction strategy in the example code snippet. The schema for extraction is defined using a JSON structure, indicating use of this strategy.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role as an extraction strategy using JSON schema, but misses the core implementation detail about using BeautifulSoup to parse HTML and apply CSS selectors for data extraction",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'LLMExtractionStrategy' class is explicitly mentioned and used in the example code to analyze content by calling a language model with specific instructions.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy being used to analyze content through language models with instructions, which aligns with the ground truth's explanation of processing HTML content through LLM models with configurable settings.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "While 'AsyncCrawlerStrategy' is not directly mentioned, it is the base class for 'AsyncPlaywrightCrawlerStrategy', indicating that functionalities described in the 'arun' method are extended from this class.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable the documented extraction strategies to perform crawling operations with custom behaviors like waiting for elements and executing JavaScript code.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but focuses too narrowly on its relationship to AsyncPlaywrightCrawlerStrategy rather than its broader role in enabling extraction strategies through its core interface methods.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This class is implicitly involved as it implements concrete methods required for crawling, extending 'AsyncCrawlerStrategy'. It is crucial for the functionalities demonstrated in 'arun'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser automation for executing JavaScript code and waiting for elements, which directly enables the documented extraction strategies by providing the underlying mechanism to run js_code and handle wait_for conditions during page crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the class implements AsyncCrawlerStrategy and enables crawling functionality, but it fails to specifically highlight how it provides the mechanism for JavaScript execution and wait conditions that enable the extraction strategies shown in the documentation.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet imports and utilizes the `AsyncWebCrawler` class to initiate an asynchronous web crawling session, which is evident by its mention and use in the function `extract_tech_content()`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler for asynchronous web crawling, but misses the crucial aspect of its customizable extraction strategies functionality that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method of the `AsyncWebCrawler` class is explicitly called to perform crawling operations with specific parameters, such as the URL and an extraction strategy.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic functionality of arun() for crawling, but misses the crucial aspect of content filtering/extraction based on specific criteria which is a key part of the relationship shown in the documentation",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The example constructs an `LLMExtractionStrategy` to instruct the LLM to extract technology-related content. This is explicitly mentioned when setting the extraction strategy for `arun()`.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the instruction-based extraction functionality but oversimplifies and misses key aspects of the LLMExtractionStrategy's implementation, such as HTML processing, model integration, and chunking capabilities shown in the code.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While not directly mentioned, `LLMExtractionStrategy` extends `ExtractionStrategy`, establishing a background relationship inherent in the code, providing extraction capability.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, and while it's more concise, it doesn't contradict the ground truth's explanation of how this enables custom content extraction",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet directly mentions using the class 'AsyncWebCrawler' to initialize the crawler with a provided 'extraction_strategy'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions the initialization with extraction_strategy but misses key functionality like asynchronous crawling, caching handling, and processing URLs that are central to the class's purpose as described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' of the 'AsyncWebCrawler' class is explicitly invoked in the code snippet to perform the URL fetching and extraction process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for URL fetching but misses crucial aspects of its functionality like handling extraction strategies, caching, and returning structured results that enable JSON data extraction as described in the ground truth",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' is explicitly defined and instantiated in the snippet to be used as the extraction strategy for the crawler which is passed into 'arun'.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used as an extraction strategy for the crawler, but misses the core functionality of how it processes HTML using BeautifulSoup and schema-based extraction described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text mentions the 'AsyncWebCrawler' class directly in the code example. It is instantiated and used to perform web crawling based on a specified extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling but misses its specific implementation of cryptocurrency price extraction functionality according to the JsonCssExtractionStrategy schema mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The referenced code snippet explicitly creates an instance of 'JsonCssExtractionStrategy' with a specific schema to extract structured data. It shows this strategy being passed to 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of JsonCssExtractionStrategy being used with a schema, but misses the crucial aspect of how it specifically extracts cryptocurrency data using CSS selectors for name, symbol, and price from table rows",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet demonstrates invoking 'arun' on an instance of 'AsyncWebCrawler' without explicitly mentioning 'arun'. It operates the crawl with 'extraction_strategy'.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() with AsyncWebCrawler and extraction_strategy, but misses the core functionality of handling URL requests, applying the strategy, and returning structured data as described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "Accesses 'result.extracted_content' to parse the JSON, implying its role in holding the extracted data as returned by 'arun'.",
    "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content holds data returned by arun(), but misses key details about the specific cryptocurrency data structure (name/symbol/price) and that it's stored as a string before JSON parsing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation directly refers to the `LLMExtractionStrategy` as a key component of the LLM-Based Extraction process, highlighting its function in using Language Models to extract structured data from web content.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's role in using Language Models for data extraction, but misses key implementation details about schema-based extraction, HTML processing, and the underlying extraction mechanisms that are central to the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example provided shows the usage of `crawler.arun()`, which suggests this method is responsible for executing the LLM strategy within an asynchronous context.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as handling LLM strategy execution, but misses the broader core functionality of handling URL fetching, caching, and general web crawling that enables any extraction strategy to work, not just LLM-based ones.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The document snippet explicitly shows the use of the 'AsyncWebCrawler' class to perform web crawling. It is used within the 'main' function as a context manager ('async with AsyncWebCrawler...'), indicating its role in managing the lifecycle of a web crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's context manager usage but misses the core error handling functionality described in the ground truth. While it correctly identifies a key aspect of the class, it overlooks the important error handling mechanism that's central to the documented functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly called within the 'extract_with_retry' function, which is part of the provided code snippet. The method is used to run the web crawling process with a specific URL and extraction strategy, reflecting its role in executing the crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's usage but misses its core error handling functionality which is the main focus of the ground truth relationship. The ground truth specifically emphasizes the comprehensive error handling aspect which is crucial to the documented error handling patterns.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The given code snippet explicitly constructs an instance of 'LLMExtractionStrategy' and uses it within the 'extract_with_retry' function, indicating its critical role as the extraction strategy employed for interacting with a specified LLM API.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy as handling LLM API interactions, but misses the core focus on error handling and parallel processing implementation described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example call to the method 'arun' on an instance of 'AsyncWebCrawler', explicitly showing its use for page timeouts and waiting conditions.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles page timeouts and waiting conditions, which aligns with the ground truth's explanation of how these controls are implemented via kwargs and crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Implicitly related as 'AsyncWebCrawler' is the class within which the 'arun()' method is defined, hence it involves the configuration or invocation of this method.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic structural relationship (arun method within AsyncWebCrawler) but misses the key functional relationship about timeout and waiting functionality implementation through kwargs parameter.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class uses an instance of an 'AsyncCrawlerStrategy' as its strategy pattern, implicitly tying the setup and invocation of 'arun' to implementations that conform to this interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the abstract strategy pattern but focuses on the AsyncWebCrawler class which isn't shown in the code. It misses the core timeout/waiting functionality described in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Implicitly related as an implementation of 'AsyncCrawlerStrategy', which can be used as the 'crawler_strategy' in 'AsyncWebCrawler', affecting the behavior described in page loading configurations.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the code as implementing AsyncCrawlerStrategy and its relationship to AsyncWebCrawler, but misses or understates the core timeout and waiting functionality that is the main focus of the documentation.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides usage examples directly utilizing the 'AsyncWebCrawler' class, indicating that this class is explicitly involved in the functionality depicted.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is directly involved in the functionality, but misses the crucial detail that proxy support is implemented through the crawler_strategy parameter rather than directly in the class.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation executes the 'arun' method within 'AsyncWebCrawler', indicating reliance on this method to perform web crawling operations as part of the proxy setup example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is integral to web crawling operations and is utilized in the proxy setup example, which aligns with the ground truth's explanation of arun being the core crawling functionality that handles proxy configurations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, 'AsyncWebCrawler' initializes its strategy with 'AsyncPlaywrightCrawlerStrategy' when a default strategy is not provided, implicitly involving this class.",
    "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description acknowledges a relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but misses the main focus on proxy support implementation described in the ground truth.",
      "error_type": "scope_mismatch"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet explicitly demonstrates the use of the 'arun' method from the 'AsyncWebCrawler' class, which is called with parameters 'simulate_user' and 'override_navigator'. This method is directly related as it configures anti-detection features.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately describe that the arun() method accepts parameters for configuring anti-bot/detection features. The predicted description correctly identifies the key functionality, even though it's less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is implicitly related as it implements the 'arun' method which is used in the documentation example. The context suggests that anti-detection features are an implementation aspect of the class.",
    "ground_truth_relationship": "The documentation discusses manual anti-bot options like 'simulate_user' and 'override_navigator' which are passed as kwargs to the AsyncWebCrawler's arun() method and forwarded to the crawler_strategy's constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler implements the arun method used in the documentation, but misses the key point about how the anti-bot options are passed through kwargs to the crawler_strategy's constructor.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet directly mentions 'JsonCssExtractionStrategy', describing tips on its usage, such as using CSS selectors and handling dynamic content.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship between code and docs mentioning CSS selectors, but misses the crucial schema-based extraction aspect that's central to how the code works.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation describes the `SlidingWindowChunking` class, explicitly mentioning its use of a sliding window approach to create overlapping chunks, and specifying parameters such as `window_size` and `step`.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship - that the class implements sliding window chunking with window_size and step parameters to create overlapping chunks. It aligns with the ground truth's explanation of how the code works.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly mentions `AsyncWebCrawler`, indicating its use by utilizing an 'async with' context manager to create a `crawler` instance.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncWebCrawler's context manager functionality but fails to mention its core purpose of supporting multiple extraction strategies and formats for web content processing, which is the main focus of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly includes calls to the `arun` method on the `crawler` instance of `AsyncWebCrawler` for processing URLs with different extraction strategies and options.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation shows usage of AsyncWebCrawler.arun() with different extraction strategies, which aligns with the ground truth's explanation of how the code implements different extraction strategy handling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "Although not deeply discussed in the snippet, `LLMExtractionStrategy` is used within an `arun` method call, demonstrating how it can be configured with a provider and schema for extracting structured data.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used within arun() for structured data extraction and can be configured with providers and schemas, which aligns with the ground truth's description of its core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet demonstrates utilizing the `JsonCssExtractionStrategy` by passing it as an argument in one of the `arun` calls, indicating its role in extracting repeated patterns according to a schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in pattern extraction, but misses the crucial detail about how it uses CSS selectors to systematically extract structured data from HTML elements",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation references `extracted_content` as part of the `CrawlResult`, showing its importance in accessing structured data results after using the extraction strategies.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that extracted_content is used for storing structured data from extraction strategies, which aligns with the ground truth showing it being used to store crawled data that gets parsed as JSON.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "`fit_markdown` is accessed to retrieve the main content in markdown form from the `CrawlResult`, spotlighting its significance in extracting and formatting the main content output.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that fit_markdown is used to store and access the main content extracted from a webpage in markdown format, with the content being used in the returned dictionary's main_content field.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Media information is included in the returned dictionary, implifying its retrieval from `CrawlResult` to incorporate media data in the output.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that media information is included in the return dictionary and comes from the CrawlResult, which aligns with the ground truth showing media being returned from the CrawlResult.media in the function's output.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'CosineStrategy' class with specific parameters like 'linkage_method', 'max_dist', 'model_name', 'semantic_filter', 'word_count_threshold', 'sim_threshold', and 'top_k'. This indicates direct usage and configuration of the CosineStrategy to implement custom clustering and content filtering pipeline.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy, including its implementation of custom clustering and content filtering pipeline with semantic filtering and specific configurable parameters. Both descriptions align on the high-level purpose and key features.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet uses 'AsyncWebCrawler' context management with 'async with' syntax for the 'extract_pricing_features' function, indicating its utility in setting up and handling the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context management functionality but misses its core purpose as the implementation engine for content filtering and extraction strategies that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function 'extract_pricing_features' in the snippet calls the 'arun' method on an 'AsyncWebCrawler' instance. This highlights 'arun' as a critical method for executing the crawl using the 'CosineStrategy' as its extraction strategy, with the goal of extracting pricing features from a given URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as a key method for crawling with an extraction strategy, but misses significant aspects covered in the ground truth like caching, HTML processing, and the broader role in handling custom clustering configurations.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The 'extract_pricing_features' function accesses 'extracted_content' from the result returned by 'arun' to extract and manipulate content data. This shows reliance on the 'extracted_content' attribute of 'CrawlResult' for processing crawl results.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the crawl result, but misses the key aspect that it specifically contains JSON-serialized clustering and filtering results with pricing features and similarity data",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly uses the `AsyncWebCrawler` class via its `arun` method to perform domain-based filtering.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class handles domain-based filtering through its arun method, which is the core relationship described in the ground truth. While the predicted description is less detailed, it captures the essential functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` of the `AsyncWebCrawler` class is explicitly called within the code snippet shown in the documentation for filtering content.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description notes arun is called in the documentation, but misses the key relationship that arun accepts domain filtering options via kwargs that are used for content control",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, `AsyncWebCrawler` uses an `AsyncCrawlerStrategy`. It's the base class for crawler strategies implemented by `AsyncPlaywrightCrawlerStrategy`.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but focuses on implementation details about AsyncPlaywrightCrawlerStrategy rather than its core purpose of enabling domain-based filtering as described in the ground truth",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Implements the `AsyncCrawlerStrategy` methods for crawling strategies. This is used by `AsyncWebCrawler` when no specific strategy is provided.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies this as an implementation of AsyncCrawlerStrategy but misses the core domain-based filtering functionality described in the ground truth. It mentions what class it implements but not its key feature.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The document contains a code example showing the use of `AsyncWebCrawler` to perform crawling tasks. Evidence: `async with AsyncWebCrawler(verbose=True) as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the existence of AsyncWebCrawler and its basic use for crawling, but misses the crucial aspect of custom execution hooks functionality through crawler_strategy that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Document suggests use of `AsyncPlaywrightCrawlerStrategy` implicitly as it's the default strategy used by `AsyncWebCrawler` when none is provided. Evidence from code context where crawler strategy is used without explicitly specifying a different strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncPlaywrightCrawlerStrategy being used implicitly as a default strategy, while the ground truth describes its core functionality of implementing custom hooks through set_hook() and execute_hook() methods for executing custom functions at specific crawling stages. These are completely different aspects of the class.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The document code example illustrates using `set_hook` to attach the `on_execution_started` hook. Evidence: `crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)`.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that set_hook is used to attach the on_execution_started hook, which aligns with the ground truth's explanation of storing hook callbacks for execution during crawling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The document includes an explicit example of calling `kill_session` to terminate a session. Evidence: `await crawler.crawler_strategy.kill_session(session_id)`.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately identify the kill_session functionality being used for cleanup and termination of browser sessions, with the predicted text correctly noting its explicit usage in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The document's code example accesses `extracted_content` from `CrawlResult` after performing a crawl. Evidence: `commits = result.extracted_content.select(\"li.commit-item\")`.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from crawl results, but misses the crucial role of hooks waiting for and validating this content before proceeding with crawling, which is a key part of the ground truth relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows an instantiation and usage of the 'AsyncWebCrawler' class to perform crawling operations. The context highlights the use within an 'async with' block demonstrating context management.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main relationship by noting the AsyncWebCrawler's usage with async context management and crawling operations, which aligns with the ground truth's description of the class implementing async context management and the arun() method functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code snippet, the 'arun' method of 'AsyncWebCrawler' is called to execute a crawling operation with a specific extraction strategy. The method handles the process of interacting with the given URL.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of the arun method executing a crawling operation with an extraction strategy, but misses crucial aspects like its asynchronous nature and the key functionality of returning a CrawlResult with extracted content",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and initializes the 'CosineStrategy' class with specific parameters for clustering and semantic filtering. This class is used as an argument to the 'arun' method of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic initialization and usage of CosineStrategy, but misses the core purpose of the class as a configurable content extraction and clustering strategy, focusing only on its instantiation and usage pattern",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly mentions the use of `AsyncWebCrawler` within an `async with` statement, showing that it is being used to manage the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that AsyncWebCrawler is used for web crawling and mentions its async context manager functionality, but misses the crucial aspect of JavaScript execution capabilities and extraction strategy coordination described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the `arun` method of `AsyncWebCrawler` explicitly to execute a web crawl, demonstrating how the method is employed to perform tasks within the crawl operation.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for web crawling, but misses the crucial aspect that the method specifically handles dynamic content through JavaScript execution and LLM extraction capabilities shown in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation shows the implementation of `LLMExtractionStrategy` as the extraction strategy passed to `arun`. This highlights its role in extracting content using specified parameters.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies LLMExtractionStrategy's role in extraction, it misses major aspects of its functionality like LLM-based extraction, support for different providers, and customizable parameters that are central to the class's purpose",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The class AsyncWebCrawler is explicitly instantiated in the provided code snippet. It is used to perform the crawling process across multiple pages.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic role of AsyncWebCrawler for crawling but misses crucial aspects like session-based browsing, JavaScript execution capability, and custom extraction strategies that are central to its dynamic content crawling functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The AsyncPlaywrightCrawlerStrategy class is implicitly related as it is the default strategy used by AsyncWebCrawler if none is specified. The snippet uses the default strategy to execute JavaScript, wait for conditions, and manage sessions.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles JavaScript execution and waiting conditions, but mischaracterizes it as being 'implicitly' used when it actually explicitly implements the core session management and page navigation functionality described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method arun() from AsyncWebCrawler is invoked in the code snippet with specific parameters for each page being crawled.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The prediction identifies that arun() is used for crawling pages, but misses key aspects about session management and caching functionality that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The JsonCssExtractionStrategy is explicitly utilized in the code snippet. It is initialized with a schema to extract commit information from the page.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core usage of JsonCssExtractionStrategy with schema initialization but misses the crucial BeautifulSoup parsing and selector application implementation details that are central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The method 'kill_session()' in 'AsyncPlaywrightCrawlerStrategy' is explicitly called to clean up the session after crawling is completed.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that kill_session() is called to clean up the session after crawling, which aligns with the ground truth's explanation of cleaning up browser resources. The omission of specific details about closing page and context objects does not change the fundamental understanding.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the usage of the 'AsyncWebCrawler' class, which is instantiated and used in the example.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its basic usage pattern, but misses the key aspect about proxy rotation functionality that is central to the documentation example.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' is invoked on an 'AsyncWebCrawler' instance within the documentation's example, suggesting its application without explicit detail.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method used with AsyncWebCrawler, but misses the core functionality of crawling and proxy support mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly used in the documentation snippet as the main interface for configuring and invoking web crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main interface for crawling operations, but misses the key point that it implements all the specific configuration options shown in the documentation through its methods. The prediction doesn't capture the full scope of functionality.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method of `AsyncWebCrawler` is explicitly called in the code example to execute a web crawling task, which aligns with the description of performing a crawl with the configured settings.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic use of arun() for web crawling, but misses the core relationship that the code implements the processing logic while the documentation shows how to configure and use it",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` is not directly mentioned but is implicitly involved as the default strategy of `AsyncWebCrawler`, managing the browser-specific features like `browser_type`, `user_agent`, and `proxy`.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is implicitly used by AsyncWebCrawler and handles key browser-related features. While more concise than the ground truth, it captures the essential relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The example code extracts `screenshot` from the `result`, implicitly interacting with the `CrawlResult` class, which handles the screenshot data after a crawl is executed.",
    "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that screenshot data is handled through the CrawlResult class after crawling, even though it doesn't specify the base64 encoding format.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "In the snippet, the `success` attribute is accessed from the crawl result to check if the crawl was successful, linking to how the `CrawlResult` communicates the status of the crawl.",
    "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly communicate that the success boolean is returned as part of the result to indicate the crawl operation's success status, even though they phrase it slightly differently",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'CosineStrategy' class as part of code examples that demonstrate how to instantiate and configure it using various parameters like 'word_count_threshold', 'top_k', 'semantic_filter', 'sim_threshold', and 'max_dist' for optimizing performance and handling different content types.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture how the CosineStrategy class implements configurable parameters that match the documentation's recommendations for different content types, with parameter values controlling text filtering and clustering behavior.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' class, explicitly mentioned in the documentation, extends 'ExtractionStrategy'. This is implied as a necessary understanding of how 'CosineStrategy' operates within the broader framework of extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class, but focuses on CosineStrategy inheritance rather than the core framework flexibility described in the ground truth",
      "error_type": "focus_misalignment"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet demonstrates the use of the `arun()` method of the `AsyncWebCrawler` class in the example code to perform a web crawling task with CSS selectors. The method call in the example directly maps to the documented functionality of extracting content using CSS selectors.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the high-level relationship between the code and documentation - both describe the arun() method's ability to use CSS selectors for content extraction, which aligns with the ground truth's explanation of CSS selector-based content extraction functionality.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While `AsyncWebCrawler` is the class containing the `arun()` method, it is implicitly used in the provided code snippet. The snippet uses an instance of `AsyncWebCrawler` (`crawler`), implying its role in executing the method call.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as containing arun(), but misses explaining the core CSS selector functionality that allows targeting specific HTML elements. It focuses on class usage rather than the selector capability described in the ground truth.",
      "error_type": "omits_core_functionality"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows how to execute JavaScript commands using the `arun` method of the `crawler`, identified as `AsyncWebCrawler`. This method is explicitly illustrated to execute JavaScript on a webpage.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that arun() executes JavaScript commands, it misses that this happens through **kwargs and crawler_strategy.crawl(), and overstates how 'explicit' this functionality is in the code",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While the specific method `arun` is used explicitly in the example to execute JavaScript, it implicitly requires the `AsyncWebCrawler` class to function, given that `arun` is a method within this class. This association is necessary for the method to be available for the depicted operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements JavaScript execution through its arun method, which accepts a js_code parameter that can be either a single command or list of commands as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the AsyncWebCrawler class and its arun method are used for JavaScript execution, and that there's a necessary relationship between them. While it's less detailed than the ground truth, it conveys the core relationship correctly.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation, `AsyncPlaywrightCrawlerStrategy` is implicitly involved as it implements `AsyncCrawlerStrategy`, which could be the underlying strategy used by `AsyncWebCrawler` to perform tasks like `arun`. This is drawn from its function of handling browser interactions and JavaScript execution.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy's role in browser interactions and JavaScript execution, but introduces uncertainty by describing it as 'implicitly involved' when the code clearly shows it directly implements JS execution through the crawl method.",
      "error_type": "uncertain_description"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Functionality showcased for JavaScript execution might be built upon interfaces defined in `AsyncCrawlerStrategy`. Given it is the base class for `AsyncPlaywrightCrawlerStrategy`, understanding it helps trace back the operations set by higher-level strategies.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class, but misses the key point about its direct role in JavaScript execution through the crawl() method that the ground truth emphasizes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the provided code snippet, an instance of `AsyncWebCrawler` is created using `async with AsyncWebCrawler()`, highlighting it as the main class used to perform the crawling operation.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and async context manager pattern, but misses the crucial arun method functionality which is a key part of the relationship shown in the documentation",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is used in `await crawler.arun(url=\"https://example.com\")` within the documentation example to trigger the crawling process, but it is not explicitly discussed in the text.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun is used for crawling but fails to mention its key functionality around content extraction, caching, and returning a CrawlResult object which are core aspects mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the example, the text mentions accessing `result.markdown` to print the clean markdown content, indicating usage of the `markdown` field from a `CrawlResult` object returned by `arun()`.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship showing that the markdown field contains cleaned webpage content accessible via result.markdown, matching the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation describes how JsonCssExtractionStrategy can demonstrate its advanced features using a complex HTML structure of a hypothetical e-commerce website. It is evident from the text that JsonCssExtractionStrategy is utilized to extract structured data from the provided HTML example.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy works with HTML and extracts structured data, but misses the crucial aspect of using BeautifulSoup and CSS selectors mapped to a schema to perform the extraction",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Given that JsonCssExtractionStrategy is utilized for HTML extraction in the example, it implies that the foundational behavior specified in ExtractionStrategy is also part of the operation.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but focuses mainly on JsonCssExtractionStrategy's inheritance rather than ExtractionStrategy's core purpose of HTML data extraction and processing described in the ground truth.",
      "error_type": "incomplete_core_purpose"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation includes a Python code example using 'arun()' to perform a web crawl. This method, 'AsyncWebCrawler.arun()', is likely used to initiate asynchronous crawling, resulting in various output formats described in the documentation.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is an asynchronous crawler method that produces the different output formats mentioned in the documentation. While it's less specific about the processing details, it captures the core relationship between the method and the documented output formats.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation explicitly mentions 'CrawlResult' by illustrating the retrieval of different HTML/markdown representations ('html', 'cleaned_html', 'markdown', and 'fit_markdown') as attributes of a 'CrawlResult' object.",
    "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the CrawlResult class and the documented output formats, correctly identifying that the class properties correspond to the different content representations shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The documentation directly calls 'result.html' to access the raw HTML from a crawl, indicating 'CrawlResult.html' is a critical component for acquiring the original HTML data generated by 'arun()'.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that 'html' is used to store/access raw HTML content from the crawl result, with the predicted description focusing on usage and the ground truth on type definition",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "Documented as 'clean_html', 'result.cleaned_html' provides a sanitized version of the original HTML, directly illustrating its use to access formatted HTML output.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that cleaned_html provides sanitized HTML output from the original HTML content, matching the ground truth's key points.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation calls 'result.markdown' to produce a standard markdown representation of the crawled content, indicating 'CrawlResult.markdown' is responsible for this conversion.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the markdown field/property provides standard markdown conversion of the crawled content, accessible via result.markdown",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The 'fit_md' output format in the documentation associates with 'result.fit_markdown' to access the most relevant markdown content, suggesting this attribute handles content relevancy.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown contains the most relevant content in markdown format, which aligns with the ground truth's explanation",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The AsyncCrawlerStrategy is the base class for crawler strategies in the codebase. Although not directly mentioned, the usage of 'await crawler.arun' suggests that the crawler instance uses a strategy based on this class.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables asynchronous web crawling capabilities needed to support the documented structured data extraction features through its crawl and crawl_many methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but misses its crucial role in enabling structured data extraction through async crawling methods. The connection to the documentation's functionality is not explicitly made.",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "LLMExtractionStrategy is used for extraction. AsyncPlaywrightCrawlerStrategy, a subclass of AsyncCrawlerStrategy, is utilized by AsyncWebCrawler (the crawler calling the strategy).",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class provides the foundational web crawling functionality needed to fetch web content that can then be processed by the LLM-based extraction strategy described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is used for web crawling and works in conjunction with LLMExtractionStrategy for the overall extraction process. While it adds some implementation details about class inheritance, the core relationship between crawling and extraction functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly demonstrates the use of the AsyncWebCrawler.arun() method to execute the extraction strategy for structured data extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() method is used for extraction strategy execution, but misses the crucial aspect of how it integrates with LLM providers and the configurability of the extraction strategy parameter for structured data processing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is explicitly mentioned in the documentation as the class being instantiated and used to set up the strategy for extracting structured data.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's presence in the documentation and its basic instantiation, but misses crucial aspects about its implementation of structured data extraction, handling of different providers, and internal processing features.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` is directly instantiated in the provided code examples. The snippet shows the crawler being used to customize the user agent and headers, indicating that it supports managing these browser identity aspects.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler supports identity management through customization of user agent and headers, which aligns with the ground truth's description of the functionality. The main relationship and purpose is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` from `AsyncWebCrawler` is utilized in the examples to perform crawling operations after configuring the identity elements. This shows that `arun()` is central to executing the configured browser identity settings.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() is the core method for executing crawls with configured identity settings, which aligns with the ground truth's explanation of how arun() implements identity management through user_agent and kwargs parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly mentioned in the snippet, `AsyncPlaywrightCrawlerStrategy` is implicitly involved as the default strategy for `AsyncWebCrawler` unless otherwise specified. It handles underlying operations such as handling user agents and headers.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy's role in handling user agents and headers, but incorrectly suggests it's only implicitly involved as a default strategy. The ground truth shows it explicitly implements identity management through specific methods.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of `AsyncWebCrawler` is directly invoked in the code example provided in the documentation snippet. It demonstrates how to use the method to perform a web crawl operation using a specified `url` and `extraction_strategy`.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the core focus on error handling that is central to the ground truth relationship between code and documentation. The documentation specifically demonstrates error handling patterns that are implemented in the code.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The `CrawlResult.extracted_content` attribute is indirectly referenced because the example checks the `extracted_content` field of the `result` object, which is an instance of `CrawlResult`. It showcases how the extracted content is processed after a crawl.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed through the result object and processed after crawling, but misses the crucial semantic matching and strategy aspects mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The `CrawlResult.error_message` attribute is used in the `else` block to print an error message if the extraction is unsuccessful. This indicates how error information is accessed from a `CrawlResult` instance.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that error_message is accessed from CrawlResult to display error information when extraction fails, which aligns with the ground truth's explanation of its purpose and usage",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly describes the `CosineStrategy` class, explaining its use cases and effectiveness in scenarios requiring semantic understanding and similarity-based content extraction.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies CosineStrategy's purpose for semantic understanding and content extraction, but misses explaining the core hierarchical clustering functionality and embedding-based similarity calculation methods that are central to its operation according to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation snippet explicitly describes the `FixedLengthWordChunking` class as a strategy that splits text into chunks based on a fixed number of words. This matches the provided code snippet, which initializes a `FixedLengthWordChunking` instance and demonstrates its use.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of splitting text into fixed-length word chunks, matching the ground truth's explanation of the implementation",
      "error_type": "none"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not mentioned directly in the text, `FixedLengthWordChunking` is a class that implicitly extends from `ChunkingStrategy`, as it is typical for such strategy classes to extend from an abstract base class that defines a uniform interface. This relationship is important to understand the architectural design of the system.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that FixedLengthWordChunking implicitly extends/implements ChunkingStrategy as an abstract base class, which aligns with the ground truth's description of their relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The text snippet explicitly mentions the use of result.media to extract video and audio elements. This implies that the CrawlResult class has a media attribute that is structured to contain lists of videos and audios.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the CrawlResult.media dictionary contains lists of video and audio elements with associated metadata that can be accessed and processed",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet in the documentation directly uses the `arun` method of the `AsyncWebCrawler` class to perform a crawl operation.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the core focus on error handling and checking success/error messages that is central to the ground truth relationship",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code checks the `success` attribute from the result of the `arun` method, indicating direct usage.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of the success attribute for checking, but misses the key purpose of error handling that is central to the ground truth relationship",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The documentation snippet explicitly accesses the `error_message` attribute to retrieve error information if the crawl fails.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how error_message is used to display failure information when crawls are unsuccessful, matching the ground truth's core meaning.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The snippet accesses the `status_code` attribute to print the HTTP status code when the crawl operation with `arun` fails.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that status_code is used for error handling during crawl operations to check HTTP status codes, which aligns with the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation suggests using the CSS Strategy for well-structured HTML content. The artifact 'JsonCssExtractionStrategy' directly correlates to this strategy type, which involves using CSS for extracting structured data from HTML.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy implements the CSS Strategy and is used for extracting data from structured HTML using CSS selectors, which aligns with the documentation's guidance.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The LLM Strategy is mentioned explicitly for natural language text with high semantic needs. The 'LLMExtractionStrategy' artifact represents this strategy, leveraging language models for extracting and interpreting natural language content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy implements LLM-based processing for natural language text with semantic understanding capabilities. While it doesn't mention all implementation details, it captures the core relationship and purpose.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation identifies the Cosine Strategy for mixed/complex content and content relevance, directly aligning with the 'CosineStrategy' artifact which uses cosine similarity for clustering and extracting relevant content.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for mixed/complex content and employs cosine similarity for content relevance, which aligns with the ground truth's explanation of using cosine similarity and hierarchical clustering for processing mixed content.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet shows an example using a 'crawler' which refers to an instance of 'AsyncWebCrawler', as evidenced by the operation 'crawler.arun(...)'. This indicates that 'AsyncWebCrawler' is the class involved in handling the crawling functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the documented link analysis by processing HTML content in the aprocess_html method, which extracts and classifies links into a structured format stored in the links dictionary, as shown in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in crawling but misses the key relationship about link analysis and classification functionality described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a code snippet that calls 'await crawler.arun(url=\"https://example.com\")'. This explicitly uses the 'arun' method to initiate a crawl, indicating that 'AsyncWebCrawler.arun()' is the method responsible for starting the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used to start the crawling process, but misses the crucial aspect that it implements the link analysis functionality and returns categorized link data as described in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation refers to analyzing links within a 'result' object. Since 'result.links' is accessed to process different link types, this demonstrates implicit use of the 'links' attribute of 'CrawlResult', which stores categorized link data.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that CrawlResult has a links attribute storing categorized link data, which matches the ground truth's description. While the ground truth provides more detail about the dictionary structure and specific link attributes, the predicted text correctly identifies the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "In the context of calling 'await crawler.arun(...)', the variable 'result' refers to an instance of 'CrawlResult', indicating this class's role in encapsulating the crawl results, including the links.",
    "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CrawlResult's role in storing crawl results, but misses the crucial aspect of how it specifically handles link categorization and metadata storage through its 'links' dictionary field",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example that demonstrates the use of the 'arun()' method of a 'crawler' object, which is indicative of the 'AsyncWebCrawler' class. This method is shown as the point where JavaScript code is executed before crawling a URL.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method handling JavaScript execution, but mischaracterizes the timing - it states JS runs 'before crawling' when the ground truth shows it runs during the crawl via crawler_strategy.crawl()",
      "error_type": "timing_misunderstanding"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'arun()' method of 'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy', which is inferred from the constructor details in the implementation. The connection arises because 'arun()' delegates the crawling strategy to an instance of 'AsyncPlaywrightCrawlerStrategy'.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on an inferred relationship between 'arun()' and AsyncPlaywrightCrawlerStrategy, while the ground truth describes the JavaScript execution functionality implemented through the crawl() method's js_code parameter. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' inherits from 'AsyncCrawlerStrategy'. This relationship is crucial for implementing the abstract 'crawl' method which undoubtedly includes the means of executing JavaScript.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as providing crawling functionality, but incorrectly assumes the existence of an 'AsyncPlaywrightCrawlerStrategy' class that isn't mentioned in the code or documentation.",
      "error_type": "unsubstantiated_assumption"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the `JsonCssExtractionStrategy`, describing how it is used with a schema to extract data using CSS selectors. This is directly shown in the example code, which demonstrates creating an instance of `JsonCssExtractionStrategy` with a `schema` and using it in the extraction.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy uses CSS selectors with a schema to extract data, which aligns with the ground truth's explanation of using BeautifulSoup's select() method with the schema's baseSelector",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example in the documentation snippet demonstrates the usage of the `arun` method of a `crawler` instance to perform crawling and extraction using `JsonCssExtractionStrategy`. This suggests that `arun` is a method invoked in connection with the extraction strategy.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used in conjunction with extraction strategies (like JsonCssExtractionStrategy) for web crawling, which aligns with the ground truth's explanation of how arun() integrates extraction strategies into the crawling workflow",
      "error_type": "none"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example shows the usage of `crawler` as an instance, implying the interaction with the `AsyncWebCrawler` class to execute a web crawl operation with the specified extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the documented schema format to extract structured data from HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler for web crawling but misses the crucial aspect of CSS-based extraction functionality and JsonCssExtractionStrategy implementation that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions using AsyncWebCrawler in the context of managing a session with the 'wait_for' parameter to handle dynamic content loading.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler supports the wait_for parameter for handling dynamic content loading, which aligns with the ground truth's explanation that this functionality is supported through the arun method's kwargs parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is used in the example to perform web crawling with the 'wait_for' parameter set, but it is not directly discussed in the documentation text.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that the arun method handles the wait_for parameter functionality, even though it doesn't detail the specific implementation. The core relationship between the code and documentation is captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly demonstrates creating a JsonCssExtractionStrategy object to extract commit information. The strategy is specified in the example's initialization.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy is used for extraction, but misses the key implementation detail that it uses BeautifulSoup to parse HTML based on schema selectors",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Implicitly, AsyncPlaywrightCrawlerStrategy is used through the AsyncWebCrawler since AsyncWebCrawler defaults to using this concrete implementation when none is provided explicitly.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on how AsyncPlaywrightCrawlerStrategy is used through AsyncWebCrawler, while the ground truth describes the wait_for functionality implementation within AsyncPlaywrightCrawlerStrategy itself. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example utilizes the kill_session() method on the crawler_strategy to end the session after crawling completes, as explicitly indicated in the demonstration.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of kill_session() being used to end the session after crawling, which aligns with the ground truth's description of cleaning up browser resources. While the ground truth provides more implementation details about closing page and context objects, the high-level relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of the AsyncWebCrawler class by using the 'async with AsyncWebCrawler(...) as crawler:' syntax, indicating that this class is used in the process being described.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage with async context management, but misses the key relationship between configuration parameters and AsyncPlaywrightCrawlerStrategy initialization described in the ground truth",
      "error_type": "incomplete_core_relationship"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly mentioned in the snippet, AsyncPlaywrightCrawlerStrategy is the default crawler strategy used by AsyncWebCrawler when initialized without specific strategy parameters. This is inferred from the default behavior defined in the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly suggests a connection between AsyncPlaywrightCrawlerStrategy and AsyncWebCrawler, but incorrectly states it is inferred rather than recognizing the explicit implementation of configuration options shown in the ground truth.",
      "error_type": "missing_implementation_details"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly shows a call to the 'arun' method of the AsyncWebCrawler class as 'result = await crawler.arun(url=\"https://example.com\")', indicating this method is central to the operation being demonstrated.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method and its async nature correctly, but misses the crucial aspect that the documented configuration parameters are handled through the constructor rather than the arun method directly.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation explicitly instructs to use the 'kill_session' method from the crawler strategy in the cleanup section to manage session resources.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies kill_session as a resource management method, but misses the core functionality of closing browser contexts and pages to prevent memory leaks, which is the key aspect described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides code snippets that use the 'arun' method to run crawling tasks. It implicitly relates to the session management context by showing how to operate within a session.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is used for crawling tasks with sessions, but misses the key point that it specifically stores the session_id to enable stateful crawling across requests",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The usage of 'arun' in the documentation implies the use of 'AsyncWebCrawler', which provides the context and interface for the `arun` method usage with session management.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session management through its 'arun' method, which accepts a 'session_id' parameter that enables maintaining state across multiple page visits as demonstrated in the documentation's state management example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the connection between AsyncWebCrawler and the arun method, but misses the crucial aspect of session management and state maintenance across page visits that is central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'kill_session' function is part of this class, and 'arun' methods indirectly rely on the 'AsyncCrawlerStrategy' subclass, indicating implementation within session management.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session and session management are part of the class, but misses key aspects like the sessions dictionary and state management across requests that are central to the ground truth relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example of using `arun()` method from the `AsyncWebCrawler` class to perform web crawling. The method call includes parameters like `url`, `excluded_tags`, and `keep_data_attributes`, which align with the functionality described for cleaning HTML by removing specified tags and data-* attributes.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the high-level functionality of the arun() method for web crawling and HTML cleaning with configurable parameters, matching the ground truth's description of HTML processing through sanitization and extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class implicitly appears as `arun()` is a method of this class. Although the class name isn't explicitly mentioned in the documentation, it is essential as the containing class for the `arun()` method.",
    "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's involvement but focuses on class/method structure rather than the actual HTML cleaning functionality described in the ground truth. It misses the core relationship between the class and HTML sanitization.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions using the 'AsyncWebCrawler' class with a proxy to perform web crawling. This aligns with the usage examples in the snippet, where 'AsyncWebCrawler' is instantiated with proxy configurations.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler works with proxy configurations, and while it doesn't explicitly mention the kwargs parameter passing to AsyncPlaywrightCrawlerStrategy, this implementation detail doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Although 'arun()' is not explicitly mentioned in the documentation, it is shown in the example where it is used to initiate a crawl. This indicates interaction with the 'AsyncWebCrawler' class for crawling operations.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as part of AsyncWebCrawler and its crawling functionality, but misses the crucial aspect of proxy configuration support through crawler_strategy.crawl() which is the main focus of the documentation",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy' by default, which inherits 'AsyncCrawlerStrategy'. This shows a hierarchical relationship where 'AsyncCrawlerStrategy' serves as a base interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncCrawlerStrategy as a base interface but focuses on inheritance hierarchy rather than its core purpose of enabling proxy-based crawling functionality as described in the ground truth",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "In the context of 'AsyncWebCrawler', 'AsyncPlaywrightCrawlerStrategy' is the default strategy used when none is specified. Its implementation specifics including proxy handling are implicitly referenced.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncPlaywrightCrawlerStrategy's role but fails to correctly describe the proxy implementation. It claims the proxy handling is 'implicitly referenced' when the ground truth shows it's explicitly implemented through ProxySettings objects.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly references the usage of the 'arun' method of the 'AsyncWebCrawler' class by showing an example use case where 'arun' is called to scrape a web page with the capability to capture a screenshot, thus demonstrating its API for these operations.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of the arun method's screenshot capabilities - the predicted description shows how to use it via example while the ground truth explains the implementation details, but they align on the fundamental relationship of arun() method handling screenshot functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'result = await crawler.arun' line in the snippet implies that 'arun()' returns an instance of 'CrawlResult' since it accesses the 'screenshot' attribute on the result. This suggests that the result provides information about the crawl operation, including a screenshot.",
    "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores the Base64-encoded image data as documented in the screenshot capabilities example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult handles screenshot functionality through its screenshot attribute, even though it focuses more on the method call aspect rather than the storage details.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The documentation snippet explicitly accesses the 'screenshot' attribute from the 'result' (an instance of 'CrawlResult') to check if a screenshot is available, which will then be saved into a file, implying its role in storing screenshot data captured during crawling.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the screenshot attribute stores screenshot data and is used to save captured screenshots, which aligns with the ground truth's description of it storing base64-encoded screenshot data for PNG generation",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation specifically discusses advanced scenarios using the 'JsonCssExtractionStrategy' class, indicating it implements functionality to extract nested structures from HTML.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy handles complex nested extraction from HTML structures. While it's less detailed than the ground truth, it doesn't contradict or misunderstand the fundamental purpose and capability.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While not mentioned in the text, 'JsonCssExtractionStrategy' extends 'ExtractionStrategy'. To understand the full capability of 'JsonCssExtractionStrategy', knowledge of its base class provides foundational behavior and requirements for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the key relationship that JsonCssExtractionStrategy inherits from ExtractionStrategy and acknowledges the base class's role in providing foundational behavior, which aligns with the ground truth's description of the base class providing architecture and capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows the usage of the 'arun' method from an instance of AsyncWebCrawler. This is explicit in the example: 'result = await crawler.arun(...)'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the usage of the arun method but misses the key content cleaning functionality that is central to the ground truth description. It fails to mention how the class implements noise removal and content preservation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun() method is used implicitly in the example code within the documentation snippet for processing a web page to obtain cleaned content.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is related to content cleaning, but misses the key point that it actively implements the cleaning features through specific parameters rather than just being used implicitly.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation explicitly mentions 'result.cleaned_html' as a property that retrieves the cleaned HTML content, which is part of the 'CrawlResult' data model.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies cleaned_html as a property for cleaned HTML content, but misses the crucial aspect that it's an Optional[str] that stores sanitized HTML after applying specific cleaning steps",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation snippet explicitly refers to 'result.markdown' as a feature for retrieving the markdown version of the cleaned content.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the markdown feature but omits the key aspect that it provides a cleaned, text-based version after noise removal",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the 'LLMExtractionStrategy' class for customizing the LLM provider, with parameters like 'provider', 'api_token', and 'instruction' demonstrated in the example code snippet.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy class allowing customization of LLM providers through provider and API token parameters, which matches the ground truth's explanation of the implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "As the base class of 'LLMExtractionStrategy', 'ExtractionStrategy' provides core methods and attributes inherited by the 'LLMExtractionStrategy'. The snippet infers this connection by demonstrating usage related to LLM extraction.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the base class that provides core functionality for LLM extraction through inheritance, which aligns with the ground truth's explanation of it being a foundation for implementing custom LLM providers.",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation explicitly mentions 'RegexChunking' as a class that splits text using regular expressions, as implemented in the code example.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of RegexChunking as a class that splits text using regular expressions, which aligns with the ground truth's explanation of using re.split() to split text based on patterns.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly shows the creation of an `AsyncWebCrawler` instance to crawl a URL, with anti-detection features enabled by passing 'magic=True'. This demonstrates that the `AsyncWebCrawler` class provides an interface for initiating crawling tasks while handling anti-bot protections.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles crawling with anti-bot protection, but misses that this functionality is specifically implemented through the crawler_strategy parameter and AsyncPlaywrightCrawlerStrategy rather than directly in the class itself.",
      "error_type": "incomplete_mechanism_description"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet includes an explicit call to the `arun()` method on the `AsyncWebCrawler` instance, using 'magic=True' to enable anti-detection features. This highlights that `arun()` can be configured to perform crawls with anti-bot strategies.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() can handle anti-bot features through configuration parameters, even though it describes a direct example rather than mentioning the delegation to crawler_strategy. The core relationship and functionality is accurately represented.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is implicitly part of the 'magic mode' mechanism by virtue of extending `AsyncCrawlerStrategy` and implementing navigation and anti-bot techniques, such as masking automation signals and simulating human behavior.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that AsyncPlaywrightCrawlerStrategy implements magic mode functionality through anti-bot and stealth features, even though it describes it more implicitly than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "`AsyncPlaywrightCrawlerStrategy`, which implicitly supports 'magic mode', extends `AsyncCrawlerStrategy`. The base class forms the foundation for crawler strategies, indicating that fundamental crawling method definitions are extended by `AsyncPlaywrightCrawlerStrategy` to support the features discussed.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class for crawler functionality, but incorrectly assumes AsyncPlaywrightCrawlerStrategy implementation details that aren't shown in the code or documentation. The ground truth more accurately describes the abstract class's role in defining interfaces for Magic Mode features.",
      "error_type": "unsupported_assumption"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly mentioned in the code snippet provided in the documentation for handling dynamic content. It is used to execute JavaScript for scrolling and interacting with a webpage as specified in the example calls.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for executing JavaScript and dynamic content handling, but misses the crucial aspect of handling form interactions and waiting for specified conditions/selectors mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While 'AsyncPlaywrightCrawlerStrategy' is not directly mentioned, the behavior described in 'arun' (such as handling JavaScript execution and dynamic content loading) is part of the implementation details of 'AsyncPlaywrightCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that AsyncPlaywrightCrawlerStrategy handles JavaScript execution and dynamic content, but doesn't specifically mention the smart_wait method or explain how it implements the scroll-to-bottom and form interaction patterns.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet demonstrates the usage of the `AsyncWebCrawler` class explicitly by opening a context using it with `async with AsyncWebCrawler(verbose=True) as crawler:`. This indicates a usage relationship.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the usage of AsyncWebCrawler but focuses on the context manager aspect rather than the core screenshot functionality relationship described in the ground truth.",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is shown in the example `result = await crawler.arun(...)`, indicating an implicit relationship where the method is used to perform the crawling operation that supports screenshot capturing.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling and supports screenshots, but misses explaining how the screenshot functionality is actually implemented through the screenshot parameter, caching, and base64 data handling.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The document emphasizes the use of `result.screenshot`, proving an implicit use of the `CrawlResult.screenshot` attribute to access and decode the screenshot data.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the screenshot field stores base64-encoded screenshot data that is accessed and decoded. The predicted description accurately captures this key relationship, even if it doesn't detail the full saving process.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation text explicitly mentions the 'JsonCssExtractionStrategy' by describing its functionality, which aligns with its implementation in the code. It discusses advantages such as speed, precision, structured output, and lack of external dependencies, which are characteristics of the 'JsonCssExtractionStrategy'.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on general advantages of the strategy rather than the core schema-based extraction relationship described in the ground truth, though it does mention the basic concept of CSS-based extraction",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The JsonCssExtractionStrategy would typically extend a common base class for extraction strategies, such as 'ExtractionStrategy', which is implied by the strategy pattern used here. Although not mentioned directly, the structure of the strategies points to an inheritance relationship with JsonCssExtractionStrategy inheriting from ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy as a base class and JsonCssExtractionStrategy as a derived class. While it doesn't mention the shared parallel processing capabilities specifically, it captures the core inheritance relationship and strategy pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions use of 'AsyncWebCrawler' to work with LLM Extraction. This indicates that AsyncWebCrawler is used to perform the main crawling process while utilizing LLMExtractionStrategy for data extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncWebCrawler provides asynchronous web crawling functionality with LLM extraction capabilities. The predicted description correctly identifies the core relationship between AsyncWebCrawler and LLM extraction, even if it doesn't mention specific implementation details like the arun method.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document snippet explicitly mentions 'LLMExtractionStrategy' as a strategy for extracting structured data or relevant content, showing that it is implemented to handle the extraction part of the crawling process.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core purpose of LLMExtractionStrategy as a strategy for extracting structured data using language models, which aligns with the ground truth's description of using LLMs to process and structure web data.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Even though it's not mentioned in the snippet, it's known from the code that LLMExtractionStrategy extends ExtractionStrategy, making it necessary for understanding LLMExtractionStrategy usage.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses the core purpose and functionality around parallel data extraction that is central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function 'arun()' is explicitly used in the provided code snippet, evidenced by the call to 'crawler.arun(...)'. This demonstrates the method being employed to perform web crawling with user-defined criteria.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a web crawling method but misses its core content filtering functionality described in the ground truth. The focus on method usage rather than its filtering capabilities is a significant omission.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although 'AsyncCrawlerStrategy' is not directly referenced in the text, 'AsyncWebCrawler.arun()' internally uses this strategy for its operations. The 'implementation' link is implicit in the usage pattern of these classes where 'AsyncWebCrawler' relies on 'AsyncCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy is used by AsyncWebCrawler, but misses the core point about how the content filtering parameters are passed through the strategy's crawl method kwargs",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly discusses the use of the 'AsyncWebCrawler' class in an example of initiating a web crawl process. It shows how to create an instance of this class and use it within an asynchronous context manager.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description acknowledges the AsyncWebCrawler class and its usage, but misses the core focus on caching functionality which is central to the ground truth relationship. While not wrong, it omits the key caching aspect that the documentation is specifically demonstrating.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the 'AsyncWebCrawler' class is explicitly invoked in the example to crawl URLs, first normally and then with the bypass cache option, highlighting the method's functionality as part of a web crawling sequence.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - both describe the arun method being used for web crawling with caching functionality that can be bypassed. While the predicted description is less detailed about the caching implementation, it correctly identifies the key functionality demonstrated in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document explicitly provides an example using a JSON-based CSS strategy for e-commerce scraping, defining a schema with a 'baseSelector'. This relates directly to how the `JsonCssExtractionStrategy` class is designed to handle similar schema-driven extraction tasks.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the JsonCssExtractionStrategy class implements the schema-based CSS extraction pattern shown in the e-commerce example, using base selectors to process HTML elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document mentions the use of LLM strategies for news article extraction, specifying an extraction `schema`. This corresponds to the `LLMExtractionStrategy` class, which supports schema-based extraction by incorporating API providers and specific schemas.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy supports schema-based extraction with API providers, which aligns with the ground truth's description of how the class implements schema-based article extraction using provider and schema parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The document provides an example of using a cosine strategy to perform topic analysis, highlighted by the semantic_filter and top_k parameters. This correlates with the `CosineStrategy` class designed for analyzing topics using cosine similarity-based methods.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that CosineStrategy performs content analysis using cosine similarity and semantic filtering, with parameters for filtering topics and returning top results. The predicted description covers the core functionality without contradicting the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the usage of JsonCssExtractionStrategy for pattern-based extraction on pages with repetitive patterns like product listings, demonstrating its initialization with a schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core purpose of JsonCssExtractionStrategy as a pattern-based extraction tool for repetitive content using a schema, which aligns with the ground truth's explanation of how it works",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example shows the use of an instance named 'crawler' which suggests AsyncWebCrawler because it shows the method 'arun' being used, which aligns with AsyncWebCrawler's methods designed for crawling and extraction tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its crawling capability, but misses the key pattern-based extraction functionality through JsonCssExtractionStrategy that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet implies the usage of the 'arun' method on the instance 'crawler', which aligns with the usage of arun in AsyncWebCrawler to execute crawling with specific extraction strategies.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun for crawling but misses the key relationship with JsonCssExtractionStrategy for structured data extraction using CSS selectors",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example accesses the 'extracted_content' from the result of 'arun', which corresponds to the structure returned by AsyncWebCrawler, thereby implying access to CrawlResult's 'extracted_content' attribute.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the result of arun(), but misses the core purpose of storing JSON-formatted pattern-matching results according to the schema structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is explicitly imported in the documentation example to demonstrate how different LLM providers can be used for data extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes that LLMExtractionStrategy is used with different providers, but misses the key implementation detail that this flexibility is achieved through constructor parameters (provider and api_token) that handle multiple service configurations. It oversimplifies the relationship to just an import statement.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends from ExtractionStrategy; this relationship is implied because LLMExtractionStrategy must implement or leverage base class functionalities.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends from ExtractionStrategy and must implement base functionality, which aligns with the ground truth showing that ExtractionStrategy is used as a base class for different LLM provider implementations.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides usage examples with the 'AsyncWebCrawler' class instantiated for different browser types like Firefox, WebKit, and Chromium with a focus on changing the 'browser_type' parameter.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncWebCrawler supports different browser types through configuration, and provides correct examples of Firefox, WebKit, and Chromium usage. While it doesn't mention the implementation details about crawler_strategy and PlaywrightCrawlerStrategy, this omission doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method is explicitly called in the example code for executing the crawl operation once the 'AsyncWebCrawler' context is initialized with the specified browser type.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that arun() works with browser selection, with the predicted description focusing on the runtime execution and the ground truth providing more implementation details. The minor differences in detail don't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, AsyncPlaywrightCrawlerStrategy is the likely default crawler strategy used by AsyncWebCrawler for interfacing with different browsers as suggested by the 'browser_type' parameter in the code snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy handles browser selection through browser_type parameter, even though it doesn't detail the implementation specifics in start() method. The core relationship between the strategy class and browser selection functionality is accurately identified.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is explicitly mentioned in the documentation snippet as part of the code example, indicating its usage for initiating a web crawler with verbose mode enabled.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its connection to verbose mode, but fails to capture that verbose enables detailed logging throughout execution via print statements. It only mentions the usage example without explaining the logging functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is implicitly part of the 'AsyncWebCrawler' usage demonstration in the example. It is called to perform the asynchronous crawl operation as indicated by the `arun(url=\"https://example.com\")` invocation.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the AsyncWebCrawler class structure and method invocation, while the ground truth specifically describes the verbose logging functionality implementation. These are fundamentally different aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates usage of the 'arun' method from the 'AsyncWebCrawler' class by making a call to 'crawler.arun'. The example provided shows how the method is invoked to perform a web crawl, indicating its primary function and use-case in performing asynchronous crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an asynchronous web crawling method, but misses the key functionality of HTML-to-markdown conversion which is the main focus in the ground truth.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The 'markdown' field in the 'CrawlResult' class is implicitly used in the documentation example as it is accessed via 'result.markdown'. Although 'markdown' isn't described in the snippet, its use in the code demonstrates how it serves as an attribute in storing and returning the markdown result of a crawl operation.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that markdown is a field storing converted markdown content accessible via result.markdown, aligned with the ground truth's explanation of it being a property storing HTML-to-Markdown converted text",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation includes 'from crawl4ai import AsyncWebCrawler' and demonstrates using it as 'async with AsyncWebCrawler(verbose=True)'. This indicates direct usage of the AsyncWebCrawler class within the example.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of AsyncWebCrawler and its import, but misses describing the core functionality implementation that matches the documented features (content filtering, processing, cache control) which is a significant omission compared to the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly called in the example as 'result = await crawler.arun(...', showing its specific usage in a web crawling task.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used in web crawling, but only describes a superficial example call while missing the key functional relationship between the documented parameters and their implementation in the method code.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result of the 'arun' method is assigned to 'result', which is used to check success with 'result.success', clearly showing use of the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of CrawlResult but misses the crucial aspect that it's a data structure class defining all the fields that store crawling outputs. It focuses only on the success check aspect.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Within the example code, the statement 'print(\"Content:\", result.markdown[:500])' demonstrates direct usage of the markdown attribute from the CrawlResult instance to display content.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the markdown attribute contains cleaned web content and is accessed via result.markdown to display content, with the ground truth adding minor details about the 500 character limit.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example uses 'for image in result.media[\"images\"]' to illustrate processing media elements. This indicates direct usage of the media attribute of CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that media is an attribute used to access media elements like images, as shown in the example code. The high-level relationship and usage pattern match the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The code 'for link in result.links[\"internal\"]' directly captures the usage of the links attribute, demonstrating it extracts link details from the crawl.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that the links attribute stores link information, but misses that it's specifically organized into internal/external categories in a dictionary structure",
      "error_type": "missing_crucial_structure"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In 'print(f\"Crawl failed: {result.error_message}\")', the error_message attribute is used for logging error details if the crawl fails.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that error_message is used to communicate failure details when the crawl is unsuccessful. The predicted description focuses on the logging aspect while the ground truth emphasizes error handling, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly demonstrates the use of the AsyncWebCrawler class to initialize a web crawler instance with specific configurations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler is used for web crawling with configurations, but misses the key relationship about user simulation functionality through simulate_user and override_navigator parameters that enable realistic browser behavior simulation.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet includes a direct call to the async method arun of the AsyncWebCrawler class, indicating its use in executing the web crawling with the user simulation parameters.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method but oversimplifies the relationship by suggesting direct implementation of user simulation, when the ground truth clarifies these features are actually implemented through kwargs parameters",
      "error_type": "oversimplification_omission"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Implicitly, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default strategy if no other strategy is provided. This is inferred from the initialization process documented in AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncWebCrawler using AsyncPlaywrightCrawlerStrategy as a default strategy, while the ground truth describes the specific user simulation functionality implemented within AsyncPlaywrightCrawlerStrategy. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The provided code snippet demonstrates the use of an 'AsyncWebCrawler' instance by calling 'arun()' to process iframes. This direct usage implies that 'AsyncWebCrawler' is designed to handle such content selections, as marked by the 'process_iframes=True' parameter.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of AsyncWebCrawler's iframe processing capability through the arun() method, noting the process_iframes parameter. While it omits mentioning remove_overlay_elements, this is a minor detail that doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method of 'AsyncWebCrawler' is explicitly called in the snippet to start the crawling process, including functions like processing iframes and removing overlay elements.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the main crawling method but portrays the iframe processing as an explicit part of the method rather than being handled through kwargs parameters as described in the ground truth",
      "error_type": "misrepresented_implementation"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Implicitly related through 'AsyncWebCrawler', which defaults to using 'AsyncPlaywrightCrawlerStrategy' when no other strategy is specified. This relationship is crucial for iframe processing capabilities.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions AsyncPlaywrightCrawlerStrategy's role but incorrectly implies the relationship is implicit through AsyncWebCrawler rather than directly describing the iframe processing functionality implemented in the class itself.",
      "error_type": "indirect_relationship_focus"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Provides an abstract strategy pattern that 'AsyncPlaywrightCrawlerStrategy' extends. This is implied as 'AsyncPlaywrightCrawlerStrategy' implements the necessary methods required for crawling operations.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract class providing a strategy pattern, but misses the core functionality of iframe processing capabilities mentioned in the ground truth.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions the creation of an instance of `AsyncWebCrawler` using an async context manager. The example code showcases how to initiate and use this class, which is responsible for handling web crawling tasks in an asynchronous manner.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship between the code and documentation - that the AsyncWebCrawler class implements async context manager functionality to handle initialization and cleanup, and the documentation shows the proper usage pattern using async with.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly called in the usage example, demonstrating how smart link filtering options can be passed to this method.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun method can be called with smart link filtering options, which aligns with the ground truth explaining that arun accepts filtering parameters through kwargs. While the ground truth provides more implementation detail, the core relationship is the same.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class implicitly relates to the documentation since 'arun()' is a method of this class. The example demonstrates instantiation of the crawler, which will likely be of this type or its derivatives, aligning the functionality with the smart link filtering feature.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is part of AsyncWebCrawler and handles URL processing, but misses the specific link filtering functionality that is central to the ground truth relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is explicitly mentioned in the documentation as part of the code snippet demonstrating how to add custom headers when using the crawler.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports custom headers, but misses the key implementation detail that this happens through kwargs being passed to AsyncPlaywrightCrawlerStrategy. It only describes the usage example without explaining the underlying mechanism.",
      "error_type": "omitted_implementation_detail"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is explicitly shown in the code snippet as being invoked with the URL provided, demonstrating usage of the class.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() with a URL parameter, but misses the key relationship regarding custom header support through crawler_strategy which is the main focus of the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is used as a default strategy within `AsyncWebCrawler` when instantiated, indicating its role in the crawling process.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description incorrectly focuses on AsyncPlaywrightCrawlerStrategy being a default strategy within AsyncWebCrawler, while the ground truth describes its custom header functionality through specific methods and implementation details.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the `JsonCssExtractionStrategy` by name and describes its purpose and usage with CSS selectors. This indicates that the class implements the functionality being documented.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the JsonCssExtractionStrategy class corresponds to the documented functionality of extracting data using CSS selectors. While it's less detailed than the ground truth, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly named, the documentation talks about using `JsonCssExtractionStrategy` with `AsyncWebCrawler`, implying a usage relationship where the `AsyncWebCrawler` class utilizes the strategy for extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the usage relationship between AsyncWebCrawler and JsonCssExtractionStrategy, but misses the specific implementation detail that it's used within the aprocess_html method for extracting structured data using CSS selectors from HTML content.",
      "error_type": "missing_key_implementation"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes an example where `crawler.arun(url=\"https://example.com\")` is called, directly referencing the use of the `arun()` method of the `AsyncWebCrawler` class.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method but misses the key purpose of markdown extraction and content processing described in the ground truth. It focuses only on the method call example rather than explaining the core functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The `result.fit_markdown` is mentioned explicitly in the example code snippet, indicating the use of the `fit_markdown` attribute of the `CrawlResult` class to access the main content in markdown format.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is an attribute used to access main content in markdown format, which aligns with the ground truth's explanation of it being a property holding cleaned, extracted content as markdown.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly uses the method `arun` from the `AsyncWebCrawler` class to crawl a webpage and obtain the raw HTML content. This is evidenced by the line in the code example: `result = await crawler.arun(url=\"https://example.com\")`.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is used to crawl webpages and obtain raw HTML content, which aligns with the ground truth's explanation of the method's functionality for retrieving unmodified webpage content.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `arun()` method is part of the `AsyncWebCrawler` class. The class is implicitly referenced because the documentation describes using its method `arun` to obtain HTML content, demonstrating its role in implementing web crawling functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements raw HTML retrieval through its arun() method, which returns a CrawlResult object containing the unmodified HTML in the 'html' property as documented in the code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between AsyncWebCrawler class and its arun() method for retrieving HTML content. Both descriptions convey that the class implements web crawling functionality through arun() to get HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides an example code snippet where an instance of 'AsyncWebCrawler' is created and used within an asynchronous context manager. This demonstrates direct implementation within the Crawl4AI framework.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship showing how AsyncWebCrawler is used with an async context manager, which matches the ground truth's description of the implementation using __aenter__ and __aexit__ methods. While it's less detailed than the ground truth, it captures the essential relationship correctly.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the example, the 'arun' method of 'AsyncWebCrawler' is called with a URL, indicating its use as the primary method for initiating a crawl. The method is invoked directly in the code provided in the documentation.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the main crawling method that takes a URL parameter, but misses significant functionality like caching, HTML processing, and the CrawlResult return object described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation example retrieves and prints 'markdown' from the 'result' object, which is returned by the 'arun' method, showing how the output of a crawl can be accessed in markdown format.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the markdown attribute contains the crawled text content that gets printed in the example, capturing the same core relationship between the code and its output",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the use of `JsonCssExtractionStrategy` with a provided schema to extract data from dynamically loaded web content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy uses a schema for data extraction but omits the crucial aspect of how it processes base elements and their fields through CSS selectors, which is a core part of how it enables structured data extraction",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text provides an example of using an instance of `AsyncWebCrawler` to perform web crawling and execute JavaScript, indicating direct usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's use for web crawling and JavaScript execution, but misses crucial details about its implementation of dynamic data extraction functionality, waiting conditions, and custom extraction strategies that are core to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code demonstrates the `arun` method of `AsyncWebCrawler`, implicitly showcasing the method being used to start the crawling process and run JavaScript execution.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for crawling and JavaScript execution, but misses the key aspect that it specifically supports dynamic content extraction through multiple capabilities (extraction strategy, js_code, screenshots) as detailed in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation, `AsyncWebCrawler` defaults to using `AsyncPlaywrightCrawlerStrategy` when initialized without specific strategy, as shown in the logic of `AsyncWebCrawler` initialization.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description claims AsyncWebCrawler defaults to AsyncPlaywrightCrawlerStrategy, while the ground truth describes AsyncPlaywrightCrawlerStrategy's functionality for handling dynamic content loading through JavaScript execution and selector waiting. These are completely different aspects of the system.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The JsonCssExtractionStrategy class is explicitly used in the example provided in the documentation snippet to define a strategy using a JSON schema for extracting content from HTML elements. The schema specifies the element patterns and fields to extract, demonstrating the class's functionality.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of JsonCssExtractionStrategy - using a JSON schema to extract content from HTML elements based on defined patterns and selectors, which aligns with the ground truth's explanation of pattern-based content extraction using schema definitions and CSS selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is implicitly used in the code example. This class likely manages the web crawling process, starting with a given URL and the specified extraction strategy. The await crawler.arun(...) pattern suggests that it uses the strategy defined by JsonCssExtractionStrategy to operate, though AsyncWebCrawler is not directly discussed in the snippet.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler manages web crawling and uses JsonCssExtractionStrategy for extraction, which aligns with the ground truth's description of pattern-based selection implementation. The core relationship between the crawler and strategy is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun() method of AsyncWebCrawler is implicitly referenced and used as it is the method that presumably executes the crawling and extraction process. This method's relation to the example indicates its role in handling asynchronous requests and processing HTML with the extraction strategy provided.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies arun() as the method that handles crawling and extraction processes, and while it's less specific than the ground truth, it accurately captures the core relationship of processing extraction strategies on crawled content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The provided code snippet in the documentation directly calls the 'arun()' method of the 'crawler' object, which is an instance of 'AsyncWebCrawler'. This matches with the 'AsyncWebCrawler.arun()' method from the available artifacts.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method but misses the key timing control functionality passed through kwargs. It describes a simpler relationship than what actually exists.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun()' method is part of the 'AsyncWebCrawler' class, implicitly linking the class to the documentation example, as it is understood to be instantiated.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct connection between arun() and AsyncWebCrawler class, but misses the crucial timing control functionality that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly uses `AsyncWebCrawler` by creating an instance of it within the `async with` statement. It indicates that the crawler is used in a context manager to manage lifecycle events like setup and teardown of resources.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage but misses the core relationship about handling protected sites with specific parameters and functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` of the `AsyncWebCrawler` instance is explicitly called in the example. It demonstrates how to initiate a crawl with specific parameters such as `url`, `magic`, `remove_overlay_elements`, and `page_timeout` to handle protected sites.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method usage and some parameters, but omits key functionality like error handling, caching, and customization features that are central to the relationship described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the example, `result.markdown` is accessed, indicating use of the `CrawlResult`'s `markdown` attribute. The code checks if `result.success` is true before accessing this property, demonstrating how to handle the crawl output if the operation is successful.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture the core relationship - that the markdown attribute stores the extracted text output from crawling and is accessed after checking for success.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides explicit code examples showing how to use the 'arun' method of the 'AsyncWebCrawler' class, specifically highlighting its ability to handle dynamic content by waiting for a specified condition ('wait_for') and using JavaScript ('js_code').",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the arun method and its dynamic content handling capabilities through wait conditions and JavaScript execution, which aligns with the ground truth's explanation of how the method implements these features via kwargs.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Since 'AsyncWebCrawler' relies on a 'crawler_strategy', the 'AsyncPlaywrightCrawlerStrategy' class implicitly implements the strategies needed for dynamic content handling as exemplified by the 'arun' method's capabilities.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes the strategy implementation aspect but misattributes the capabilities to the 'arun' method rather than the actual smart_wait and crawl methods that implement the dynamic content handling features.",
      "error_type": "method_attribution_error"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' extends 'AsyncCrawlerStrategy'. Therefore, 'AsyncCrawlerStrategy' provides the interface and abstract methods which 'AsyncPlaywrightCrawlerStrategy' implements, thereby facilitating the functionalities described in the documentation for dynamic content handling.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract interface class that provides the foundation for implementing dynamic content handling capabilities, which aligns with the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the class 'CosineStrategy', indicating that it implements configuration options such as 'semantic_filter', 'word_count_threshold', 'sim_threshold', 'max_dist', 'linkage_method', 'top_k', 'model_name', and 'verbose'. These parameters match the initialization parameters described in the 'CosineStrategy' class from the provided code artifacts.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that CosineStrategy implements the exact configuration parameters listed in the documentation with matching names and default values.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' class inherits from 'ExtractionStrategy', even though 'ExtractionStrategy' is not explicitly mentioned in the documentation. This implies that 'CosineStrategy' extends 'ExtractionStrategy', allowing it to implement the abstract methods or utilize its properties.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, and conveys the basic idea that CosineStrategy is an implementation of the abstract base class ExtractionStrategy. While it's less detailed than the ground truth about the configuration aspects, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of the LLMExtractionStrategy class when creating an instance of this strategy to extract content using an LLM with a particular schema.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that LLMExtractionStrategy is used for content extraction with LLMs and schemas, but omits crucial aspects about the class implementing structured content extraction and using LLM completions to parse web content into specified formats",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation code example includes a call to `result.extracted_content`, implying a relationship with the CrawlResult class's 'extracted_content' attribute, which is part of the method's output.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the existence of extracted_content as part of the result, but misses the crucial aspect that it specifically stores LLM-processed structured content as a JSON string matching a Pydantic schema",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation's code snippet makes a direct call to `crawler.arun`, indicating a use of the AsyncWebCrawler.arun method for executing the crawling strategy.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is involved in the crawling process but misses the key relationship with LLM-based extraction functionality described in the ground truth. The prediction focuses only on the crawling aspect without acknowledging the extraction strategy implementation.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the use of the 'AsyncWebCrawler' class to perform web crawling operations in the 'extract_article_content' function.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage for web crawling but omits crucial aspects about its support for different extraction strategies and context management shown in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly called twice in the snippet - once with the 'JsonCssExtractionStrategy' and once with the 'LLMExtractionStrategy', indicating its role in executing crawl tasks with specified strategies.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core functionality of arun() as executing crawl tasks with different extraction strategies, which aligns with the ground truth's description of it handling both pattern-based and LLM-based content extraction with specified strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' class is instantiated with a schema for structured content extraction within the 'extract_article_content' function, as described in the provided documentation snippet.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for structured content extraction based on a schema, which aligns with the ground truth's description of using CSS selectors to extract data according to a predefined schema structure",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'LLMExtractionStrategy' class is used to perform semantic analysis on the article content, as shown in the example where it's instantiated with parameters for the extraction process.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that LLMExtractionStrategy is used for semantic analysis of article content with configurable parameters, which aligns with the ground truth's description of its implementation for ArticleAnalysis with custom providers, schemas, and instructions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet implicitly uses 'CrawlResult.extracted_content' as it fetches and processes the 'extracted_content' from the 'pattern_result' and 'analysis_result' which are outcomes derived from 'arun()'.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that extracted_content is used in the CrawlResult object from pattern-based and LLM-based extractions, even if it uses slightly less technical language",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The 'media' attribute of the 'CrawlResult' is used in the documentation snippet as part of the returned data structure, demonstrating its role in capturing media results of the crawl.",
    "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that media is captured during crawling and returned as part of the final results structure. The predicted description captures the essential relationship even if less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation describes the 'Cosine Strategy' in detail, specifying its purpose and key steps, which directly maps to the 'CosineStrategy' class. The specific functions like chunking, vector conversion, similarity calculation, clustering, and ranking are likely implemented within this class.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the high-level relationship between the documentation and code, correctly identifying that the CosineStrategy class implements the 5 key steps outlined in the documentation. While the predicted description is less detailed than the ground truth, it correctly identifies the core mapping between documentation and implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Since 'CosineStrategy' deals with extracting relevant content sections from web pages, it is likely to extend from 'ExtractionStrategy', which serves as a base strategy class for extraction functionalities.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify ExtractionStrategy as a base class that provides fundamental structure for content extraction strategies, with the predicted description correctly identifying the inheritance relationship between ExtractionStrategy and CosineStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides a code example calling the 'arun' method of 'crawler'. This directly corresponds to the AsyncWebCrawler.arun() method, as it takes similar parameters such as url, word_count_threshold, remove_overlay_elements, and process_iframes.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documented options and the arun method implementation, recognizing that it handles similar parameters and configuration options as shown in the documentation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'arun' method of AsyncWebCrawler utilizes an instance of a crawler strategy that likely implements various crawling capabilities implied in the documentation, such as 'remove_overlay_elements' and 'process_iframes'. From the artifacts, AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy and provides detailed implementations of these behaviors.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements the documented crawling options, mentioning the key capabilities like handling iframes and removing overlays. The core functionality relationship is accurately captured, even if it's slightly less specific than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly mentions using the 'crawler.arun()' method, which is part of the 'AsyncWebCrawler' class. This indicates the 'AsyncWebCrawler' is employed as the main interface for performing web crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented best practices by incorporating caching mechanisms (through async_db_manager), configurable extraction strategies, and error handling with detailed success/failure reporting in its arun() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main interface for web crawling, but misses crucial aspects mentioned in the ground truth - specifically the implementation of caching mechanisms, configurable extraction strategies, and comprehensive error handling that are core to how it implements the documented best practices.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example provided in the documentation snippet utilizes the 'arun()' method within 'AsyncWebCrawler' to execute a crawling procedure. This shows direct use of this method as part of the code sample.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method usage but misses key aspects described in the ground truth about error handling patterns and caching implementation that align with the documentation's best practices",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The strategy choice examples in the text explicitly mention 'Cosine' as an option for content relevance, implying reliance on the 'CosineStrategy' class for execution.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions CosineStrategy being used for content relevance, but oversimplifies by only implying the connection rather than explaining the actual mechanism of using cosine similarity metrics for semantic filtering that the ground truth describes.",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The mention of different extraction strategies ('LLM', 'Cosine', etc.) implies that any strategy class is likely following the 'ExtractionStrategy' pattern or is a subclass, aligning with common software design principles in the context.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance/strategy pattern structure but misses the crucial parallel processing functionality and the abstract extract() method implementation aspect described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly references the `arun` method invocation of a `crawler` instance, showcasing an example of its usage with specific parameters within the Python code block.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes arun() method usage but fails to mention its core content filtering functionality described in the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet implies the use of `AsyncWebCrawler` class, as an instance of this class (denoted as `crawler`) is using its `arun` method. Although the class itself isn't mentioned by name, the class's method `arun()` is employed, thus establishing the connection.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the use of AsyncWebCrawler and its arun method, but misses the core relationship about the filtering options/parameters that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides an example using an instance of 'AsyncWebCrawler' to perform web crawling operations on a given URL. It shows how to conduct complex interactions like handling dynamic page content and session management.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's basic purpose for web crawling, but misses crucial aspects about its implementation of complex page interactions through specific features like JavaScript execution, dynamic content loading, and session management that are central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' of 'AsyncWebCrawler' is explicitly used multiple times in the documentation as part of loading initial and additional content from the dynamic page, demonstrating its role in executing crawl strategies with specific parameters.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used for dynamic page crawling with multiple parameters, matching how it's used in the documentation example for handling complex interactions like cookie consent and content loading.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example finishes by calling 'kill_session' on 'crawler.crawler_strategy', showing explicit use of this method for cleanup after multiple dynamic page interactions.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that kill_session is used for cleanup after interactions, but misses the crucial detail about closing browser resources (page and context objects) to prevent memory leaks",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although 'AsyncPlaywrightCrawlerStrategy' is not directly instantiated in the example, it is implicitly involved as the default strategy used by 'AsyncWebCrawler', providing implementation details for session management.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy's connection to AsyncWebCrawler but misses key functionality aspects around dynamic page handling, cookie consent, and JavaScript execution that are core to the implementation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The text snippet explicitly mentions 'TopicSegmentationChunking' as a strategy that uses the TextTiling algorithm to segment text into topic-based chunks, which identifies thematic boundaries.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that TopicSegmentationChunking uses TextTiling algorithm for topic-based text segmentation, which aligns with the ground truth's explanation of implementing TextTiling via NLTK for topic segmentation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is specifically mentioned in the documentation as the method being called to return a `CrawlResult` object. Evidence: \"The `arun()` method returns a `CrawlResult` object...\"",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() returns a CrawlResult object, but misses explaining the core processing functionality and how it generates all the documented properties through extraction/chunking strategies.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation describes accessing various properties of the returned `CrawlResult` including `html`, `cleaned_html`, `markdown`, `success`, `status_code`, `media`, and `links`. Evidence: \"...returns a `CrawlResult` object with several useful properties.\"",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation shows how to access various properties of CrawlResult that are defined in the class definition. Both descriptions align on the key properties demonstrated (html, cleaned_html, markdown, success, status_code, media, links) and their availability.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The documentation snippet explicitly refers to accessing `result.html` as a means to retrieve raw HTML from the `CrawlResult`: \"print(result.html)         # Raw HTML.\"",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the html property contains raw HTML content from the crawl result. The predicted description references this through the documentation example while the ground truth states it directly, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation explicitly lists the `cleaned_html` attribute of `CrawlResult`: \"print(result.cleaned_html) # Cleaned HTML.\"",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that cleaned_html is an attribute of CrawlResult, but misses the crucial detail that it's an Optional[str] type and its purpose as a sanitized version of the HTML content",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The Markdown representation of the crawl result is explicitly accessed in the documentation: \"print(result.markdown)     # Markdown version.\"",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that markdown content can be accessed through result.markdown property and is shown in the documentation example, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation covers accessing `result.fit_markdown`: \"print(result.fit_markdown) # Most relevant content in markdown.\"",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that fit_markdown contains markdown content but misses the crucial aspect that it's optional and specifically contains only the most relevant content from the webpage",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The document describes checking the `success` status: \"print(result.success)      # True if crawl succeeded.\"",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that success is a boolean field indicating crawl success, matching the ground truth's core meaning",
      "error_type": null
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The HTTP status code retrieval is explicitly mentioned: \"print(result.status_code)  # HTTP status code (e.g., 200, 404).\"",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that status_code represents HTTP status codes, but misses the Optional[int] typing and its role in crawl success verification",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The document details accessing extracted media via `result.media`: \"print(result.media)        # Dictionary of found media (images, videos, audio).\"",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that media content is accessed through result.media and that it contains media elements (images, videos, audio). While it doesn't explicitly mention the dictionary structure, it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "Access to `result.links` is directly explained: \"print(result.links)        # Dictionary of internal and external links.\"",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that result.links provides a dictionary of internal and external links, which aligns with the ground truth's explanation of the links property implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet describes an example using the `AsyncWebCrawler` class to demonstrate its ability to perform asynchronous crawling of web pages. This is clearly evidenced by the line `async with AsyncWebCrawler(verbose=True) as crawler:` where an instance of `AsyncWebCrawler` is initiated and used.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class enables asynchronous web crawling and can be used with async context managers (async with), which aligns with the ground truth's explanation about the class implementing __aenter__ and __aexit__ methods to enable the async with syntax.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly uses the `arun` method of the `AsyncWebCrawler` class to perform the actual crawling operation as seen in the line `result = await crawler.arun(url=\"https://www.nbcnews.com/business\")`.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses crucial aspects about its implementation of async functionality, content extraction, and error handling that are key parts of the ground truth relationship.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation uses the `markdown` attribute of the resulting `CrawlResult` object indirectly in the example through `print(result.markdown)`. The `markdown` attribute is part of the `CrawlResult` class which encapsulates the result of the `arun` method.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the markdown attribute stores/contains the extracted content and is used via result.markdown in the example code to display the content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet involves invoking 'crawler.arun', suggesting the use of an AsyncWebCrawler instance (likely named crawler). The method 'arun' is used to perform crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct class and async nature but misses the core media processing functionality described in the ground truth. It focuses only on the high-level crawling aspect without mentioning the media type organization that is central to the documented relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the example code, the 'arun' method of the AsyncWebCrawler class is directly invoked to execute a crawling operation, demonstrating its role in fetching media data.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling operations, but misses the crucial aspect of media selection and structured media data access through the CrawlResult object's media dictionary, which is the main focus of the documented functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation explicitly refers to accessing 'media' attributes such as images, videos, and audios from a CrawlResult instance (result), ascribed to contain such data post-crawl.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the 'media' dictionary contains categorized media elements (images, videos, audios) accessed after crawling, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly mentions the use of the 'AsyncWebCrawler' class in the example function 'integrated_js_and_wait_crawl()' to perform web crawling with integrated JavaScript execution.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for web crawling with JavaScript execution, but misses crucial aspects about how it implements this through the arun method and manages browser sessions. It describes just the usage rather than the implementation relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example in the text calls the method 'arun()' of the 'AsyncWebCrawler' class to execute the crawling process as described in the documentation.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling but misses its key integrated JavaScript functionality for handling pagination and content loading that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class, as shown in the usage, relies on 'AsyncPlaywrightCrawlerStrategy' for executing its crawling strategy, as explicitly seen through the instantiation strategy used for crawling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description incorrectly focuses on AsyncWebCrawler's usage of AsyncPlaywrightCrawlerStrategy, while the ground truth describes AsyncPlaywrightCrawlerStrategy's internal JavaScript execution and waiting functionality through its csp_compliant_wait method.",
      "error_type": "wrong_focus_and_relationship"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation includes a call to 'kill_session()' of the 'AsyncPlaywrightCrawlerStrategy' within the provided code snippet, explicitly closing the session after crawling.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core purpose of kill_session being used to clean up and close browser resources after the crawling process is complete",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The text describes the use of 'JsonCssExtractionStrategy' in order to extract commits using a defined schema as part of the web crawling strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JsonCssExtractionStrategy as a schema-based extraction mechanism, which aligns with the ground truth's explanation of how it processes HTML content using defined selectors and schema.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example usage of the 'arun' method of the 'AsyncWebCrawler' class. It demonstrates two separate calls to 'arun' with different extraction strategies (CSS and LLM).",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() method and its ability to use different extraction strategies, but misses the key point about combining strategies sequentially for more powerful extraction as emphasized in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "While the LLMExtractionStrategy class is not explicitly named in the code snippet, it is referenced implicitly by 'llm_strategy' in the example, suggesting that it is being used as the extraction strategy in the second call to 'arun'.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy is used for extraction, but misses explaining its key role in semantic analysis and how it processes content after CSS extraction. It focuses only on the existence of the class rather than its functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy class is implicitly referenced in the code snippet through 'css_strategy'. The snippet shows the initial extraction step using a CSS strategy, pointing toward this class being part of the process.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as being related to CSS-based extraction, but incorrectly states it's only 'implicitly referenced' when it's actually explicitly shown in the code. It also misses explaining its role in the combination strategy pattern.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet uses the `arun` method of the `AsyncWebCrawler` class to configure the crawler to simulate human behavior, mask automation signals, and enable anti-detection features by setting `simulate_user`, `override_navigator`, and `magic` parameters to true.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method accepts and handles stealth-related parameters for anti-detection features, which aligns with the ground truth's explanation of how these parameters are passed through kwargs to implement anti-detection functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` initializes with `AsyncPlaywrightCrawlerStrategy` as the default strategy, which is related because the documentation implies using a strategy that supports configuration for anti-detection features.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes anti-detection features exist but incorrectly suggests AsyncWebCrawler is involved and misses that these features are directly implemented in AsyncPlaywrightCrawlerStrategy through JavaScript injection",
      "error_type": "incorrect_class_relationship"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` extends from the `AsyncCrawlerStrategy` abstract base class, inheriting the structure necessary to implement the methods that configure the anti-detection features described.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy provides the base structure for implementing anti-detection features, which aligns with the ground truth's explanation of how the abstract class defines the interface for stealth capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly describes a schema for extracting complex HTML structures, indicating the usage of a strategy to parse HTML content into structured data, which aligns with the role of the JsonCssExtractionStrategy class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the schema-based HTML extraction purpose but misses the key implementation detail about recursive parsing using BeautifulSoup's select() method",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly calls the 'arun' method on an instance of 'AsyncWebCrawler' to perform metadata extraction from a URL.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler is used for metadata extraction, but it oversimplifies the relationship by only mentioning the arun method while missing the crucial role of aprocess_html in actually extracting and processing the metadata into the result object.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is explicitly mentioned in the example code provided in the documentation snippet as responsible for initiating the metadata extraction from the URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as handling metadata extraction but oversimplifies by only mentioning it initiates extraction without acknowledging it returns a CrawlResult object with accessible metadata properties",
      "error_type": "oversimplification"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "The 'metadata' attribute of the 'CrawlResult' class is implicitly used in the documentation example to access the extracted metadata, showing that 'AsyncWebCrawler.arun()' returns a 'CrawlResult' object.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that metadata is stored in a CrawlResult object returned by arun(). While it is less detailed than the ground truth about the specific metadata fields available, it correctly identifies the key relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet includes a code example that directly uses the `AsyncWebCrawler` class to perform a web crawling task. This is evident from the instantiation and usage of the `crawler` object, which is likely an instance of `AsyncWebCrawler`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling, but misses the core relationship about HTML-to-text customization through configuration parameters that the ground truth emphasizes",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet in the documentation calls the `arun` method on the `crawler` object. This method is a part of the `AsyncWebCrawler` class and is explicitly used in the provided example to perform the crawling operation with specific parameters for HTML to text conversion.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is used for crawling with specific parameters, and while it doesn't explicitly detail the html2text options, it correctly captures the core relationship between the documentation and code showing parameter passing functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The documentation snippet implies the use of a crawling strategy. The `AsyncWebCrawler` typically depends on an `AsyncCrawlerStrategy` or its derivatives, like `AsyncPlaywrightCrawlerStrategy`, to define how crawling operations are executed.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is a crawling strategy implementation, it misses the main point about HTML to text customization functionality described in the ground truth. The prediction focuses on general dependency structure rather than the specific HTML to text configuration capability.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The NlpSentenceChunking class is explicitly mentioned in the documentation snippet as it is used to split text into sentences using NLP models, ensuring accurate sentence boundaries.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that NlpSentenceChunking uses NLP models for sentence boundary detection, which aligns with the ground truth's explanation of using NLTK's sent_tokenize for accurate sentence splitting.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly shows the usage of the `arun()` method by calling it with an URL, which is directly related to invoking the web crawling process as demonstrated in the text.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only captures the basic URL crawling aspect but misses the core markdown extraction functionality described in the ground truth.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "In the provided code example, there is a print statement displaying `fit_markdown`, which is a class-level attribute of `CrawlResult`, implicitly demonstrating its use in displaying the main content extracted by the process.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies fit_markdown as being used to display content, but misses the core purpose of storing cleaned main content in markdown format and removing boilerplate",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation directly instructs users to import `AsyncWebCrawler` and create an instance of it within an async context manager, indicating that this class is explicitly used in the provided code snippet.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler can be used with an async context manager, which matches the implementation in the code showing __aenter__ and __aexit__ methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly mentions using the `arun()` method to crawl a URL with JavaScript code and waiting conditions.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() handles JavaScript execution and waiting conditions, but misses the key functionality around form interactions and dynamic content handling through scrolling and load more interactions that are central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the documentation does not explicitly state the `AsyncWebCrawler`, the `arun()` method's usage implies its involvement as it belongs to this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and arun() method, but fails to acknowledge the key functionality of dynamic content handling through js_code and wait_for parameters that enable scrolling and form interactions",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` is designed to use a `crawler_strategy`, which defaults to `AsyncPlaywrightCrawlerStrategy`. This implies methods like `arun()` eventually interact with this class for operations like JavaScript execution.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as a key component for handling operations, but misses the core focus on dynamic content loading and smart_wait functionality that's central to the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions and uses the 'CosineStrategy' class. It provides examples of how this class should be initialized with parameters like 'word_count_threshold', 'top_k', and others, indicating direct usage of this class in practice.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how the documentation provides configuration guidance for CosineStrategy's parameters and their practical usage, which aligns with the ground truth's explanation of how these parameters control text filtering and clustering behavior for different content types.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is likely a subclass of the more abstract 'ExtractionStrategy' since it is implementing extraction logic as inferred by its initialization and parameters in the examples. This piece of the hierarchy can be deduced based on typical design patterns in strategy implementations.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted relationship focuses on CosineStrategy being a subclass, while the ground truth emphasizes ExtractionStrategy's framework and configuration flexibility. While inheritance might be true, it misses the core purpose described in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation directly mentions the 'arun' method being used to perform a crawling operation with domain-based filtering. The example shows 'arun' as being key to initiating the crawling with specific arguments for excluding domains.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is used for crawling with domain-based filtering functionality, which aligns with the ground truth's explanation of how the method handles domain filtering options through kwargs.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method in the 'AsyncWebCrawler' class is explicitly called in the documentation snippet's example. This indicates that the method is used to initiate a web crawling process with specific anti-detection features.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the web crawling method, but misses the key relationship that arun() accepts and implements stealth-related arguments through kwargs to enable anti-detection features",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class uses a 'crawler_strategy', which by default is 'AsyncPlaywrightCrawlerStrategy', implementing 'AsyncCrawlerStrategy'. This base class provides the abstract definition for the strategy methods likely used in tasks like masking automation signals.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class providing abstract methods, but focuses on the general strategy pattern rather than specifically explaining its role in anti-detection features as described in the ground truth.",
      "error_type": "missing_key_purpose"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class extends 'AsyncCrawlerStrategy' and implements the crawling functionalities. While not directly mentioned, this strategy is the default for 'AsyncWebCrawler', used to possibly set the anti-detection features depending on the user simulation and navigator overrides.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as extending AsyncCrawlerStrategy and its role in crawling, but misses the key point about how it specifically implements anti-detection through JavaScript injection to override navigator properties. The prediction speculates about 'possibly' setting anti-detection features while the ground truth clearly describes the concrete implementation mechanism.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The text snippet explicitly mentions the 'JsonCssExtractionStrategy' as a class designed for extracting data using CSS selectors. This is directly aligned with the provided code example.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies JsonCssExtractionStrategy's purpose for CSS-based extraction, it omits the crucial implementation detail about using BeautifulSoup's select() method and the schema-based field extraction process that's central to how it works",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the provided code example, 'arun' is implicitly used as the method to execute the web crawling task with the 'extraction_strategy' parameter set to an instance of 'JsonCssExtractionStrategy'.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that 'arun' is used for web crawling and accepts extraction_strategy, but misses the core aspect that it implements the execution logic that enables CSS-based extraction and integrates it into the crawling workflow",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method from `AsyncWebCrawler` is explicitly called in the provided snippet to initiate the crawling process for media extraction from a URL.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses the crucial aspect of the media selection functionality and the structured media data returned in the CrawlResult that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code snippet in the documentation demonstrates accessing the `media` attribute from the `CrawlResult` object returned by the `arun` method to access different types of media.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the media dictionary attribute is used to access different media types from the crawler result, which aligns with the ground truth's explanation of the media dictionary storing and providing access to categorized media elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation explicitly mentions adding a chunking strategy called `RegexChunking` and provides an example of how RegexChunking is used with a regex pattern to split text.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that RegexChunking is a chunking strategy that uses regex patterns, but misses mentioning the crucial default pattern and the core splitting functionality that applies patterns iteratively to create paragraphs",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example in the documentation shows the `arun` method being invoked on an `AsyncWebCrawler` instance. While `arun` is not directly discussed, it is crucial in demonstrating how the RegexChunking strategy is applied in practice.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's role in using RegexChunking but misses the key implementation detail that arun validates and defaults the chunking_strategy parameter. While it shows usage, it doesn't capture the integration aspect.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the documentation example, an `AsyncWebCrawler` instance is created, showing how the user sets up the crawler with verbosity enabled but without much explanation on the class's structure.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and its verbosity setting, but misses the core functionality of RegexChunking integration, which is the main focus of the ground truth relationship.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is explicitly mentioned as it is used in example code snippets to perform browser-specific crawling with parameters such as 'browser_type'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler supports different browser types through configuration parameters, which aligns with the ground truth's explanation of browser engine selection via crawler_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun() method is implicitly related via the example code where AsyncWebCrawler's instances call it to perform crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as being used by AsyncWebCrawler for crawling, but misses the key relationship about browser engine selection via browser_type parameter that is central to the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler utilizes AsyncPlaywrightCrawlerStrategy which is a subclass of AsyncCrawlerStrategy. This establishes an implicit connection since AsyncWebCrawler relies on this strategy pattern.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a strategy pattern base class, but focuses on AsyncPlaywrightCrawlerStrategy specifically rather than explaining how it defines the interface for multiple browser types (Chromium, Firefox, WebKit).",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This strategy does the actual operation as seen in the example by implementing methods that can be invoked via AsyncWebCrawler for diverse browser operations.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that this is a strategy for browser operations but misses the specific focus on browser type selection functionality described in the ground truth. While it mentions browser operations generally, it doesn't highlight the key relationship with browser type selection.",
      "error_type": "missing_key_feature"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly mentions calling the method 'arun' on the 'crawler' object, demonstrating its usage for converting HTML to text configurations.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method usage but misses the key aspect of html2text configuration options being passed as kwargs for webpage content conversion",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not directly stated, the 'arun' method is part of the 'AsyncWebCrawler' class. The documentation implicitly relates to 'AsyncWebCrawler' as the method belongs to this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun is part of AsyncWebCrawler, but misses the key relationship about HTML-to-text customization functionality through kwargs parameters, which is the main point of the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' uses a 'crawler_strategy' that defaults to 'AsyncPlaywrightCrawlerStrategy'. The documentation does not explicitly mention this, but utilizing 'arun' indirectly relies on the strategy which is a subclass of 'AsyncCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements HTML to text customization by accepting configuration options through its crawl method's kwargs parameter, which can be passed down from the crawler.arun() method documented in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy as a default strategy, while the ground truth describes how the class implements HTML to text customization through configuration options. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncPlaywrightCrawlerStrategy', the default crawling strategy in 'AsyncWebCrawler', extends 'AsyncCrawlerStrategy', indicating a foundational role in providing abstract methods for various crawling operations.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface used to implement HTML-to-text customization options through its crawl method, which accepts kwargs to pass configuration parameters like the html2text options shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class with multiple abstract methods, but misses the core purpose of enabling HTML-to-text customization through configuration parameters. The predicted text focuses more on inheritance structure than functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly uses the 'AsyncWebCrawler' class in code examples to demonstrate setting a custom user agent and headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the AsyncWebCrawler class handles identity management through custom user agent and headers configuration, as shown in the documentation examples. While it doesn't explicitly mention the implementation details in the arun method, the core relationship is correctly identified.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is shown in the documentation as the method called on the 'AsyncWebCrawler' instance to execute a web crawling task with custom settings, implying its significant role in carrying out the primary functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes arun() as the main execution method but misses the specific identity management functionality through user_agent and headers that is central to the ground truth relationship",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Given the hierarchical structure in the codebase, 'AsyncWebCrawler' likely depends on 'AsyncCrawlerStrategy' as part of its initialization or operations strategy, though not directly mentioned, it indicates a typical inheritance relationship.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies a relationship between AsyncWebCrawler and AsyncCrawlerStrategy, but misses the core purpose of identity management and focuses instead on inheritance patterns",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This strategy class is likely used by 'AsyncWebCrawler' as its default strategy, implementing methods required by 'AsyncCrawlerStrategy', implied by its usage context.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies this as a strategy class used by AsyncWebCrawler, but misses the core identity management functionality (custom user agents and headers) that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet demonstrates the use of the AsyncWebCrawler class through the context manager pattern, indicating its role directly in handling asynchronous web crawling operations which fits the intended usage as per documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager pattern and async nature, but misses the main point about its specific functionality for handling overlays through the remove_overlay_elements parameter, which is the core focus of the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet includes a function call to `arun()` method of AsyncWebCrawler, showing its functionality for performing specific crawl tasks, such as handling overlay removal and taking screenshots.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main functionality of the arun() method for handling crawl tasks including overlay removal and screenshots, which aligns with the ground truth's description of the method's implementation of these features",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, providing implementations for the asynchronous crawling actions demonstrated in the snippet.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description only states that AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, while the ground truth specifically describes the overlay removal functionality implemented in the remove_overlay_elements method. These are completely different aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation describes the use of 'await crawler.arun(...)'. Here, 'crawler' is an instance of a class that supports the 'arun' method, indicating the use of 'AsyncWebCrawler' as it provides this method. This is evidence that 'AsyncWebCrawler' is utilized in the context provided.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an arun() method that accepts extraction strategies (JsonCssExtractionStrategy or LLMExtractionStrategy) and JavaScript execution parameters to handle dynamic content extraction, as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler provides the arun method, but fails to mention the key functionality of handling extraction strategies and JavaScript execution for dynamic content extraction, which are central aspects highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The documentation mentions classes 'JsonCssExtractionStrategy' and 'LLMExtractionStrategy', which imply functionality for extraction strategy. These classes extend 'ExtractionStrategy', a base class for different extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy and LLMExtractionStrategy inherit from/extend ExtractionStrategy as a base class, which aligns with the ground truth's explanation of ExtractionStrategy providing the foundational structure for these implementations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' is explicitly described in the snippet as an extraction strategy that utilizes a schema for extraction, such as selecting HTML elements using CSS selectors. This indicates it's implementing an extraction process as mentioned.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JsonCssExtractionStrategy as using a schema with CSS selectors for extraction, which aligns with the ground truth's description of pattern-based extraction using BeautifulSoup and CSS selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is explicitly mentioned to demonstrate the extraction of content using an LLM provider and a schema, implementing a distinct form of extraction strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that LLMExtractionStrategy is used for extracting content using LLM providers with schema support, which aligns with the ground truth's description of processing HTML content through LLM models with configurable providers and schemas.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet demonstrates the use of 'await crawler.arun(...)', showing an example usage of this method to facilitate actions which include page interactions and data extraction.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for page interactions, but misses the crucial aspect that it specifically implements extraction strategies (JsonCssExtractionStrategy, LLMExtractionStrategy) for structured data extraction, which is a core part of the documented functionality.",
      "error_type": "key_feature_omission"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation explicitly mentions the `RegexChunking` class as a chunking strategy that splits text using regular expressions. It details the class usage in an example block demonstrating how to use the class.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core functionality of RegexChunking as a text splitting strategy using regular expressions, which aligns with the ground truth's explanation of using re.split() to split text based on patterns.",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The `RegexChunking` implements a strategy for chunking. As it is expected to inherit from an abstract `ChunkingStrategy` class, there is an implicit relationship that `RegexChunking` provides the specific implementation details for the abstract method it inherits.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core inheritance relationship between RegexChunking and ChunkingStrategy, noting that RegexChunking implements the abstract methods defined by ChunkingStrategy. While it's less detailed than the ground truth, it conveys the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is directly used in the documentation as an instance is created and its method 'arun' is called. This shows that the class is responsible for crawling based on the provided URL and parameters, including handling media processing.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements media processing through its aprocess_html method, which extracts and processes images with metadata (src, alt, context) using WebScrappingStrategy and stores the results in the media dictionary as documented in the code examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles crawling and media processing, but misses the key implementation details about how media processing occurs through WebScrappingStrategy and the structured metadata extraction that the ground truth describes.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' of the AsyncWebCrawler class is explicitly used in the documentation. The examples provided demonstrate how the method is called with parameters like URLs and options to handle media and lazy-loaded content, reflecting the functionality discussed.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() being used for web crawling and handling media content, but misses important functionality around caching, screenshots, and customizable crawling strategies mentioned in ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The CrawlResult class is explicitly referenced since results of the crawls are stored in a CrawlResult object as hinted in the documentation with 'result.media[\"images\"]'. This indicates the object structure that holds the media output after processing.",
    "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CrawlResult stores crawl results and references the media access, but misses explaining the actual media processing functionality and metadata structure that is central to the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The 'media' attribute of the CrawlResult class is directly referenced in the documentation. The example accesses media information within the 'result' object, which indicates the media processing output stored in this attribute.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that media data is stored in the media attribute and accessible via the result object, which aligns with the ground truth's explanation of CrawlResult.media storing media-related data. While the ground truth provides more details about specific media metadata, the core relationship is correctly understood.",
      "error_type": ""
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text provides a code example that directly uses `crawler.arun()` method, which is part of the `AsyncWebCrawler` class. Evidence can be found in the line `result = await crawler.arun(...)`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that crawler.arun() is used, but misses the key relationship about CSS selector functionality being implemented through the css_selector parameter to target specific HTML elements. The core purpose of the relationship is not captured.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` class uses a strategy to perform crawling. The `AsyncCrawlerStrategy` is the base class that provides the definition for such strategies. Even though it's not directly mentioned, it's implicitly used as it establishes the interface that the `AsyncWebCrawler` requires.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables CSS selector-based content extraction through its crawl method, which is demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class for crawling functionality, but misses the key point about CSS selector-based content extraction which is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncPlaywrightCrawlerStrategy` class extends `AsyncCrawlerStrategy` and could be used by `AsyncWebCrawler` for performing crawling actions. This extension provides concrete implementations needed for executing the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements CSS selector functionality through its crawl method, which uses Playwright's page.wait_for_selector() to target and extract specific content elements as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, but misses the crucial CSS selector functionality which is the main focus of the ground truth relationship. The ground truth specifically emphasizes the class's CSS selector implementation for content extraction.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "`arun()` is the method called in the example shown in the text to perform web crawling with CSS selectors. It explicitly implements the functionality documented.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() implements the CSS selector functionality shown in the documentation. While it is less detailed than the ground truth, it captures the core relationship between the code and documentation without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet provides an example using `crawler.arun` to retrieve a cleaned HTML result, indicating that `AsyncWebCrawler` is the class implementing the `arun` method, as evidenced by its signature and context of usage.",
    "ground_truth_relationship": "The AsyncWebCrawler's aprocess_html method implements HTML cleaning by using a WebScrappingStrategy to remove unwanted elements and produce cleaned_html output, which directly fulfills the documentation's promise of sanitizing and removing unnecessary HTML elements while preserving structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as implementing arun, but misses the key point about the HTML cleaning functionality being implemented through WebScrappingStrategy in aprocess_html method, which is central to the documented cleaning functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is used directly in the documentation example to demonstrate how to obtain and print `cleaned_html`. The code snippet in the documentation matches the signature and functional description provided in the `AsyncWebCrawler.arun()` method.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() method usage but misses the core functionality of HTML cleaning/sanitization that is central to the ground truth. It focuses more on method invocation rather than the actual purpose.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation shows `cleaned_html` being accessed in the example. This indicates the `CrawlResult` class attribute `cleaned_html` is part of the resulting object from the `arun` method, showing the retrieval of cleaned HTML as an output.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that cleaned_html is an output attribute of the CrawlResult from arun() that contains cleaned HTML content, which aligns with the ground truth's explanation of it storing sanitized HTML after processing.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet explicitly imports and uses 'AsyncWebCrawler' as indicated by 'from crawl4ai import AsyncWebCrawler'. It's used in the example where an instance of 'AsyncWebCrawler' is created and used to run the 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of AsyncWebCrawler but misses key functionality around extraction strategies and customization that's central to the ground truth relationship. It only describes basic instantiation and method calls without capturing the broader purpose.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example in the documentation calls the 'arun' method of the 'AsyncWebCrawler'. This is implicit in the task the 'AsyncWebCrawler' executes within the 'extract_tech_content' asynchronous function.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between the documentation and code (use of arun method), but misses the key relationship about using LLMExtractionStrategy for content filtering which is a crucial aspect of what the example demonstrates",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'LLMExtractionStrategy' is explicitly mentioned in the snippet's code as part of the extraction process, highlighting its usage for extracting technology-related content using a large language model provider.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for content extraction using a language model provider, which aligns with the ground truth's explanation of how it implements content extraction functionality through LLM models with specific instructions.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation explicitly references the 'result.media' attribute, demonstrating how it stores video and audio data for further processing.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that result.media stores video and audio data, which aligns with the ground truth's explanation of how the media dictionary contains lists of video and audio metadata dictionaries.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is explicitly mentioned in the example provided in the documentation. It is used to perform a crawl operation with specific options passed as arguments, such as `url`, `word_count_threshold`, and other processing flags. The documentation provides a direct example of how to use this method.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that arun is an asynchronous crawling method that accepts configuration options as shown in the documentation example. While it doesn't mention all implementation details, it correctly identifies the main functionality and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncWebCrawler` internally uses an instance of a strategy derived from `AsyncCrawlerStrategy`. The usage of options like `remove_overlay_elements` and processing of iframes likely link back to methods defined or enforced by the strategy pattern implemented in `AsyncCrawlerStrategy`.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy enables the crawler configuration options through its abstract methods, particularly noting how the strategy pattern allows for these configurations via kwargs. While it's more speculative in tone, it captures the same core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation explicitly mentions the `arun()` method and provides a usage example: `result = await crawler.arun(url=\"https://example.com\")`. This shows that `arun()` is used to make asynchronous web crawling requests.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an asynchronous web crawling method, but misses the crucial aspect that it processes and returns a structured CrawlResult object containing multiple content formats and metadata from the crawled page.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The `arun()` method is documented to return a `CrawlResult` object, as seen in the line: 'The `arun()` method returns a `CrawlResult` object with several useful properties.'",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() returns a CrawlResult object, but fails to describe the key relationship that the CrawlResult class defines the properties shown in the documentation example through its type-annotated fields.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "In the documentation, `result.html` is used to retrieve the raw HTML, which suggests that `html` is an attribute of `CrawlResult`.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that html is an attribute/property of CrawlResult that stores raw HTML content, which aligns with the ground truth's description of a string property storing raw HTML content",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation example calls `result.cleaned_html` to access cleaned HTML content, indicating that `cleaned_html` is an attribute of `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies cleaned_html as an attribute of CrawlResult that provides access to cleaned HTML content, which aligns with the ground truth's description of it being a property storing sanitized HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation utilizes `result.markdown` to access a markdown version of content, pointing to `markdown` as an attribute of `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that markdown is an attribute of CrawlResult accessible via result.markdown that provides a markdown version of the content, which aligns with the ground truth's explanation",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "When the documentation lists `result.fit_markdown` to get the most relevant content, it implies `fit_markdown` is an attribute of `CrawlResult`.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies fit_markdown as an attribute of CrawlResult, but misses the crucial information that it's an optional string containing the most relevant content in markdown format",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "Checking `result.success` as indicated in the documentation example reveals `success` as a boolean attribute in `CrawlResult` to verify crawl completion.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that success is a boolean attribute of CrawlResult used to verify crawl completion, which aligns with the ground truth's explanation of it being a boolean field indicating successful crawl operation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "Example in documentation uses `result.status_code` to obtain HTTP status codes, identifying `status_code` as an attribute of `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies status_code as an attribute of CrawlResult used for HTTP status codes, which aligns with the ground truth that it stores the HTTP status code from web requests. The minor details about Optional[int] typing in the ground truth don't change the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Utilization of `result.media` maps to the `media` attribute within `CrawlResult`, collecting multimedia from a response, as per the documentation.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the media property stores multimedia content from crawl responses, which aligns with the ground truth's explanation of the empty dictionary storing media elements like images, videos, and audio.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "Documentation example fetching `result.links` indicates `links` is an attribute in `CrawlResult` to collate URLs encountered.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that links is an attribute of CrawlResult used to store URLs found during crawling, which aligns with the ground truth's description of it being a dictionary property storing internal and external links.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly discusses the 'JsonCssExtractionStrategy' by name as a feature of Crawl4AI. It describes its function and how it allows users to extract structured data from web pages using CSS selectors based on a predefined schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that JsonCssExtractionStrategy uses CSS selectors to extract structured data based on a schema. While it doesn't mention BeautifulSoup specifically, it correctly describes the main functionality and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides an example of using 'JsonCssExtractionStrategy' with the 'AsyncWebCrawler'. It describes how to implement this strategy with AsyncWebCrawler, suggesting a usage relationship where AsyncWebCrawler interacts with JsonCssExtractionStrategy for data extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used by AsyncWebCrawler for data extraction, which aligns with the ground truth showing the implementation in aprocess_html method. While the predicted description is more general, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions `CosineStrategy` and demonstrates how to create a strategy instance with specific parameters like `linkage_method`, `max_dist`, and `model_name`. This indicates that `CosineStrategy` implements custom clustering as described.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy's custom clustering functionality but omits the important content filtering pipeline aspect mentioned in the ground truth. It fails to mention the semantic filtering and word count threshold capabilities that are core features.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation includes a `Content Filtering Pipeline` example that shows the `extract_pricing_features` function using the `AsyncWebCrawler` class to perform a web crawl with the `CosineStrategy` as an `extraction_strategy`. This implies that `AsyncWebCrawler` uses strategies like `CosineStrategy` for its crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between AsyncWebCrawler and extraction strategies, mentioning how the AsyncWebCrawler uses strategies for crawling operations. While it doesn't detail all features like caching and clustering support, it correctly identifies the main functional relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example function in the documentation calls the `arun` method on an instance of `AsyncWebCrawler`, specifying the `CosineStrategy` as the `extraction_strategy`. This illustrates the role of `arun()` in executing a crawl operation with a given strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of arun() with extraction strategy, but misses crucial aspects about its role in handling clustering configurations, caching, and the full extraction pipeline mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "Within the `Content Filtering Pipeline`, the example function accesses `result.extracted_content` to manipulate the returned crawl data, showing how this attribute of `CrawlResult` is fundamental to processing the output of a crawl executed by `AsyncWebCrawler`.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly explains that extracted_content is used to process the output data from the crawler, which aligns with the ground truth's explanation that it stores the JSON-serialized results containing pricing features and related data.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet directly mentions using the 'arun' method of 'crawler', which corresponds to 'AsyncWebCrawler.arun()'. This is the main action point in the sample code where the URL is processed with exclusion parameters for links.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship correctly - that the arun method is used to process URLs with filtering options. While it doesn't detail the **kwargs mechanism mentioned in the ground truth, this is a minor implementation detail that doesn't affect the fundamental relationship being described.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "'AsyncWebCrawler' is the class context in which 'arun' method is called. It implicitly supports the functionalities demonstrated in the documentation by providing the 'arun' method used in the code sample.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncWebCrawler class handles the link filtering functionality through its arun method, which matches the ground truth's core meaning. While it doesn't detail the specific filtering parameters, it correctly identifies the class-method relationship and implicit support for the documented functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' likely uses an 'AsyncCrawlerStrategy' given the structure and naming conventions, providing a strategy-oriented approach for handling various crawling configurations, which indirectly supports the URL processing with options specified in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy provides the structural foundation for handling crawling configurations, which aligns with the ground truth's explanation of how it enables link filtering functionality through its abstract methods and kwargs parameter support.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates how to use the `AsyncWebCrawler` class with different browser types such as 'firefox', 'webkit', and 'chromium'. This is shown in the provided code examples where `AsyncWebCrawler` is instantiated with different initialization parameters.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of browser selection in AsyncWebCrawler through different initialization parameters, which aligns with the ground truth's explanation that this is implemented via the crawler_strategy parameter and PlaywrightCrawlerStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` of the `AsyncWebCrawler` class is used in the code examples provided in the documentation snippet. While the method itself is not explicitly discussed, the examples demonstrate its usage for crawling a URL.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() usage but misses its core functionality of enabling browser selection through crawler_strategy, which is central to the ground truth's explanation",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` utilizes a strategy pattern, with `AsyncPlaywrightCrawlerStrategy` as the default. `AsyncCrawlerStrategy` is the base class for `AsyncPlaywrightCrawlerStrategy`, forming an underpinning part of the broader strategy system used by `AsyncWebCrawler`.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture that AsyncCrawlerStrategy is a base interface/contract for different browser implementations. While the predicted version mentions strategy pattern specifically and the ground truth focuses more on browser types, they both convey the same core relationship of AsyncCrawlerStrategy defining the interface for different browser implementations.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The `AsyncWebCrawler` class uses `AsyncPlaywrightCrawlerStrategy` as its default strategy for crawling, as indicated in its `__init__` method. This is critical when using `AsyncWebCrawler` with browsers, aligning with the browser selection context provided in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description indicates AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as default, but misses the core relationship about how AsyncPlaywrightCrawlerStrategy itself implements browser selection through its start() method based on browser_type parameter.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly uses 'AsyncWebCrawler' by creating an instance with AsyncWebCrawler(verbose=True) within the example code. This demonstrates usage for asynchronous web crawling.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler usage but misses the key context manager functionality (__aenter__/__aexit__) that enables the 'async with' syntax, which is the core relationship shown in the documentation example",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "While 'arun()' isn't directly mentioned in the text explaining its use, the example code calls await crawler.arun(url=\"...\"), which demonstrates its use within the AsyncWebCrawler.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes arun() is used asynchronously within AsyncWebCrawler but fails to mention its core functionality of handling web crawling, content extraction, and error handling that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses AsyncWebCrawler in the example, indicating this class is used for managing the crawling task including the setup and teardown of the crawling environment.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in managing crawling tasks, but misses the crucial aspect of its ability to handle dynamic content through JavaScript execution and extraction strategies that is central to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method arun is explicitly used in the example to perform a single URL crawl, demonstrating how parameters like js_code and extraction_strategy are passed.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's basic usage for URL crawling and parameter passing, but misses the crucial aspect of handling dynamic web content through JavaScript execution and LLM extraction, which is a central focus of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The code example explicitly utilizes LLMExtractionStrategy by instantiating it and passing it as a parameter for extraction purposes.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that LLMExtractionStrategy is used for extraction but misses the crucial aspect that this is the actual implementation class that defines the core LLM-based extraction functionality, rather than just its usage",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly demonstrates the use of the `AsyncWebCrawler` class, as indicated by its instantiation in the example code `async with AsyncWebCrawler(...) as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class but doesn't capture the key relationship about how it accepts proxy and headers parameters through kwargs that are passed to AsyncPlaywrightCrawlerStrategy for magic mode features",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows a usage example where the method `arun()` is called on an instance of `AsyncWebCrawler`. Although `arun` is not explicitly discussed, it is demonstrated in the line `result = await crawler.arun(...)`.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a method of AsyncWebCrawler but fails to mention its core functionality of implementing proxy and magic mode features through crawler settings and anti-detection capabilities",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly describes the `JsonCssExtractionStrategy`, especially the details about the `schema` which is used to define extraction tasks. The mention of `baseSelector`, `fields`, and the nature of data types matches the purpose and functionality of the `JsonCssExtractionStrategy` class.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on explaining the schema structure but misses the key implementation aspect of how BeautifulSoup actually uses this schema to extract data via select()",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is a specific implementation of `ExtractionStrategy` as suggested by the text's focus on using CSS selectors for extraction. The document indirectly implies the use of `ExtractionStrategy` as the base class, leading us to understand the typical structure of the extraction strategy implementations.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and its implementations like JsonCssExtractionStrategy, and understands that it serves as a base class for extraction methods. While it doesn't mention the parallel processing capabilities explicitly, this omission doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method is explicitly mentioned and demonstrated in the documentation as the method being used to perform crawling and extract metadata.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's role in crawling but omits the key functionality of metadata extraction and processing through CrawlResult that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The `CrawlResult` class is explicitly used in the documentation example to hold the result of the `arun` method. The result (`metadata`) is accessed from the returned `CrawlResult` object, and shown in the example.",
    "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult class is used to hold metadata from the crawl operation, which aligns with the ground truth that metadata is stored in the metadata field of CrawlResult",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "Within the example usage, the `metadata` attribute of a `CrawlResult` instance is explicitly accessed to retrieve various pieces of metadata such as title, description, and others, as shown in the text.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that metadata is stored as a dictionary containing page metadata like title, description, etc. as demonstrated in the example. The predicted description captures the core relationship even though it doesn't list all specific fields.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code snippet in the documentation directly mentions the use of the 'arun' method to perform crawling with specified timing controls like 'page_timeout' and 'delay_before_return_html'. This indicates direct usage of 'AsyncWebCrawler.arun()' method.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles timing controls through its parameters, even though it doesn't explicitly mention the **kwargs implementation detail. The core relationship between the code and documentation is accurately captured.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While 'AsyncWebCrawler.arun()' is explicitly mentioned and demonstrated in the text, 'AsyncWebCrawler' itself is implicated as it is the class that provides the 'arun' method. This hints at the role that 'AsyncWebCrawler' plays in handling the timing control functionalities described in the snippet.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements timing control through its arun method which accepts page_timeout and delay_before_return_html parameters that are passed to the underlying crawler_strategy for managing page load timeouts and content capture delays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as handling timing control through arun, but fails to explicitly state the key timing parameters (page_timeout and delay_before_return_html) and their purposes that are central to the ground truth's description of the timing control relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides an example using the `AsyncWebCrawler` class directly in the code snippet to perform web crawling and extract data from a webpage. This is evident from the line `async with AsyncWebCrawler(verbose=True) as crawler:`.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions describe how the AsyncWebCrawler implements functionality used in the cryptocurrency price extraction example, with the predicted description capturing the core relationship between the code and documentation by noting the direct usage of AsyncWebCrawler for web crawling and data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet initializes a `JsonCssExtractionStrategy` with a schema to extract structured data using CSS selectors. This is indicated by the line `extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea that JsonCssExtractionStrategy uses CSS selectors with a schema, but misses the crucial context that it's specifically for extracting cryptocurrency data with specific fields (name, symbol, price) from Coinbase HTML structure",
      "error_type": "missing_critical_context"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `AsyncWebCrawler.arun()` is explicitly called in the provided snippet to perform the crawling operation: `result = await crawler.arun(...)`. This method is crucial for initiating the web retrieval and data extraction process.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called to perform crawling, but misses crucial aspects of its implementation including the extraction strategy application and structured data return that are core to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code accesses `result.extracted_content` to parse and print the extracted cryptocurrency prices with `crypto_prices = json.loads(result.extracted_content)`, demonstrating the use of this attribute from the `CrawlResult` class.",
    "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship - that extracted_content stores the scraped cryptocurrency data from Coinbase as a string that gets parsed into JSON. The predicted description accurately describes how the content is accessed and parsed, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides an example of how `JsonCssExtractionStrategy` can be used to extract structured data from a complex HTML document, specifically referencing its ability to handle nested elements typical of e-commerce site layouts.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly convey that JsonCssExtractionStrategy is used to extract structured data from HTML using CSS selectors, with the predicted description accurately emphasizing its application to complex nested structures like e-commerce sites.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The documentation demonstrates the use of JavaScript execution commands during the crawling process, which aligns with functionalities provided by `AsyncPlaywrightCrawlerStrategy`, specifically through its `crawl` method that enables JavaScript to be executed on a page.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JavaScript execution in the code, noting that it's handled through the crawl method, and accurately reflects that JavaScript commands can be executed on pages during crawling. While it doesn't mention all implementation details like page.evaluate(), it gets the main relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is used explicitly in the examples where JavaScript code is executed during crawling. This directly correlates to the method being responsible for executing such logic via the crawler strategy.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies that arun() handles JavaScript execution, but misses the key detail that it works by passing js_code through kwargs to crawler_strategy.crawl(). The prediction oversimplifies the implementation mechanism.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates setting up a proxy with the `AsyncWebCrawler` class during instantiation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that proxy setup occurs during AsyncWebCrawler instantiation, it fails to mention that this functionality is implemented through the crawler_strategy parameter rather than directly through the proxy parameter.",
      "error_type": "incomplete_implementation_detail"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Even though the method `arun()` is not explicitly mentioned, the snippet demonstrates using the method within the `AsyncWebCrawler` context, indicating its role in initiating a crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies arun() as a method for crawling, it misses the crucial aspect about proxy configuration handling that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of the CosineStrategy class for different extraction strategies. Each use case initializes this strategy with different parameters for specific content extraction tasks.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for different extraction tasks with configurable parameters, but fails to mention the core mechanism of cosine similarity matching that is central to how the strategy actually works",
      "error_type": "missing_core_mechanism"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes an example where the AsyncWebCrawler's arun method is used to invoke a crawl operation using the CosineStrategy. Although arun is not explicitly described in the text, its use is demonstrated in example code.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's role in executing crawls with CosineStrategy, but misses the crucial aspect of how arun implements the documented use cases through flexible extraction parameters and configurations.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy, which is highlighted in the documentation, inherits from ExtractionStrategy. This base class is not directly mentioned in the snippet but is crucial as it defines the interface and foundational behavior for all extraction strategies, including CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, but misses explaining the key purpose of ExtractionStrategy as a foundation for various content extraction use cases shown in the documentation",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly introduces the 'NlpSentenceChunking' class, explaining that it uses NLP models to split text into sentences, indicating that it implements sentence chunking based on natural language processing techniques.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that NlpSentenceChunking uses NLP techniques for sentence splitting, which aligns with the ground truth's explanation of using NLTK's sent_tokenize for sentence boundary detection.",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'NlpSentenceChunking' class, according to the naming convention and typical class structure, is likely a subclass of 'ChunkingStrategy'. This relationship is implicit as the documentation provides a class snippet that hints at inheritance by highlighting chunking ability based on its purpose.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that NlpSentenceChunking is a subclass of ChunkingStrategy and inherits its chunking functionality, which aligns with the ground truth's description of the inheritance relationship and required chunk() method implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The `AsyncWebCrawler` class is directly used in the provided code snippet (`async with AsyncWebCrawler(verbose=True) as crawler`). This indicates that the functionality explained in the documentation relies on this class to perform web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for web crawling, but misses the crucial dynamic content and session-based aspects with JavaScript injection and wait conditions that are core to its functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, `AsyncWebCrawler` is initialized with a strategy, which is implemented by `AsyncPlaywrightCrawlerStrategy`. This is critical to enabling the crawling operations described, particularly for handling JavaScript and dynamic content.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy as a strategy pattern, but misses the core functionality focus on session-based crawling and dynamic content handling that is central to the ground truth description.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is explicitly instantiated and utilized in the code snippet (`extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`). This strategy is tasked with extracting data from the crawled pages as described in the documentation.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately notes the instantiation and usage of JsonCssExtractionStrategy, but misses the crucial aspect of how it implements schema-based parsing and structured data extraction from GitHub's dynamic pages",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session` method of `AsyncPlaywrightCrawlerStrategy` is explicitly called in the code (`await crawler.crawler_strategy.kill_session(session_id)`). It stops a session which is necessary for handling dynamic pagination in the crawling process.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core purpose of the kill_session method - stopping/cleaning up a session used for dynamic content crawling, even though it doesn't explicitly mention closing page and context objects",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of 'fit_markdown' from CrawlResult as ideal for handling blog post and news article content, showing its utility within usage examples.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that fit_markdown is a property designed for handling blog posts and articles content, aligning with Best Practice #1 in the documentation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet shows how to handle media by using 'media' from CrawlResult to filter images based on their relevance score, suggesting its role in media processing.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the media property is used for filtering images based on relevance scores, which aligns with the ground truth's description of media handling functionality.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation explains how to select internal links related to content using 'links' from CrawlResult, demonstrating direct application within practical examples.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain how links from CrawlResult can be filtered for internal content links, capturing the same core relationship and practical usage",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet includes a direct call to 'arun', which is a method of 'AsyncWebCrawler', for cleaning content purposefully, seen in a usage example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method's content cleaning capability but misses other key functionalities mentioned in ground truth like media processing and link extraction",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly shows usage of the AsyncWebCrawler class in a sample script, highlighting how to instantiate and use it within an asynchronous context.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship - how the AsyncWebCrawler class is used with its asynchronous context manager pattern, matching the implementation in the code with __aenter__ and __aexit__ methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example script, the AsyncWebCrawler is employed to call the 'arun' method, indicating its role in executing the crawling task.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between AsyncWebCrawler and arun method, but misses crucial aspects about the method's functionality including content extraction, caching, and return of CrawlResult that are central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'arun' method is inferred to return a CrawlResult object, as indicated by the print statement within the documentation snippet that accesses the 'markdown' attribute, which is part of a CrawlResult.",
    "ground_truth_relationship": "The CrawlResult class defines all possible return values from the crawler.arun() method shown in the documentation, including the markdown property that's used in the example print statement.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that arun() returns a CrawlResult object containing the markdown property shown in the example. While it's less detailed than the ground truth, it conveys the same essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The 'result.markdown' access in the snippet suggests that the 'markdown' attribute is part of the returned CrawlResult instance, demonstrating direct access to the value stored in this attribute.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the markdown attribute is accessible from the CrawlResult instance returned by AsyncWebCrawler.arun() and contains webpage content",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet is explicitly using the class `AsyncWebCrawler` in the example provided to demonstrate basic configuration settings such as `headless`, `verbose`, and `sleep_on_close`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The prediction correctly identifies that AsyncWebCrawler uses these configuration settings, but misses that they are passed through kwargs to AsyncPlaywrightCrawlerStrategy rather than being directly used by AsyncWebCrawler itself",
      "error_type": "missing_core_mechanism"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` of `AsyncWebCrawler` is implicitly invoked in the example snippet to perform the crawling action (`result = await crawler.arun(url=\"https://example.com\")`).",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method being called, but misses the core relationship about how the configuration parameters are handled through the constructor rather than directly in arun()",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly referenced in the snippet, the class `AsyncWebCrawler` is initialized to use `AsyncPlaywrightCrawlerStrategy` as its default strategy, which becomes part of its `crawl` method calls inherently. This connection is necessary for fully understanding the implementation of asynchronous operations within `AsyncWebCrawler`.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on AsyncWebCrawler's connection to AsyncPlaywrightCrawlerStrategy, while the ground truth specifically describes how AsyncPlaywrightCrawlerStrategy directly implements the documented browser configuration options. While related, they describe different aspects of the relationship.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly refers to using 'crawler.arun()' which directly indicates the usage of the 'AsyncWebCrawler' class as it carries out the 'arun' method for executing the crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the arun method of AsyncWebCrawler, it misses the core functionality of content cleaning and noise removal that is central to the ground truth relationship. The prediction focuses only on the method existence rather than its cleaning functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet's code example demonstrates calling 'crawler.arun()' with multiple relevant parameters like 'url', 'word_count_threshold', and 'remove_overlay_elements', directly involving the 'arun' method to implement the functionality described for cleaning content.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the arun() method implements content cleaning functionality through parameters that control how content is filtered and processed during crawling. The predicted description accurately captures this core relationship even if it doesn't mention every specific parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation snippet refers to 'print(result.cleaned_html)', which implies the 'CrawlResult' model's 'cleaned_html' attribute generated by the 'arun()' method's return, contributing to showing clean HTML.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies cleaned_html as an output attribute, but misses the crucial aspect that it stores sanitized HTML after specific cleaning steps (basic cleaning, content relevance, layout analysis)",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet includes 'print(result.markdown)', which implicitly traces to 'CrawlResult.markdown', showcasing its role in providing a markdown version of the cleaned content, derived from the crawl result returned by the 'arun' method.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the markdown property provides a cleaned text version of the crawled content from the crawl result, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text explicitly demonstrates the usage of the 'arun' method of 'AsyncWebCrawler' to execute JavaScript on a webpage before crawling it. The method name 'arun' is directly mentioned and used in the example code provided.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun as being used to execute JavaScript, but incorrectly states it's 'explicitly demonstrated' in the code when the JavaScript functionality is actually implemented through kwargs and crawler_strategy.crawl()",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the text, 'AsyncWebCrawler' utilizes 'AsyncCrawlerStrategy', and its default implementation is 'AsyncPlaywrightCrawlerStrategy'. This is inferred from the context as 'AsyncPlaywrightCrawlerStrategy' implements the async strategy pattern that supports JavaScript execution, a functionality showcased by the 'arun' method in the example.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JavaScript execution capability but incorrectly attributes it to an 'arun' method and makes assumptions about class relationships not shown in the code. The ground truth correctly states that JS execution is handled through the crawl() method with js_code parameter.",
      "error_type": "method_misattribution"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'LLMExtractionStrategy' class is explicitly mentioned in the documentation snippet as being used to support multiple LLM providers for extraction, demonstrated through examples using different provider names and API keys.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between LLMExtractionStrategy and multiple LLM providers, matching the ground truth's explanation of provider flexibility through constructor parameters. Both descriptions emphasize the class's ability to work with different LLM services.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The text snippet mentions instantiating an 'AsyncWebCrawler' with custom headers, directly relating to the class 'AsyncWebCrawler'. This demonstrates the usage of 'AsyncWebCrawler' for initializing a crawler with specific headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts headers, but misses the key implementation detail that this happens through kwargs being passed to AsyncPlaywrightCrawlerStrategy, which is crucial to understanding how the header support actually works",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code snippet, the 'arun' method of an 'AsyncWebCrawler' instance is called to perform a crawling operation. This suggests that the method is part of the public interface of the 'AsyncWebCrawler' class for executing the crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is a method of AsyncWebCrawler used for crawling, but misses the crucial functionality about custom header support which is the main focus of the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions and demonstrates the use of `AsyncWebCrawler` in the `main` function, showing how to use it with the `extract_with_retry` function for handling retries during extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used with error handling functionality, though it focuses more on the usage example while the ground truth describes the internal implementation. The core relationship between the code and documentation is accurately captured - both describe error handling capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet directly refers to `LLMExtractionStrategy` as a parameter when creating an instance, highlighting its role in the strategy pattern used for extracting content through LLM APIs.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy as a component but focuses only on its instantiation, missing the core error handling and parallel processing functionality described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the retry function `extract_with_retry`, `arun()` method of `AsyncWebCrawler` is called to execute the web crawling and extraction process based on the given URL and strategy, demonstrating its use implicitly.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() is used for crawling/extraction but misses the key focus of the ground truth - its comprehensive error handling implementation, which is the main point being illustrated in relation to the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation explicitly mentions the 'kill_session' method as a way to clean up session resources in the 'Resource Management' section of the provided snippet.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session is used for resource cleanup as shown in the documentation, which aligns with the ground truth's explanation of resource management and memory leak prevention",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the 'AsyncWebCrawler' is explicitly called in examples for managing sessions in the 'State Management' section. This indicates its use in executing crawl operations with specific session IDs.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is used for session management and crawling with session IDs, which aligns with the ground truth's explanation of how arun() enables stateful crawling across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' implicitly relies on 'AsyncPlaywrightCrawlerStrategy', as it is used as a default strategy according to its class definition. The snippet does not directly mention AsyncPlaywrightCrawlerStrategy, but it is integral given its involvement in session handling via 'kill_session'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session management through its sessions dictionary and kill_session method, directly supporting the documented session best practices for naming, resource cleanup, and state management across multiple page requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description incorrectly suggests the relationship is implicit and focuses on AsyncWebCrawler, while the ground truth correctly identifies that AsyncPlaywrightCrawlerStrategy directly implements session management functionality through its sessions dictionary and kill_session method.",
      "error_type": "incorrect_focus_and_relationship_type"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is explicitly called in the provided code snippet. It is used to perform a web crawling operation asynchronously on a given URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is an asynchronous web crawling method, but misses its key functionality of converting HTML to markdown format, which is a crucial aspect mentioned in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The `markdown` attribute of the `CrawlResult` class is accessed after calling `arun()`, suggesting that it holds the markdown version of the crawled HTML. This is inferred from the example where `result.markdown` is printed.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that markdown is an attribute of the CrawlResult that contains the markdown-formatted version of the crawled content, accessed after crawling",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code snippet from the documentation uses 'arun()' method of the 'AsyncWebCrawler' class to initiate crawling on a URL (e.g., 'https://example.com').",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling URLs, but misses the crucial aspect that it powers the link analysis features and returns categorized links as described in the documentation",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation demonstrates accessing the 'links' attribute of the result from 'arun()', using it to classify and analyze links such as 'internal' and 'external'.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the links property contains categorized link data (internal/external) from crawl results with associated details. While it doesn't list all link types and details mentioned in the ground truth, it gets the fundamental relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions the usage of 'AsyncWebCrawler' in the example function 'crawl_with_advanced_config'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used in the example, but it understates the relationship by not acknowledging that the class fully implements all the configuration options shown in the documentation example.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is called on an instance of 'AsyncWebCrawler' to perform the crawl operation as demonstrated in the provided code.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but fails to acknowledge its core implementation aspects including error handling, caching, and content extraction that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' uses 'AsyncPlaywrightCrawlerStrategy' implicitly since it's the default strategy used when no strategy is provided during the initialization of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but oversimplifies it as just a default strategy relationship while missing the main point about the comprehensive feature set implementation shown in the documentation example.",
      "error_type": "oversimplification"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is explicitly instantiated in the example shown in the documentation snippet, which instructs how to enable verbose logging when using this class.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the verbose parameter's presence but focuses only on instantiation example, missing the core purpose of enabling detailed logging messages throughout code execution as described in ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' of the 'AsyncWebCrawler' class is used in the example to perform a web crawl. Although not explicitly discussed, its usage is demonstrated in the snippet showing a practical implementation of the method.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for web crawling but misses the core relationship about verbose logging functionality that is the focus of the documentation",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly discusses the 'Cosine Strategy' and its methodology. The implementation of this strategy is found in the 'CosineStrategy' class, which performs tasks such as vectorizing text and clustering based on semantic similarity, as described in the documentation steps.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the Cosine Strategy and its use of text vectorization and clustering, but fails to explicitly connect the implementation to all 5 documented steps that are clearly mapped in the ground truth explanation.",
      "error_type": "incomplete_mapping"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' class extends the 'ExtractionStrategy'. Although not mentioned in the documentation, this relationship is necessary because 'CosineStrategy' is a specific implementation of an extraction strategy, and the base class likely provides essential methods and properties that are overridden or used.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that ExtractionStrategy is the base class that provides the foundational structure for implementing extraction strategies. The predicted description correctly identifies the inheritance relationship and purpose, even if it doesn't detail all the specific methods.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation explicitly mentions `FixedLengthWordChunking` as the class used for chunking text based on a fixed number of words. The description and usage example directly correlate to how this class is documented to function.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of splitting text into fixed-length word chunks, which aligns with the ground truth's implementation description",
      "error_type": "none"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Since `FixedLengthWordChunking` is a class for chunking strategies, and given the naming convention, it likely extends from a base `ChunkingStrategy` class as part of polymorphic design. This assumption is reinforced by the presence of `ChunkingStrategy` in the available artifacts list.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that FixedLengthWordChunking extends/inherits from the ChunkingStrategy base class as part of a polymorphic design, which aligns with the ground truth's description of ChunkingStrategy serving as the base interface for FixedLengthWordChunking.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet demonstrates the use of a schema for CSS-based extraction, which directly correlates with the `JsonCssExtractionStrategy` class. This class is responsible for extracting web content using schemas, as evidenced by its parameters like `baseSelector` and `fields`.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the JsonCssExtractionStrategy class and the schema-based extraction functionality, even though it doesn't mention specifics about BeautifulSoup's select() method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the use of the `LLMExtractionStrategy` class to extract structured data from the OpenAI pricing page. It details parameters like 'provider', 'api_token', 'schema', 'extraction_type', and 'instruction' used within the strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship that LLMExtractionStrategy is used for structured data extraction using provider-based LLM models, and correctly identifies the key parameters used. While it may not detail the Pydantic schema implementation, it captures the core functionality and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation specifies the creation of an `AsyncWebCrawler` instance, which is used to execute the `arun` function, thus employing the `AsyncWebCrawler` class directly in the example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class is used for crawling via the arun function, but misses significant functionality mentioned in the ground truth like processing HTML content and extracting structured data using strategies",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage of `AsyncWebCrawler`, the `arun` method is invoked to crawl the specified OpenAI pricing URL, this is shown as part of the example implementation.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler.arun() for crawling the OpenAI URL, but omits crucial aspects about extraction strategy, result processing, and error handling that are central to the relationship shown in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation mentions 'Crawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data...' indicating AsyncWebCrawler is designed to handle LLM-based data extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of AsyncWebCrawler supporting LLM-based data extraction. The predicted description correctly identifies it as a tool for extracting structured data using LLMs, which aligns with the ground truth's explanation of its LLM extraction support through the extraction_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'LLMExtractionStrategy' as a component used with AsyncWebCrawler, likely indicating its role in the logic for extracting information.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy's basic role but misses key aspects of its asynchronous nature and structured data extraction capabilities that are central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While not explicitly mentioned, LLMExtractionStrategy is a specialized implementation of an extraction strategy, extending the base class ExtractionStrategy, indicating a hierarchy of strategy implementations possibly used in extraction tasks.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies ExtractionStrategy as a base class with hierarchy, but misses the crucial parallel processing aspect and the core purpose of extracting structured data from web pages",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows the use of 'AsyncWebCrawler', starting the asynchronous web crawling session with the 'async with' statement and configurations like 'verbose=True'.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions AsyncWebCrawler and its async usage pattern, but misses the crucial aspects of dynamic content crawling, session-based browsing, and custom extraction strategies that are central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The '.arun()' method of 'AsyncWebCrawler' is invoked within the crawl_dynamic_content() function as part of the session management process for crawling GitHub commit pages with parameters like 'url', 'session_id', and 'extraction_strategy'.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that AsyncWebCrawler.arun() is used within the dynamic content crawling process with session management, which aligns with the ground truth's description of it being the core method for dynamic web crawling with session features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of 'JsonCssExtractionStrategy' using a schema for extracting commit details like 'title' from HTML elements on the page.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function of JsonCssExtractionStrategy in extracting data using a schema, but misses the crucial BeautifulSoup parsing implementation and broader relationship with the extraction workflow shown in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "'kill_session()' is used to clean up the crawling session at the end of the process. It's invoked on the 'AsyncWebCrawler.crawler_strategy' within the code.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that kill_session() is used for cleanup at the end of the crawling process, which aligns with the ground truth's explanation of cleaning up browser resources. The minor details about closing page/context objects don't change the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncPlaywrightCrawlerStrategy', which contains the 'kill_session' method mentioned, extends 'AsyncCrawlerStrategy', serving as a base class defining the interface for all crawler strategies.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable dynamic session-based crawling operations demonstrated in the documentation example, particularly through its crawl method which is used to navigate through GitHub's paginated commit history.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class defining an interface, but incorrectly states it specifically extends to AsyncPlaywrightCrawlerStrategy and focuses on the kill_session method which isn't shown in the base class definition. The ground truth more accurately describes its role in enabling dynamic session-based crawling.",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "As a concrete implementation of 'AsyncCrawlerStrategy', 'AsyncPlaywrightCrawlerStrategy' features methods used and extended by 'AsyncWebCrawler', including session control as shown.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as implementing AsyncCrawlerStrategy and being used by AsyncWebCrawler, but it misses the crucial focus on session management and dynamic content handling that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as a tool for extracting highly structured data from complex web pages. It outlines advanced usage tips for this strategy.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as a tool for extracting structured data, but misses the key ground truth relationship about how the code implementation directly supports the documented incremental/gradual schema development approach.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly describes using JsonCssExtractionStrategy for extracting patterned information from web pages. This is demonstrated with example code that directly instantiates and utilizes this class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for pattern-based extraction from web pages, which aligns with the ground truth's explanation of how it implements pattern-based scraping using CSS selectors and schema definitions",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code example shows the use of the `arun` method from an object `crawler`, which indicates the use of `AsyncWebCrawler`. It indirectly implies that AsyncWebCrawler utilizes the JsonCssExtractionStrategy as a strategy for extracting data from the given URL.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based extraction through its aprocess_html method, which specifically checks for JsonCssExtractionStrategy instances and processes them using the schema format shown in the documentation to extract structured data from repeating HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler uses JsonCssExtractionStrategy for data extraction. While it doesn't detail the schema format and pattern-based extraction specifics shown in the ground truth, it accurately describes the core relationship between these components.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet instructs using the `arun` method while demonstrating the extraction strategy, implying its role in submitting requests and applying extraction strategies like JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that the arun method accepts and applies extraction strategies like JsonCssExtractionStrategy, which aligns with the ground truth's explanation of how arun() processes web pages using extraction strategies to extract structured data.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the document, `extracted_content` is accessed from the result of `arun`, illustrating usage of CrawlResult attributes. This indicates linking between extraction strategy results and CrawlResult's structure during retrieval.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the result of arun, but misses the core purpose of storing JSON-formatted pattern matches based on the schema definition",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet clearly demonstrates the usage of the 'AsyncWebCrawler' class as the context manager that facilitates the crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as a context manager but misses the core functionality about user simulation that is central to the ground truth relationship. While not wrong, it omits the key feature being documented.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of the 'AsyncWebCrawler' is explicitly shown being used in the documentation to perform a crawling operation with user simulation parameters.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method is shown being used with user simulation parameters in the documentation, which aligns with the ground truth that these behaviors are implemented through kwargs parameters.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, 'AsyncPlaywrightCrawlerStrategy' is implicitly involved as it is a possible default strategy used by 'AsyncWebCrawler' for actual crawling operations, particularly under user simulation settings.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the connection between AsyncPlaywrightCrawlerStrategy and AsyncWebCrawler for user simulation, but incorrectly describes it as 'implicit' when the code shows it's an explicit implementation in the class. However, it correctly notes the user simulation functionality.",
      "error_type": "incorrect_relationship_characterization"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' extends 'AsyncCrawlerStrategy', implicitly adding methods for crawling under different settings mentioned in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship but misses the key point about the abstract methods being used specifically for user simulation features. Instead, it just vaguely mentions 'different settings'.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'CosineStrategy' as the strategy used for semantic filtering and clustering content. The parameters 'semantic_filter', 'word_count_threshold', and 'sim_threshold' are directly used in the snippet, indicating their role in configuring the strategy.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that CosineStrategy uses semantic filtering and clustering with configurable parameters like semantic_filter, word_count_threshold, and sim_threshold as shown in the usage example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the provided code snippet, 'AsyncWebCrawler' is instantiated within an 'async with' statement, indicating its use in an asynchronous context to perform web crawling operations. While not the primary focus, it is essential for executing the strategy provided by 'CosineStrategy'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's async context manager usage, but misses the crucial point that it implements the exact functionality shown in the documentation example with arun() and extraction strategy support",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows the use of 'arun()' method within 'AsyncWebCrawler'. It implicitly indicates this method is responsible for executing the crawling with the specified 'CosineStrategy' and the URL 'https://example.com/reviews'.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the key relationship - that arun() is an asynchronous crawling method that takes an extraction strategy and URL as input. While it omits some implementation details like CrawlResult and thresholds, it correctly conveys the core functionality shown in both code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses 'result.extracted_content' indicating a reliance on 'CrawlResult' structure to retrieve the content extracted by the 'CosineStrategy'. It shows how outputs of the crawling process are structured.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that extracted_content is used to retrieve content after crawling, which aligns with the ground truth's explanation of it holding gathered text content.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet in the documentation directly mentions 'AsyncWebCrawler', which is an instantiated class. The snippet uses 'AsyncWebCrawler' in an asynchronous context for web crawling with an option for verbosity.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship showing AsyncWebCrawler is used asynchronously for web crawling, which matches the ground truth's description of using AsyncWebCrawler through its async context manager and arun() method. While the predicted version is less detailed, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function 'arun' is explicitly invoked in the snippet, where a URL is passed to initiate a web crawl. This demonstrates direct usage of 'arun' as part of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() being used for web crawling with a URL parameter, but fails to capture the comprehensive functionality including HTML extraction, caching, processing, and return of CrawlResult that the ground truth describes.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation prints 'result.markdown', implying 'result' returns an instance with 'markdown' content. This suggests a reliance on 'CrawlResult' where 'markdown' is an attribute.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that result.markdown is an attribute containing content from the crawl, which aligns with the ground truth's explanation of markdown storing extracted text content",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "While not explicitly named, the documentation implies that 'arun' returns a 'CrawlResult' instance. This is inferred from accessing 'markdown' in the 'result'.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the CrawlResult class is used to store the output of the crawler's arun method, which includes markdown content as shown in the documentation example.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The document mentions different strategies like LLM and Cosine for extraction, which are types of 'ExtractionStrategy'. This suggests that 'ExtractionStrategy' is used as a base class/interface for these specific strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class for different strategies, but misses the crucial aspect of shared parallel processing functionality and the abstract extract() method implementation pattern described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document mentions using 'LLM for complex interpretation', which directly corresponds to the 'LLMExtractionStrategy' class implementing such functionality.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM's use for complex interpretation, while the ground truth describes error handling and retry logic implementation. These are completely different aspects of the code.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The document recommends 'Cosine for content relevance', indicating that the 'CosineStrategy' explicitly addresses this recommendation by implementing relevant functionalities.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship between CosineStrategy and the documentation's recommendation of using 'Cosine for content relevance'. While the ground truth provides more implementation details, the predicted text correctly identifies the essential connection.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example in the document demonstrates the usage of 'arun()' method of 'AsyncWebCrawler' to perform crawling and handle potential errors during content extraction.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the error handling aspect but misses the important caching and strategy selection components mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The document's example uses 'result.error_message' to check and report errors if crawling fails, indicating that 'error_message' is a critical attribute for error handling.",
    "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that error_message is used for error handling and reporting when crawling fails, which aligns with the ground truth's explanation of how it enables error handling best practices",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The document implies the usage of 'result.extracted_content' to process or load data post-extraction, signifying its role as a primary attribute for handling extracted information.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that extracted_content is used to store and handle extracted data, which aligns with the ground truth's explanation of it being used to store crawled data that can be parsed as JSON after successful extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly discusses the 'advanced usage of JsonCssExtractionStrategy', which directly refers to this class designed for complex HTML structure extraction.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the class's purpose for complex HTML extraction but omits the crucial implementation details about recursive processing and schema-based selectors that are core to its functionality",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends from ExtractionStrategy. This implicit connection stems from class hierarchy, as JsonCssExtractionStrategy inherits base features for extraction strategies from ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy. While it's more concise than the ground truth, it captures the core architectural relationship that enables the functionality described in the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation directly references `AsyncWebCrawler` as the context manager used for initiating the session.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's context manager role but misses the core session management functionality through session_id that is central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun()` is explicitly used in the code example to perform actions within a session using the `session_id`.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used with sessions but misses the key implementation details about session_id being passed through kwargs and stored in crawl_result for state persistence.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Though not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the default crawler strategy employed by `AsyncWebCrawler` as inferred from the constructor logic not shown in the snippet. It relates as the underlying strategy implementation.",
    "ground_truth_relationship": "The code implements session management through the sessions dictionary in AsyncPlaywrightCrawlerStrategy, which stores browser contexts and pages mapped to session_ids as demonstrated in the documentation's session usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncPlaywrightCrawlerStrategy as being used by AsyncWebCrawler, but misses the main point about session management functionality which is the core relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session()` method is explicitly invoked in the example to terminate the session identified by `session_id`.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session() is used to terminate sessions, but misses the important implementation details about closing browser page/context objects and cleanup of the sessions dictionary",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates using the 'AsyncWebCrawler' class when discussing proxy rotation, directly instantiating this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class is used in the proxy rotation example and accurately notes its usage pattern with context management. While it doesn't mention all implementation details, it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the documentation snippet, 'arun' is explicitly called on the 'crawler' object, which is an instance of 'AsyncWebCrawler'. This indicates its usage in executing a crawl with the specified URL.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that arun() is used on the crawler object to execute crawls with URLs, which aligns with the ground truth's description of arun() as implementing core crawling functionality with URL parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation provides a code snippet using 'crawler.arun' to execute a crawl operation. This directly relates to the 'AsyncWebCrawler' class, which includes the 'arun' method as part of its interface.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot capabilities through its arun() method, which accepts a screenshot boolean parameter and stores the captured image data in the screenshot_data variable, which is then included in the returned CrawlResult object.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and its presence in AsyncWebCrawler, but fails to mention the core screenshot functionality and how screenshot data is handled, which is the main focus of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun' is explicitly referenced in the example within the 'browser-config.md' documentation. This indicates the method's role in initiating the crawling process with specific arguments like 'screenshot=True'.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() method handles screenshot functionality, even though it focuses more on the documentation example rather than implementation details. The core relationship of arun() enabling screenshot capture is accurately conveyed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The result of the 'arun' method is checked for the presence of a 'screenshot', which is a base64 encoded image. This refers to the 'CrawlResult.screenshot' attribute, implicitly demonstrating its use in handling screenshot data.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the screenshot property contains base64-encoded image data that can be decoded into a PNG file. The predicted description captures the core functionality even though it approaches it from a usage perspective.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly references the AsyncWebCrawler class in the example function integrated_js_and_wait_crawl. It is used as the context manager in an asynchronous call, demonstrating the instantiation and its role in managing crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage as a context manager for crawling, but misses crucial aspects about its implementation of integrated JavaScript execution and extraction strategy handling that are central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example function in the text, the async method arun of the AsyncWebCrawler instance is explicitly invoked. The arun method is used iteratively for each page to manage the crawling process alongside setting various options like session_id and js_code.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun is used for crawling with various options, but misses the core functionality of integrated JavaScript execution and waiting through js_code parameter that is central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation includes explicit configuration of the JsonCssExtractionStrategy in the example. This extraction strategy is set up with a schema to extract commits, indicating its role in defining how data is retrieved from the pages.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that JsonCssExtractionStrategy processes HTML content based on a defined schema structure for extracting data, specifically in the context of commit information extraction. The predicted description mentions the core configuration aspect while the ground truth elaborates on the implementation details, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The text snippet explicitly invokes the kill_session method to close the session with the identifier 'integrated_session'. This method plays a crucial role in session management, as described in the documentation.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core purpose of kill_session as a cleanup method that closes/terminates session resources. The predicted description is more general but accurately reflects the same relationship described in the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly mentions 'AsyncWebCrawler' in the example code as the class used to perform web crawling.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only mentions AsyncWebCrawler class being used for web crawling, but misses the crucial caching functionality aspect that is central to the ground truth relationship. While it correctly identifies the class, it omits the key feature being discussed.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation includes an example calling 'arun()' method to demonstrate crawling with and without bypassing the cache.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() handles caching, but misses the crucial implementation detail that it checks for cached content using async_db_manager.aget_cached_url() when bypass_cache is False. It describes only the example usage without the underlying mechanism.",
      "error_type": "omission_of_key_mechanism"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "In the example, the result returned from 'arun()' method is used, which is of 'CrawlResult' type according to the method's return type.",
    "ground_truth_relationship": "The CrawlResult class contains key fields like 'markdown' and 'success' that store the cached crawl results mentioned in the documentation example's demonstration of caching behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that CrawlResult is used as the return type from arun(), but misses the key relationship about how CrawlResult stores cached crawl data through fields like markdown and success, which is central to the caching behavior described in the documentation",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation example uses 'result1.markdown' and 'result2.markdown' which accesses the 'markdown' field in 'CrawlResult'.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the markdown field is accessed from CrawlResult objects in the example code, which aligns with the ground truth's explanation of CrawlResult.markdown storing webpage content.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, 'AsyncPlaywrightCrawlerStrategy' is the default crawler strategy used unless otherwise specified.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements caching through the use_cached_html parameter, which when true stores HTML content in a local file and retrieves it on subsequent requests, matching the documentation's description of caching crawl results for faster subsequent crawls.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncPlaywrightCrawlerStrategy being a default strategy, while the ground truth explains its caching functionality through use_cached_html parameter. These are completely different aspects of the class.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet describes defining a schema for CSS-based data extraction with references to JSON-like structures, including 'nested' and 'list' types, similar to what the JsonCssExtractionStrategy class is designed to interpret and use for extracting HTML content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship between the JsonCssExtractionStrategy class and the schema documentation, correctly identifying that it handles JSON-structured schemas with CSS selectors for HTML data extraction",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet directly mentions the `arun` method in a Python code example, indicating its usage for retrieving raw HTML of a webpage.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun is used for retrieving HTML content, but misses the key functionality of storing it in CrawlResult.html and omits that it specifically gets unmodified webpage content.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The text snippet shows accessing the `html` attribute from the result of `arun()`, demonstrating its role in storing and returning the raw HTML obtained through the crawling process.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core purpose and functionality of the html attribute - storing and providing access to the raw HTML content obtained through crawling",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The 'E-commerce Scraping' use case explicitly mentions a 'CSS Strategy' with a JSON-based schema definition, which aligns with the 'JsonCssExtractionStrategy'. The strategy is designed to extract elements from a webpage using CSS selectors specified in a schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between JsonCssExtractionStrategy and the e-commerce scraping use case, highlighting the CSS selector-based extraction using a JSON schema structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The 'News Article Extraction' use case mentions 'LLM Strategy', which corresponds to the 'LLMExtractionStrategy'. This strategy utilizes a language model to extract content from web pages, specifically using schemas provided by model-based APIs.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy uses language models for content extraction and supports schema-based extraction, which aligns with the ground truth's explanation of how the class implements schema-based article extraction with provider and schema parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The 'Content Analysis' use case describes the usage of a 'Cosine Strategy' for topic analysis. This aligns with the 'CosineStrategy' class, which employs cosine similarity techniques for document analysis and clustering.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy as a content analysis tool using cosine similarity, which matches the ground truth's explanation of its use for text clustering and analysis with semantic filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation mentions the use of the AsyncWebCrawler class in the example code, specifically showing how it is instantiated and used within an asynchronous context manager to perform web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler usage but misses the crucial aspect of custom execution hooks functionality through crawler_strategy, which is the main focus of the ground truth relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the 'arun' method of the 'AsyncWebCrawler', indicating its usage in executing the crawling logic with specific parameters.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the execution method but misses the crucial aspect of how it enables custom hooks during the crawling process through crawler_strategy",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' is implied as part of the execution strategy for the 'AsyncWebCrawler'. The example suggests using 'set_hook' and 'kill_session', methods characteristic of this strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between AsyncPlaywrightCrawlerStrategy and AsyncWebCrawler, and mentions set_hook and kill_session methods. However, it fails to clearly explain the core purpose of the hooks system for executing custom functions at specific crawling stages, which is the main focus of the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The 'kill_session' method is explicitly mentioned in the documentation as being used to terminate a session, showing the utility of ending a browsing session within a hook implementation.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies kill_session's role in terminating sessions, but incorrectly frames it in the context of hook implementation when the ground truth shows it's used for general cleanup after crawling completes",
      "error_type": "context_misunderstanding"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The 'set_hook' method is explicitly used in the documentation to set the 'on_execution_started' hook, demonstrating its role in facilitating custom hook-based operations.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that set_hook is used to register custom hooks (specifically mentioning on_execution_started) which matches the ground truth's explanation of storing hook callbacks for execution during crawling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides a code example demonstrating the use of the 'arun' method. The example showcases how different extraction strategies can be applied using 'arun', as mentioned in the snippet.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles extraction strategies, but misses the key point about combining multiple strategies sequentially as shown in the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The variable 'css_strategy' used in the snippet suggests the implementation of a CSS-based extraction strategy, typically represented by a class such as JsonCssExtractionStrategy. Although not explicitly named, it aligns with the context of 'css_strategy'.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements a CSS-based extraction approach, which aligns with the ground truth's explanation of it being part of a combinable strategy using CSS selectors for data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The presence of 'llm_strategy' in the example alludes to a large language model-based extraction component such as LLMExtractionStrategy. Though not explicitly referenced, its role is inferred as an instantiation of a strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy as involving large language models, but misses the key point about its role in combined crawling strategies and its specific functionality for semantic analysis after CSS extraction.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the use of `LLMExtractionStrategy`. This is evident from the code example where `LLMExtractionStrategy` is instantiated with parameters such as `provider`, `api_token`, and others to create an extraction strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of LLMExtractionStrategy with key parameters, but misses crucial aspects about its functionality like chunking, rate limiting, and parallel processing for different LLM providers that are emphasized in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `arun` is shown in the snippet being called on a `crawler` object, which is likely an instance of a class implementing or encapsulating `AsyncWebCrawler`. It uses `extraction_strategy` to perform a crawl, as shown in the example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun is called on a crawler object and uses extraction_strategy for crawling, which aligns with the ground truth's explanation of AsyncWebCrawler.arun() implementing structured data extraction via extraction_strategy parameter.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the example, the `extracted_content` attribute is accessed from the result of an `arun` call, indicating where the structured data outcome of the LLM extraction is held.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the result of arun() and contains the structured data output. While it doesn't mention Pydantic models specifically, it captures the core relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions using `AsyncWebCrawler` in the example function `wait_for_parameter_crawl()`, indicating its role in managing the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's involvement but doesn't capture the key aspect of how it supports the wait_for parameter through its arun method and crawler_strategy mechanism",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is called within the example function with a specific URL and extraction strategy, showing its role in processing crawl requests.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as handling crawl requests but misses the crucial relationship with the wait_for parameter functionality described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is instantiated and provided as the `extraction_strategy` argument in the example, illustrating its role in extracting content based on CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in extracting content using CSS selectors, but misses the crucial aspect of schema-based extraction and BeautifulSoup parsing implementation details that are central to the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The `kill_session()` method is explicitly called at the end of the example session to terminate the session using a specified `session_id`.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that kill_session is called to terminate the session, but misses the key detail about cleaning up browser resources (closing page and context objects) which is an important part of the resource cleanup process described in the ground truth.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet in the documentation calls `crawler.arun()` which directly refers to the `AsyncWebCrawler.arun()` method for executing the crawl process.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between the documentation and arun() method, but misses the core focus on error handling and status checking that is central to the ground truth relationship",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation checks `result.success` to verify the crawl's success, indicating that `result` is an instance of `CrawlResult` and `success` is an attribute accessed in the code.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the success boolean property is used to check if a crawl operation completed successfully. While it focuses more on the code structure and the predicted mentions CrawlResult explicitly, both convey the same fundamental purpose of error handling verification.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet references `result.error_message` to display error details if the crawl fails, linking directly to this attribute within `CrawlResult`.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that error_message is used to store and display error details when a crawl operation fails",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The documentation uses `result.status_code` to display the HTTP status code of the crawl, linked to this attribute in `CrawlResult`.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the status_code attribute is used to show HTTP status codes from crawl results, matching how it's used in the documentation example",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "While not directly mentioned, `arun()` returns a `CrawlResult` object which is implicitly used when accessing `success`, `error_message`, and `status_code`.",
    "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult provides the fields used in error handling, even though it frames it more implicitly through arun()'s return value. The core relationship between the class fields and error handling functionality is preserved.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly demonstrates the instantiation of the `AsyncWebCrawler` class, using it in an asynchronous context manager to crawl a protected site.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler being used in an async context manager, but misses the key point that the class implements protected site crawling functionality with specific features like magic mode and overlay removal that are shown in the documentation example.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` of the `AsyncWebCrawler` class is explicitly called to perform crawling, with parameters specified in the code snippet for handling protected sites.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() method usage but misses significant functionality like caching, error handling, and customizable parameters that are core to enabling protected site crawling",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, `AsyncPlaywrightCrawlerStrategy` is the likely default strategy for `AsyncWebCrawler` operations, implicitly handling the strategy mechanics for crawling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as being connected to AsyncWebCrawler operations, but misses the crucial aspect that it specifically implements protected site crawling functionality using headless browser automation with features like popup removal and timeouts.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly showcases an example usage of the method `arun` within the `AsyncWebCrawler` class. The `arun` method is invoked to perform a crawl with specific parameters like `process_iframes` and `remove_overlay_elements`.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method being used for crawling, but fails to mention that iframe processing is implemented through kwargs being passed to crawler_strategy.crawl(), which is a key part of the functionality.",
      "error_type": "omission_of_key_mechanism"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While the `AsyncWebCrawler` class isn't directly mentioned, its method `arun()` is explicitly referenced within the snippet. This implies the use of the `AsyncWebCrawler` class implicitly through its method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements iframe processing through its arun() method which accepts process_iframes and remove_overlay_elements parameters to control iframe content extraction and overlay handling during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description acknowledges the connection between AsyncWebCrawler and its arun() method, but misses the core functionality related to iframe processing which is the main focus of the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions `LLMExtractionStrategy` as the strategy used for extracting structured data using language models. Evidence: `strategy = LLMExtractionStrategy( provider=\"ollama/llama2\", schema=Product.schema(), instruction=\"Extract product details from the page\")`.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy and its basic purpose, but misses crucial functionality aspects like HTML processing, chunking, and the actual extraction process that are key parts of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example in the documentation shows usage of the `AsyncWebCrawler` class with the `LLMExtractionStrategy`, hinting at an implicit utilization. Evidence: `result = await crawler.arun( url=\"https://example.com/product\", extraction_strategy=strategy)`.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements support for the documented LLMExtractionStrategy through its arun() method's extraction_strategy parameter, which processes the extracted content and integrates it with the caching system.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of AsyncWebCrawler with LLMExtractionStrategy, but misses the important implementation detail about how the class processes and caches the extracted content through its arun() method.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` in `AsyncWebCrawler` is showcased in the example, demonstrating how `LLMExtractionStrategy` is used to run extractions. Evidence: `result = await crawler.arun(...)`.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun's role in executing LLMExtractionStrategy but misses crucial aspects of its core functionality (handling URL fetching, caching, HTML processing) that are essential to enabling the extraction process",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation text snippet explicitly shows the usage of the `arun` method of the `AsyncWebCrawler` class for making requests with a 'wait_for' parameter to await certain page elements or JavaScript conditions, making it an explicit relationship.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship that the arun method supports waiting functionality through wait_for parameters for both CSS and JavaScript conditions, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not mentioned explicitly in the text, the `arun` method belongs to the `AsyncWebCrawler` class. This makes `AsyncWebCrawler` implicitly involved as any behavior of `arun` is naturally a part of its class.",
    "ground_truth_relationship": "The AsyncWebCrawler's arun() method implements the documented wait conditions through its kwargs parameter, which allows passing CSS selectors and JavaScript expressions via the wait_for parameter to control when the crawler proceeds with extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between AsyncWebCrawler and arun() method, but misses the crucial wait condition functionality described in the ground truth which is a key aspect of their relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The implementation of the `arun` method in `AsyncWebCrawler` indirectly depends on the crawling strategy. `AsyncPlaywrightCrawlerStrategy` is a default or primary strategy used within `AsyncWebCrawler`, implicating this artifact as part of the broader pathway through which `arun` achieves its functionality.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies a connection between AsyncPlaywrightCrawlerStrategy and AsyncWebCrawler's arun method, but misses the core implementation detail that this class directly handles the documented wait conditions through its smart_wait method.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions `JsonCssExtractionStrategy` in the context of defining a schema and combining it with JavaScript execution for dynamic content extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between JsonCssExtractionStrategy and its purpose in handling schema-based extraction with JavaScript execution for dynamic content, even if it doesn't detail all the implementation specifics.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Documented as the context manager used for executing asynchronous web crawling and JSON-based CSS extraction. The `AsyncWebCrawler` class is mentioned in the example code provided in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as a context manager for web crawling and JSON-CSS extraction, but misses the key dynamic data extraction capabilities with JavaScript execution and waiting conditions that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is demonstrated in the example code to execute the crawl operation using the defined `JsonCssExtractionStrategy`.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function of arun() for crawling with JsonCssExtractionStrategy, but misses the crucial aspect of supporting dynamic content extraction through JavaScript execution and screenshot capabilities that the ground truth emphasizes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The document explicitly mentions 'CSS Strategy' as the go-to for well-structured HTML content. The 'JsonCssExtractionStrategy' class implements a CSS-based extraction strategy, adhering to the description.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements the CSS Strategy mentioned in the documentation and its use for well-structured HTML content",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document clearly mentions 'LLM Strategy' for natural language text and best semantic understanding, which is implemented by the 'LLMExtractionStrategy' class, aligning with the described function.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the LLMExtractionStrategy class and its documented purpose for natural language text processing and semantic understanding, which matches the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "'Cosine Strategy' is highlighted in the document for mixed/complex content and best content relevance, which aligns with the functionalities provided by 'CosineStrategy'.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is meant for mixed/complex content and emphasizes content relevance, which aligns with the ground truth's explanation of how it processes mixed content using cosine similarity and hierarchical clustering",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides an example of using the 'arun' method, which is part of the 'AsyncWebCrawler' class, to perform web crawling with a proxy. This is explicitly shown in the usage example calling 'crawler.arun(url=\"https://example.com\")'.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the arun method works with proxy functionality through the AsyncWebCrawler class. The predicted description identifies the core relationship between arun and proxy-enabled web crawling, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the code snippet, an instance of 'AsyncWebCrawler' is created using the 'with' statement and proxy configuration, indicating direct usage of this class for performing crawling operations with proxy authentication.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that AsyncWebCrawler uses the 'with' statement and accepts proxy configuration for authenticated crawling. While it doesn't explicitly mention AsyncPlaywrightCrawlerStrategy, this omission doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' can be configured with a strategy pattern, where 'AsyncPlaywrightCrawlerStrategy' is used if not provided specifically. This is implicit because the usage of 'AsyncPlaywrightCrawlerStrategy' is a default behavior of 'AsyncWebCrawler' when instantiating a crawler if no 'crawler_strategy' is explicitly passed.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description discusses the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy as a strategy pattern relationship, while the ground truth describes the implementation of authenticated proxy support through ProxySettings configuration.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet directly mentions 'JsonCssExtractionStrategy' in the context of selecting and testing CSS selectors, handling dynamic content, and ensuring error handling. This class implements the strategy necessary for processing CSS selectors within an extraction process.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship between the code and documentation - that JsonCssExtractionStrategy implements CSS-based extraction using selectors, with the documentation providing relevant tips for using these selectors effectively.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example of using the 'arun' method of the 'crawler' instance, which is associated with the 'AsyncWebCrawler' class. This is evident from the line `result = await crawler.arun(url=\"https://example.com\")`, where 'arun' is called to perform a web crawl.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an async method for web crawling, but misses the crucial relationship with output formats that is central to the ground truth - namely that arun() is responsible for generating the different output formats mentioned in the documentation.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The documentation uses 'result.html' to access the raw HTML from the output of the 'arun' method, indicating 'CrawlResult.html' as a property used to retrieve data.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that html is used to store/access raw HTML content, which aligns with the ground truth's definition of html as a string property for raw HTML content",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet shows usage of 'result.cleaned_html' to access cleaned HTML, which indicates 'CrawlResult.cleaned_html' as a property designed for sanitized HTML content.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that cleaned_html is a property that provides access to sanitized HTML content, which aligns with the ground truth's core meaning",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The line 'markdown = result.markdown' directly points to the use of 'CrawlResult.markdown' to access the standard markdown version of crawled content.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that markdown is accessed via result.markdown and represents the standard markdown version of the content, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "Within the example, 'fit_md = result.fit_markdown' directly links to 'CrawlResult.fit_markdown', used to access a markdown variant focused on relevant content.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown is used to access the most relevant content in markdown format, which aligns with the ground truth's explanation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes examples using the 'arun' method directly, passing parameters such as 'url', 'wait_for', 'process_iframes', 'js_code', and 'delay_before_return_html'. These parameters are consistent with those defined in the 'AsyncWebCrawler.arun()' method signature.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method parameters correctly but fails to explain the key concept that these parameters enable dynamic content handling through kwargs. The implementation detail about how the method supports dynamic content features is missing.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'arun()' method in the documentation belongs to the 'AsyncWebCrawler' class, implying the usage of this class when handling dynamic content as demonstrated in the examples.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the arun() method belongs to AsyncWebCrawler class, but misses the crucial functionality aspect of how it handles dynamic content through specific parameters (wait_for, js_code, delay_before_return_html) which is a key part of the relationship described in the ground truth.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Since 'arun()' in 'AsyncWebCrawler' directly involves crawling operations, it interacts with 'AsyncCrawlerStrategy'-based strategies like 'AsyncPlaywrightCrawlerStrategy'. Particularly, strategies are responsible for executing tasks like 'js_code' evaluation and 'process_iframes'.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description gets part of the core idea by mentioning the strategy's role in executing tasks like js_code and process_iframes, but it misframes the relationship as being about arun() and AsyncWebCrawler when the ground truth focuses on the implementation of dynamic content handling through smart_wait and crawl methods.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class extends 'AsyncCrawlerStrategy', providing an asynchronous approach to crawling. This extension is crucial to implementing the dynamic content handling methods described in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The prediction correctly identifies the asynchronous nature and crawling functionality, but incorrectly specifies 'AsyncPlaywrightCrawlerStrategy' which isn't mentioned in the code, and misses the abstract base class nature that defines the interface for dynamic content handling.",
      "error_type": "implementation_specificity"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly mentions the `arun` method of the `AsyncWebCrawler` class. The example usage demonstrates configuring various content filtering parameters, such as word_count_threshold, excluded_tags, and link and media exclusions.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles content filtering parameters, which matches the ground truth that these parameters are implemented as optional kwargs and processed during HTML extraction. While it doesn't detail the internal processing, it captures the core relationship between the method and its filtering functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions the `AsyncWebCrawler` class as supporting session-based crawling with a `session_id` parameter, indicating its role in managing persistent browser sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the AsyncWebCrawler class supports session-based crawling through the session_id parameter. The predicted description captures the core functionality even though it doesn't mention all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `arun` within `AsyncWebCrawler` is implicitly referenced by mentioning the usage of `session_id` across `arun` calls for maintaining state, relating directly to the high-level functionality described in the document.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the arun method supports session-based functionality through the session_id parameter to maintain state across requests. The predicted description focuses on the high-level concept while the ground truth provides more implementation detail, but the core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned, `AsyncPlaywrightCrawlerStrategy` extends `AsyncCrawlerStrategy`, which `AsyncWebCrawler` can utilize for executing browsing tasks that include session management elements as described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes the inheritance relationship and session management capability, but describes it more vaguely and misses explaining the concrete implementation through the sessions dictionary and kill_session() method that the ground truth specifically highlights.",
      "error_type": "incomplete_implementation_details"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The document explicitly uses 'LLMExtractionStrategy' to define a strategy for extracting specific parts of an article using an LLM. This association is clear in the example where 'LLMExtractionStrategy' is instantiated with parameters 'provider' and 'schema'.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as a class that uses LLMs to extract specific content based on schema and provider parameters. While it omits some implementation details like instruction parameters and structured format parsing, it gets the fundamental relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is used in the provided example to perform a web crawl with the given strategy. The example demonstrates its usage to execute the 'LLMExtractionStrategy'.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for web crawling with strategies, but misses the key aspect about LLM-based extraction functionality and schema-based content processing that is central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The variable 'result' in the example is assigned the return value of 'arun', which is an instance of 'CrawlResult'. This class encapsulates the results of the web crawling strategy execution.",
    "ground_truth_relationship": "The CrawlResult class stores the extracted_content field that holds the structured data parsed by LLMExtractionStrategy into the ArticleContent format shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that result is a CrawlResult instance from arun, but misses the key relationship between extracted_content and the ArticleContent structure shown in the documentation",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example in the documentation assigns 'result.extracted_content' to a variable, indicating that the 'CrawlResult' class provides an 'extracted_content' attribute, which holds the extracted content as described.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is an attribute storing extracted content, but misses the crucial aspect that it specifically stores the content as a JSON string matching a Pydantic schema",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The AsyncWebCrawler class is directly instantiated using the line `async with AsyncWebCrawler(verbose=True) as crawler:`. This shows that the AsyncWebCrawler is the main component facilitating the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main component for crawling but misses crucial aspects about its asynchronous functionality, caching capabilities, and support for extraction strategies that are central to its purpose as described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method `arun` is explicitly called in the usage example: `result = await crawler.arun(...)`. This indicates that this function is being utilized to execute the crawling operation, passing various parameters like URL and extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's usage for crawling operations but omits crucial aspects about its implementation of caching, structured result handling, and JSON extraction workflow described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The JsonCssExtractionStrategy is explicitly instantiated with `extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)`. This strategy is utilized to define how data from the webpage is extracted.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy is used for data extraction but misses the crucial schema-based BeautifulSoup processing approach that is core to how it actually works",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example includes `product_data = json.loads(result.extracted_content)`, which accesses the extracted_content attribute of the CrawlResult class to fetch and parse the data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that extracted_content is an attribute accessed from the CrawlResult that needs to be JSON-parsed, which aligns with the ground truth's explanation of it being a raw string requiring JSON parsing.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The text snippet shows an example usage of the 'arun()' method from the 'AsyncWebCrawler' class, where several parameters like 'url', 'page_timeout', 'delay_before_return_html', and 'wait_for' are passed. These directly relate to the method signature of 'arun()'.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() method usage and parameters, but misses the key relationship that these timeouts/waiting controls are implemented through **kwargs being passed to crawler_strategy.crawl()",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While 'AsyncWebCrawler' is not explicitly mentioned in the text, it implicitly appears as the class containing the 'arun()' method, which is demonstrated in the usage example. The method belongs to this class, and hence understanding its relationship to the 'AsyncWebCrawler' class is necessary to fully comprehend the usage.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method of AsyncWebCrawler but doesn't capture the key functionality of timeout and waiting parameters implementation that is central to the ground truth. It only describes the structural relationship rather than the functional one.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncWebCrawler' can utilize different strategies, including 'AsyncCrawlerStrategy', which is indirectly related through strategy pattern implementation. While not directly in the provided snippet, understanding how 'AsyncWebCrawler' operates implies recognizing it may utilize strategies like 'AsyncCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the strategy pattern implementation and relationship between AsyncWebCrawler and AsyncCrawlerStrategy, while the ground truth specifically describes AsyncCrawlerStrategy's role in handling timeouts and waiting capabilities. While not entirely wrong, it misses the core functionality focus.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "This specific strategy ('AsyncPlaywrightCrawlerStrategy'), might be used within the 'AsyncWebCrawler' context, especially when detailed configurations like page timeouts and waits are involved, though not explicitly mentioned, it is part of the 'AsyncCrawlerStrategy' pathways.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles timeout and waiting configurations, which aligns with the ground truth's explanation of implementing these behaviors through smart_wait and related methods. While it's less detailed, it captures the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation specifies the use of the 'JsonCssExtractionStrategy' class from the 'crawl4ai.extraction_strategy' module to create an extraction strategy based on a schema for content extraction, as evidenced by the commented code snippet.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of using JsonCssExtractionStrategy with a schema, but misses key aspects about pattern-based selection, recursive extraction, and handling of nested data structures that are central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of an 'AsyncWebCrawler' instance is called with the extraction strategy applied to a URL. This is demonstrated by the example that uses 'await crawler.arun(...)'. This implies usage of the 'arun' method for asynchronous crawling tasks.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun is used for asynchronous crawling with an extraction strategy, but misses the key functionality of processing the extraction strategy schema to extract structured data based on defined selectors and patterns",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The usage of 'arun()' in the code snippet explicitly relies on an instance of 'AsyncWebCrawler', which provides access to the method. While its instantiation isn't directly shown, it is implied through 'crawler.arun()'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements pattern-based selection through its aprocess_html method, which uses JsonCssExtractionStrategy to extract structured data according to the schema defined in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the basic usage of arun() method and AsyncWebCrawler instantiation, while missing the core relationship about pattern-based selection and JsonCssExtractionStrategy implementation, which is the main focus of the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun()` method is explicitly mentioned in the usage example within the documentation snippet. It is shown being invoked with parameters such as `url` and `word_count_threshold`, indicating its role in controlling the content retrieval process based on provided criteria.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() and its role in content control, but focuses mainly on its invocation pattern rather than explaining how it actually implements content filtering through processing parameters",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not directly mentioned, the `AsyncWebCrawler` class is implicitly referenced through its method `arun()`. It provides the context for where `arun()` resides, making `AsyncWebCrawler` essential for understanding the environment in which content control can be executed.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncWebCrawler and arun(), but fails to highlight the key aspect that the filtering options described in the documentation are directly implemented as parameters in arun(). Instead, it focuses on the more general context.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls `arun()` method on a `crawler` object. This method is part of the `AsyncWebCrawler` class, providing evidence from the surrounding code that explicitly mentions using `arun()` for executing the crawl function.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the wrong relationship, describing the existence and usage of arun() in a caller context, while the ground truth describes the internal error handling implementation within the arun() method itself",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned, the instantiation of the `extraction_strategy` implies the use of a strategy pattern, and `AsyncPlaywrightCrawlerStrategy` is a concrete implementation of `AsyncCrawlerStrategy` possibly used by `AsyncWebCrawler`.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements comprehensive error handling through try-catch blocks and error messages, matching the documentation's emphasis on handling extraction failures, timeouts, and general exceptions during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted text focuses on strategy pattern implementation and class inheritance while the ground truth specifically describes the error handling functionality and mechanisms within the class. These are fundamentally different aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions `Cosine Strategy` as an effective strategy when semantic understanding is required. This strategy is likely to be used in conjunction with the `arun()` function as indicated in the snippet.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies CosineStrategy as being used for semantic understanding, but misses crucial aspects like hierarchical clustering and oversimplifies by focusing mainly on the arun() function usage rather than the core functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses the AsyncWebCrawler class explicitly in the example, as indicated by the import statement and its instantiation with `AsyncWebCrawler(verbose=True)`.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the use of AsyncWebCrawler class but only mentions the import and instantiation, missing the crucial aspects of session management and interaction methods that are central to the ground truth's description of persistent browser sessions and specific user interactions.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of AsyncWebCrawler is implicitly used in the example to perform a crawl for each page. This usage implies its relevance in implementing the described session-based crawling functionality.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun's role in performing crawls but fails to mention the crucial session management aspect through session_id that maintains state across requests, which is a key feature described in the ground truth",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "This method is implicitly referred to by the call `await crawler.crawler_strategy.kill_session(session_id)`, which is aimed at closing the crawl session as part of session management.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the key relationship - that kill_session is used to close/cleanup crawl sessions, which aligns with the ground truth's explanation of session cleanup functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the example, `result.extracted_content.count('.content-item')` indicates reliance on this attribute to assess the items extracted, which forms part of the content processing logic.",
    "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that extracted_content is used to process/count extracted items, which aligns with the ground truth's explanation of it storing HTML content matching CSS selectors for item counting",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly uses the 'AsyncWebCrawler' class within the example code, indicating it as a central part of the implemented crawling functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as central to the crawling functionality, but misses the crucial aspects of its async context management and support for multiple extraction strategies shown in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method of 'AsyncWebCrawler' is explicitly invoked to perform crawling with specific strategies for both structured and LLM-based analysis, as shown in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of arun() as demonstrated in the documentation - it accurately describes how the method is used to perform crawling with different extraction strategies. While it doesn't mention all implementation details, it correctly identifies the main relationship and usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly mentions and uses 'JsonCssExtractionStrategy' as the strategy to extract structured content from the web page using a CSS selector-based schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's use of CSS selectors for extraction, but misses the crucial aspect of it implementing a structured extraction pattern with a specific schema format (baseSelector and fields properties) as shown in the example.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly details using 'LLMExtractionStrategy' with a specified provider and schema for performing LLM-based semantic analysis.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy being used for LLM-based semantic analysis with providers and schemas, which aligns with its implementation shown in the documentation example",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Though not directly mentioned, 'CrawlResult' is implicated by the return type of the 'arun' method, which outputs a result with structured data, semantic analysis, and media fields.",
    "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that CrawlResult serves as the output container for extraction operations, containing structured data and media content. The predicted description correctly identifies its role as the return type for 'arun' method calls, even if it's more implicit.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides an example using the 'arun' method of a crawler object, which aligns with the usage of the 'AsyncWebCrawler.arun()' method in the available artifacts.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct method (arun) but misses the key relationship about how it accepts anti-bot control parameters documented in the manual options section",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation explicitly mentions the 'TopicSegmentationChunking' class. It is described as implementing text segmentation into topic-based chunks using the TextTiling algorithm, matching the class's functionality.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of using TextTiling algorithm for topic-based text segmentation, which aligns with the ground truth's explanation of the implementation. While it doesn't mention the keyword extraction feature, this omission doesn't change the fundamental relationship described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The artifacts explicitly mention the use of parameters such as 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k' in conjunction with 'CosineStrategy'. This indicates that 'CosineStrategy' is the class implementing the functionality described in the documentation snippet.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy implements the functionality described in the documentation, with the parameters semantic_filter, sim_threshold, word_count_threshold, and top_k. While it's less detailed than the ground truth, it captures the core relationship between the documentation and code.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "'CosineStrategy' is an implementation of an extraction strategy and typically extends a base class for extraction. Although not mentioned explicitly in the snippet, it is related implicitly as it extends 'ExtractionStrategy'.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class that gets extended, but misses the crucial aspect that it serves as a template for implementing various content extraction configurations with specific parameter settings",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation explicitly details the 'SlidingWindowChunking' class, describing its purpose and parameters, and even gives a code example using this class.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies the class and its documentation, but fails to explain the actual relationship between the code implementation and documentation - specifically how the sliding window algorithm is implemented in the code to match the documented behavior.",
      "error_type": "incomplete_relationship_explanation"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'SlidingWindowChunking' class, detailed in the documentation snippet, is an implementation of the base 'ChunkingStrategy' class, as is typical in object-oriented design patterns where specific strategies implement an abstract base strategy.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that SlidingWindowChunking is an implementation of the abstract ChunkingStrategy class, with the predicted description focusing on the inheritance relationship while the ground truth adds more implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'CosineStrategy' class, detailing the initialization parameters such as 'semantic_filter', 'word_count_threshold', 'sim_threshold', 'max_dist', 'linkage_method', 'top_k', 'model_name', and 'verbose'. These parameters correspond directly to the constructor of the 'CosineStrategy' class.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the documentation parameters match the CosineStrategy class constructor parameters with their corresponding default values. Both descriptions convey the same core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'CosineStrategy' class is a subclass of 'ExtractionStrategy', which is not mentioned directly in the snippet but can be identified from the code structure. Its relation to 'CosineStrategy' is inherent as the base class.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is a subclass of ExtractionStrategy and captures the inheritance relationship. While it's more concise than the ground truth, it doesn't contradict the fundamental relationship described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The document snippet explicitly discusses the use of `CosineStrategy` with parameters such as `semantic_filter`, `word_count_threshold`, `sim_threshold`, `max_dist`, and `top_k` to perform content similarity analysis and topic clustering as demonstrated in the Python code snippet.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation by identifying the CosineStrategy class, its key parameters, and its purpose for content similarity analysis and topic clustering. Both descriptions align on the fundamental functionality and configuration options.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example within the document shows usage of `AsyncWebCrawler.arun()` to execute the `CosineStrategy` as part of the crawling and content extraction process. Although `arun()` is not directly discussed, it's used in the example code, establishing its role in executing the strategy.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify arun() as the method that executes crawling while accepting and implementing extraction strategies like CosineStrategy. The predicted description captures the core relationship even though it's less detailed.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "`CosineStrategy` is derived from `ExtractionStrategy`, indicating that all functionalities and interfaces applicable to extraction strategies apply to `CosineStrategy` as well. This inheritance is not explicit in the document but is inherent in the object-oriented structure shown in the artifacts.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, and implicitly acknowledges the shared functionality. While it doesn't explicitly mention the parallel processing and abstract method details, it captures the core parent-child class relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses the `AsyncWebCrawler` class within an asynchronous context to run its tasks, as can be seen in the code `async with AsyncWebCrawler(verbose=True) as crawler:`. This demonstrates a clear use of the class for initializing and managing the lifecycle of web crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately describes the AsyncWebCrawler's context management but misses the main focus of the ground truth - the screenshot functionality. While it's not wrong, it fails to capture the core relationship between the class and its screenshot capabilities.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The function `arun` is explicitly called in the documentation snippet through `await crawler.arun(url=url, screenshot=True, bypass_cache=True)`. This method is responsible for initiating the crawling process, which is essential for capturing the webpage screenshot as depicted in the example.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for screenshot capture, but misses explaining how the screenshot functionality is actually implemented through screenshot_data and caching, which is a key aspect of the ground truth.",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The `CrawlResult.screenshot` attribute is used in the snippet to retrieve and decode the captured screenshot as indicated by `if result.success and result.screenshot:`. It illustrates usage of the result object to access the screenshot data returned after crawling.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the screenshot field stores base64-encoded screenshot data that can be accessed and decoded from the CrawlResult object. The predicted description captures the core relationship between the screenshot attribute and its usage in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The 'AsyncWebCrawler' class is used explicitly in the code snippet from the documentation, which demonstrates setting up a crawler to handle multiple interactions on a dynamic page.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for handling dynamic pages, but misses crucial aspects about how it implements the complex interactions through specific features like session management, JavaScript execution, and dynamic content loading that are central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The method 'arun()' is explicitly called in the code snippet to execute a crawling task with various JavaScript and content waiting parameters, as demonstrated in the documentation example.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for crawling with parameters, but misses the crucial aspect that it specifically implements dynamic page interactions like cookie handling and infinite scroll pagination mentioned in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "In the example, 'kill_session()' is called to clean up the session after interactions, specifically mentioned in the documentation.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that kill_session is used for cleanup after interactions, but misses the crucial aspect of what it's cleaning up (browser resources, page and context objects) and why it's important (preventing memory leaks)",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class, while not explicitly named in the snippet, implements the behavior that 'AsyncWebCrawler' relies on for performing page interactions, as it's the underlying strategy class used by 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the complex interaction example by providing methods like crawl() and smart_wait() that handle dynamic page loading, cookie consent, session management, and JavaScript execution as shown in the documentation's crawl_dynamic_content() example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is used by AsyncWebCrawler, but fails to explain the key functionality for handling complex interactions like dynamic page loading, cookie consent, and session management that are central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' inherits from 'AsyncCrawlerStrategy', forming the base for strategy implementation, implicitly involved in the page interaction examples.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the essential interface methods used by the documented complex interaction example, particularly through its crawl method that enables the dynamic page interactions shown in the crawl_dynamic_content function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but incorrectly specifies 'AsyncPlaywrightCrawlerStrategy' which isn't mentioned in the code/docs. It also misses the key connection to the complex interaction example's functionality.",
      "error_type": "addition_of_incorrect_details"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation directly shows how to instantiate and use the `AsyncWebCrawler` class. The example code snippet begins with `from crawl4ai import AsyncWebCrawler` and utilizes `AsyncWebCrawler` to perform web crawling operations, including verbose mode configuration and contextual crawling using the `arun` method.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the documentation shows how to use AsyncWebCrawler, but misses a key aspect - it doesn't mention the comprehensive functionality support for content filtering, processing, and cache control that the ground truth emphasizes as core relationships between the code and documentation.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the documentation's example, the `arun` method of the `AsyncWebCrawler` class is called to perform a web crawl on a specified URL with various parameters. This method is crucial for invoking the actual crawling process as shown in `await crawler.arun(url=\"https://example.com\", ...)`, linking its usage with the example's outcomes.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for web crawling with parameters, but misses the key relationship between the documentation's example parameters and their implementation in the actual method's validation, caching, and processing logic",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The `success` attribute of the `CrawlResult` is explicitly checked in the example to determine if the crawl operation was successful. This is illustrated by the conditional statement `if result.success:`.",
    "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that success is used as a boolean check to determine if the crawl operation completed successfully before proceeding with content processing, even though it doesn't explicitly mention all the types of content processing that follow",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation's example accesses the `markdown` attribute of `CrawlResult` to print the content of the successful crawl result. It is used within `print(\"Content:\", result.markdown[:500])` to extract and display crawled content.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the markdown attribute contains the crawled content and is accessed via result.markdown[:500] to display the first 500 characters. The core relationship and functionality are consistently described.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example loops through `result.media[\"images\"]` to print details of found images, demonstrating the use of `media` to handle image extraction results within `CrawlResult`.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that media dictionary stores image data that can be accessed and processed through result.media['images']. Both descriptions show the same usage pattern and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation demonstrates iterating over `result.links[\"internal\"]` to display internal links found during the crawl, showcasing the use of `links` to handle link extraction.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic functionality of accessing links through result.links, but misses the key structural detail that links are categorized into 'internal' and 'external' in the dictionary, which is a crucial aspect mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "If the crawl fails, the documentation example demonstrates printing `result.error_message` to display error details, indicating its role as an error reporting mechanism in `CrawlResult`.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that error_message is used to report/display error details when a crawl fails (result.success is False). The predicted description captures the core error reporting relationship shown in the example code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The `arun` method of the `AsyncWebCrawler` class is explicitly used in the example provided to demonstrate crawling a webpage content using different extraction strategies.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that arun() is used with different extraction strategies to crawl webpage content, which aligns with the ground truth's explanation of the same functionality being demonstrated with multiple strategy types.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The `LLMExtractionStrategy` is explicitly mentioned as part of the code example where it is used to perform structured data extraction using an LLM provider.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for structured data extraction with an LLM provider, but misses key aspects like custom schemas and instructions that are central to its functionality as shown in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The `JsonCssExtractionStrategy` is explicitly mentioned in the code example to highlight how repeated patterns can be extracted from a webpage, demonstrating its integration with the `AsyncWebCrawler.arun()` method.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of JsonCssExtractionStrategy as a pattern extraction tool that works with repeated HTML elements, showing its integration with the crawler system. While the predicted is less detailed about the CSS selector implementation, it correctly identifies the main purpose and relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "While not explicitly mentioned, the `extracted_content` attribute of `CrawlResult` is used implicitly when accessing structured and pattern data in the JSON format, as detailed in the example code.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that extracted_content stores crawled data as a string that gets parsed as JSON for structured and pattern-based extraction results",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The `fit_markdown` attribute is accessed in the example for returning the main content in markdown format, indicating its implicit role in handling the results of the crawl.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is used to store and access the main content extracted from the webpage in markdown format, as shown in the example where it's accessed via result.fit_markdown and stored in the main_content field",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The text snippet explicitly mentions the use of LLMExtractionStrategy. It describes customizing the strategy using different LLM providers, indicating that this class implements configurable logic for extracting information using a language model.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that LLMExtractionStrategy enables customizable LLM provider integration for extraction, matching the ground truth's explanation of how the class allows users to specify providers and API tokens for LLM-based extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned, LLMExtractionStrategy extends ExtractionStrategy. The text implies functionality enhancements over a base strategy for extraction, which aligns with the paradigm of subclassing to extend base class functionality.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that ExtractionStrategy is an extensible base class designed to support different LLM implementations. The predicted description correctly identifies the inheritance relationship and extensibility purpose, which aligns with the ground truth's explanation of the class serving as a foundation for implementing custom LLM providers.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'AsyncWebCrawler' class for proxy configuration by showing examples directly using 'with AsyncWebCrawler(...) as crawler'.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles proxy configuration through its initialization, even though it doesn't explicitly mention the kwargs parameter. The core functionality of enabling proxy configuration through the class is accurately represented.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun' method is implicitly used in the document's examples as 'result = await crawler.arun(url=\"https://example.com\")', where it plays the role of executing a crawl task upon initialization of 'AsyncWebCrawler'.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method used for crawling, but misses the core relationship about proxy configuration support that is central to the ground truth description",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned directly in the snippet, 'AsyncWebCrawler' can utilize 'AsyncPlaywrightCrawlerStrategy' as a default fallback when crawling, and proxy settings might be passed down to strategies implementing 'AsyncCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly indicates proxy support exists but incorrectly suggests it's through AsyncWebCrawler fallback rather than directly implemented in AsyncPlaywrightCrawlerStrategy via ProxySettings objects.",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the documentation snippet, `AsyncWebCrawler` is used in the example code demonstrating how to enable Magic Mode. It clarifies that the `AsyncWebCrawler` object is used to perform the crawling operation with the `arun` method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in performing crawling operations, but misses the crucial aspect of how it implements Magic Mode's anti-bot protections through its crawler_strategy parameter and AsyncPlaywrightCrawlerStrategy.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a Python code example that makes an explicit call to the `arun` method on an `AsyncWebCrawler` instance, which is part of the public API used to start the web crawling process with specific parameters.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method of AsyncWebCrawler for web crawling, but misses the crucial relationship with the magic parameter and anti-bot protection features that are central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, `AsyncCrawlerStrategy` is the base class that `AsyncPlaywrightCrawlerStrategy` extends. It provides the abstract interface which `AsyncWebCrawler` depends upon through `AsyncPlaywrightCrawlerStrategy`, enabling the strategies for web crawling, including features implied by Magic Mode.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class for crawling functionality, but it makes specific claims about AsyncPlaywrightCrawlerStrategy that aren't evident in the context and misses the direct connection to Magic Mode's anti-bot features mentioned in the ground truth.",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The implementation of anti-bot protections as specified in the Magic Mode documentation would extend or be a feature of `AsyncPlaywrightCrawlerStrategy`, which inherits from `AsyncCrawlerStrategy`. Hence, it's implicitly linked as the specific strategy used by `AsyncWebCrawler` for executing Magic Mode functions.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that Magic Mode is implemented through AsyncPlaywrightCrawlerStrategy and accurately describes the inheritance relationship. While it's more abstract than the ground truth, it captures the core architectural relationship and implementation approach.",
      "error_type": ""
    }
  }
]