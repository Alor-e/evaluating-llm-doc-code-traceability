[
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True)'. This shows that the AsyncWebCrawler class is the main entry point for crawling.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main entry point for crawling, but misses describing the extensive functionality and methods (arun(), content filtering, processing, caching) that directly map to the documented example usage. It's an oversimplification that omits crucial aspects.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform a crawl on the given URL with specific options.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the core relationship between the documented example and the actual implementation, particularly how the parameters shown in the example map to validation and processing logic in the code",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The arun() method returns a CrawlResult object. The snippet uses this object to check for success and to access various attributes containing the crawl results.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that CrawlResult is used to hold crawl results, but misses that it's primarily a data structure class definition rather than just a return type from arun(). While not entirely wrong, it presents an incomplete view of the class's core purpose.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet prints the first 500 characters of the 'markdown' attribute of the CrawlResult, demonstrating its use for clean content output.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute contains cleaned crawled content and that the example shows accessing the first 500 characters of this content via result.markdown[:500]",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation iterates over the 'media' property of the CrawlResult to process images found in the crawled page.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that media/images from crawled pages are stored in the media property and can be processed, which aligns with the ground truth's explanation of CrawlResult.media storing and enabling access to media elements like images.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The code snippet demonstrates iterating over the 'links' attribute to handle internal links, emphasizing its role in content extraction.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of accessing links but misses the crucial detail that links are categorized into internal/external in a dictionary structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The crawl result's 'success' attribute is checked to determine if the crawl operation was successful before proceeding to process content.",
    "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - checking the success attribute to determine if crawl was successful before processing content. While it doesn't detail all the types of content processed (media, links), this is a minor omission that doesn't change the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In case of a failure, the 'error_message' attribute of the CrawlResult is used to print detailed error information, as shown in the snippet.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly convey that error_message is used to provide error information when a crawl operation fails (when success is False)",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler', indicating that this class is the entry point for initiating the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class and its usage pattern, but misses the crucial functionality of JavaScript execution and extraction strategy coordination described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun()' method on an AsyncWebCrawler instance to perform the crawl with dynamic JavaScript execution and extraction parameters, which makes this method a key part of the demonstrated functionality.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for web crawling, but misses the crucial aspect that it specifically handles dynamic content through JavaScript execution and LLM extraction capabilities, which is a key focus of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "Within the arun() method call, an instance of LLMExtractionStrategy is explicitly created and passed as the extraction_strategy parameter. This class provides LLM-based extraction functionality by processing instructions and API parameters, demonstrating its direct role in the extraction process.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's role in processing instructions and API parameters, but misses the broader context that it's a complete extraction strategy class for handling dynamic web content with LLM-based extraction, including features like chunking and different extraction types.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly instantiated in the snippet, it underpins the design of LLMExtractionStrategy by defining the extraction interface, making its role implicit in the overall extraction mechanism.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is a base class that provides the foundation for other extraction strategies, specifically mentioning LLMExtractionStrategy as an implementation. The predicted description accurately conveys this inheritance and design pattern relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This confirms that the AsyncWebCrawler class is the user\u2010facing API for crawling.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the main class but focuses solely on it being a user-facing API, missing the core caching functionality that is central to the ground truth description.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the method 'arun' is called on the AsyncWebCrawler instance (e.g., result1 = await crawler.arun(url=...) and result2 = await crawler.arun(url=..., bypass_cache=True)). This shows that the arun() method is central to triggering a crawl and managing caching behaviour.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling and mentions caching behavior, but misses the key implementation detail that caching is done via async_db_manager.aget_cached_url() and how the bypass_cache parameter controls this behavior",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet accesses 'result1.markdown' (and similarly result2.markdown) to display the output, implying that the CrawlResult object returned by arun() includes a public attribute 'markdown'. Although not mentioned by name, the attribute is used by the example for output formatting.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the CrawlResult object has a markdown attribute/property that contains the crawled webpage content, and both reference its usage in the example code for displaying content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:', indicating that this class serves as the entry point to handle complex, interactive crawling sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the entry point for crawling sessions, but misses crucial functionality around handling complex page interactions, JavaScript execution, and session management that are core to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls 'await crawler.arun(...)' with parameters such as url, js_code, and wait_for to handle initial page load and subsequent dynamic interactions. This demonstrates its role in processing dynamic content.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used for handling dynamic page interactions through parameters like js_code and wait_for, which aligns with the ground truth's explanation of how it enables complex interactions like cookie consent and pagination.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "After performing multiple page interactions, the snippet calls 'await crawler.crawler_strategy.kill_session(session_id)' to clean up the session. This explicit call indicates the use of the kill_session method for session management.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that kill_session is used for cleanup but misses the crucial detail about closing browser resources (page and context objects) to prevent memory leaks",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly demonstrates the use of the AsyncWebCrawler class by creating an instance using an async context manager and passing a 'browser_type' parameter.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of AsyncWebCrawler with async context manager but misses the key point that browser type selection is implemented through the crawler_strategy parameter rather than directly through a browser_type parameter",
      "error_type": "incomplete_implementation_detail"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet shows a call to the 'arun()' method on the AsyncWebCrawler instance, indicating it is the core method to perform a crawl using the configured browser.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the core crawling method but misses the key relationship about browser engine selection through browser_type parameter that is central to the documentation's purpose",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler by default instantiates AsyncPlaywrightCrawlerStrategy (if no alternative strategy is provided) and passes the 'browser_type' parameter to it. This strategy class handles launching the appropriate browser engine (Chromium, Firefox, or WebKit) based on the parameter value.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that AsyncPlaywrightCrawlerStrategy handles browser type selection and can launch Chromium (default), Firefox, or WebKit based on the browser_type parameter. The predicted description correctly identifies the key relationship even though it frames it through AsyncWebCrawler's perspective.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy to implement the abstract methods required for crawling. This abstract base class defines the interface that underpins all browser strategy implementations used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions express that AsyncCrawlerStrategy is an abstract base class that defines the interface for browser-based crawling implementations. The predicted description captures the core concept of the interface role, even though it doesn't explicitly list all browser types.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'advanced features of JsonCssExtractionStrategy' in the context of processing a complex e-commerce HTML structure. This indicates that the strategy is intended to extract data (such as product details, reviews, and related items) using CSS selectors as defined in its schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JsonCssExtractionStrategy - using CSS selectors to extract structured data from HTML elements in an e-commerce context. It aligns with the ground truth's explanation of using BeautifulSoup and schema-based extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not mentioned explicitly in the snippet, it serves as the abstract interface which JsonCssExtractionStrategy implements, making it an essential part of the extraction chain.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class, but focuses too narrowly on JsonCssExtractionStrategy (which isn't even shown in the code) while missing the core purpose of HTML data extraction and parallel processing described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly shows instantiation of an AsyncWebCrawler object with the 'proxy' parameter to set up a proxy. This explicitly demonstrates how to configure proxy settings when creating a crawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports proxy configuration, but misses that this is implemented through the crawler_strategy parameter rather than a direct proxy parameter.",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the provided example, after instantiating AsyncWebCrawler, the 'arun' method is directly called to execute the crawling process on a specified URL. This demonstrates explicit usage of the arun() method.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method's basic crawling functionality but misses the key relationship with proxy configuration that is central to the ground truth description",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a call to crawler.arun(), indicating its use in retrieving the crawl result that contains the cleaned HTML. This directly shows the functional entry point for the cleaning operation.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the entry point for crawling, but fails to mention the key HTML cleaning functionality through sanitization and extraction strategies that is central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet prints result.cleaned_html, explicitly referencing the cleaned_html attribute of the CrawlResult object that holds the sanitized HTML output.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that cleaned_html is a property containing sanitized HTML output, which aligns with the ground truth's explanation of it storing sanitized HTML after removing unwanted elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet includes usage examples that explicitly instantiate CosineStrategy with various parameters such as 'word_count_threshold', 'top_k', 'verbose', 'semantic_filter', 'sim_threshold', and 'max_dist' to optimize extraction performance and handle different content types.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly convey that CosineStrategy's configurable parameters (word_count_threshold, sim_threshold, max_dist) enable customization for different content types and optimization needs. The predicted description effectively captures the code-documentation relationship by highlighting the same key parameters and their role in extraction performance.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the documentation snippet, its presence is fundamental as the base class for CosineStrategy, thereby implicitly supporting the documented functionality.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but focuses too narrowly on CosineStrategy inheritance rather than the core framework functionality described in the ground truth",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly states that advanced techniques can be used with JsonCssExtractionStrategy. It advises users to leverage schema keys such as 'default' and 'transform' to clean or format extracted data, directly naming JsonCssExtractionStrategy as the tool for handling complex web page structures.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as a tool for handling web page structures and data extraction, but focuses on different aspects (transform/default keys) than the ground truth which emphasizes the gradual schema definition and incremental processing capabilities.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although the documentation does not mention ExtractionStrategy directly, any advanced usage with JsonCssExtractionStrategy inherently relies on the interface and behavior defined by its base class ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the key parallel processing functionality implemented in ExtractionStrategy that the ground truth focuses on",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet is titled 'Cosine Strategy' and describes a method that breaks down content into chunks, converts text to vectors, and uses cosine similarity for clustering. This directly maps to the CosineStrategy class which implements similarity\u2010based clustering for content extraction.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the core concept of Cosine Strategy using chunks and similarity-based clustering, but misses explaining the full 5-step workflow detailed in the ground truth that includes ranking and filtering steps",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, making ExtractionStrategy an inherent part of the implementation. Although the documentation does not mention ExtractionStrategy by name, the underlying design of CosineStrategy relies on the abstraction provided by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is the base class providing the fundamental structure and interface for implementations like CosineStrategy. The predicted description correctly identifies the inheritance relationship, even though it's less detailed about the specific methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an asynchronous context manager ('async with AsyncWebCrawler(...)'). This shows that AsyncWebCrawler is the primary class to configure browser options.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the primary class and its use with async context manager, but misses mentioning its comprehensive implementation of browser configurations and features shown in the documentation",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the arun() method on the AsyncWebCrawler instance to perform the crawl. This direct invocation shows the intended usage for starting the crawling process.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called to perform crawling, but misses the crucial aspect that the code actually implements the core processing logic while the documentation shows how to configure and use it with various options",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "After invoking arun(), the code accesses properties such as result.markdown, result.screenshot, and result.success. These properties belong to CrawlResult, highlighting its role as the response object for the crawl.",
    "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies CrawlResult as the response object that provides access to properties like markdown, screenshot, and success, which aligns with the ground truth's explanation of CrawlResult being the structured response object for the crawl function.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler internally instantiates AsyncPlaywrightCrawlerStrategy as its default crawling strategy when no alternative is provided. This enables the advanced browser configurations used in the example.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly implies a relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but incorrectly states this is via default instantiation. The ground truth more accurately describes that the example demonstrates using features implemented in AsyncPlaywrightCrawlerStrategy without making claims about default instantiation.",
      "error_type": "incorrect_assumption"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, the default strategy used, extends AsyncCrawlerStrategy. This inheritance relationship ensures that the crawling interface defined in the abstract base class is maintained.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class, but incorrectly specifies AsyncPlaywrightCrawlerStrategy as the default strategy, which isn't mentioned in the code or documentation.",
      "error_type": "unsupported_claim"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler by using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This shows that the AsyncWebCrawler class is directly used to create a crawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler is used for crawling, but focuses on instantiation syntax rather than the core error handling functionality described in the ground truth. It misses the key relationship between the class and the documented error handling pattern.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code sample, the method arun() is invoked via 'await crawler.arun(url=..., extraction_strategy=..., bypass_cache=True)'. This clearly demonstrates that the AsyncWebCrawler.arun() method is used to perform the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a method used for crawling, but misses the core focus of the ground truth which is about error handling implementation. While not wrong, it fails to capture the main relationship described in the documentation about error handling and retries.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The example constructs an instance of LLMExtractionStrategy using parameters such as provider, api_token, and instruction. This shows that LLMExtractionStrategy is the extraction strategy implemented for LLM-based content extraction.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that LLMExtractionStrategy is used for LLM-based extraction but misses the crucial aspect of error handling and parallel processing implementation that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the sample code, 'json.loads(result.extracted_content)' is used to parse the content extracted from the crawl. This indicates that the 'extracted_content' field is a public attribute of the crawl result that holds the extracted data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies extracted_content as a field containing extracted data, but misses the crucial aspect of error handling and retry mechanism that is central to the ground truth description",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly advises using a CSS Strategy for well-structured HTML and highest structural accuracy. 'JsonCssExtractionStrategy' is the implementation that uses CSS selectors to extract content, matching the 'CSS Strategy' described in the guide.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements the CSS Strategy from the documentation and accurately notes its use of CSS selectors for structured HTML extraction, which aligns with the ground truth's core message.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation mentions using an LLM Strategy for natural language text and best semantic understanding. 'LLMExtractionStrategy' directly corresponds to this recommendation by utilizing language models for extraction, aligning with the guide\u2019s requirements.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy implements the LLM Strategy option from the documentation, highlighting its use for natural language text and semantic understanding. While it's more concise than the ground truth, it captures the essential relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The guide explicitly advises using the Cosine Strategy for mixed or complex content where content relevance is key. 'CosineStrategy' implements extraction based on cosine similarity and clustering, which fulfills the noted requirement for semantic relevance and moderate performance.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that CosineStrategy is designed for mixed/complex content processing using cosine similarity and clustering, with moderate performance characteristics.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler object with 'headless=False', indicating that this class is used to perform crawling operations. The usage example clearly shows its direct instantiation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as a crawler class but focuses on its instantiation pattern rather than its core functionality of handling overlays as described in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method (via 'await crawler.arun(...)') on the AsyncWebCrawler instance. This method is responsible for executing the crawl with parameters such as 'remove_overlay_elements=True' and 'screenshot=True'.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method execution but fails to mention the key overlay removal functionality that is central to the ground truth relationship description.",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned by name in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler_strategy. This strategy internally implements functions such as remove_overlay_elements (for removing overlay elements) and take_screenshot (for capturing content), which are triggered by the parameters 'remove_overlay_elements=True' and 'screenshot=True'.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements overlay removal and screenshot functionality that matches the documented capabilities, even though it describes the relationship through AsyncWebCrawler's interface rather than directly.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides explicit examples of JSON configuration for nested objects, simple lists, nested lists, and lists of objects. These examples mirror the schema structure that JsonCssExtractionStrategy uses to extract structured data from HTML by processing a 'schema' with a 'fields' array.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship between JsonCssExtractionStrategy and JSON schema configuration for structured data extraction using nested fields and selectors, aligning with the ground truth's description of the BeautifulSoup-based implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy inherits from ExtractionStrategy, the abstract base class for extraction strategies. Although the documentation does not mention ExtractionStrategy explicitly, it underpins the design of extraction configurations, making it an implicit part of the trace chain.",
    "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class, but misses its core purpose of implementing different extraction patterns and incorrectly focuses on JsonCssExtractionStrategy which isn't mentioned in the code or documentation.",
      "error_type": "incomplete_and_irrelevant_focus"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls the arun() method on a crawler instance with parameters such as page_timeout, delay_before_return_html, and wait_for. This explicit call controls how the page loads and waits for dynamic content before capturing HTML.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles page loading and waiting controls, but incorrectly suggests these parameters are used directly in arun() when they are actually passed through **kwargs to crawler_strategy.crawl()",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The variable 'crawler' used in the snippet is an instance of AsyncWebCrawler. Although not directly referenced by name in the snippet, its use is implicit because arun() is a method of this class.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the documented timeout and waiting functionality through its **kwargs parameter, which accepts page_timeout, delay_before_return_html, and wait_for settings that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship with AsyncWebCrawler but misses the core functionality about timeout and waiting parameters being implemented through kwargs. It focuses only on class instantiation rather than the actual timeout/waiting feature relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Inside AsyncWebCrawler.arun(), the crawling operation is delegated to the crawler_strategy, which by default is an instance of AsyncPlaywrightCrawlerStrategy. This strategy processes parameters like page_timeout and wait_for to control page load behavior and dynamic content waiting.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the core timeout and waiting functionality is handled through the crawler strategy, even though it doesn't mention specific method names. The high-level relationship between the documented parameters and their implementation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract crawling interface (including methods to manage waiting and timeouts). This base class underpins the overall crawling mechanism even though it is not directly invoked in the example.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that AsyncCrawlerStrategy is an abstract base class that provides the foundation for crawling functionality with support for timeout and waiting capabilities through its interface methods. The predicted description adds detail about AsyncPlaywrightCrawlerStrategy but doesn't contradict the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an example where AsyncWebCrawler is instantiated with a 'proxy_config' argument. This directly demonstrates how the class is used to configure an authenticated proxy.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles proxy configuration, but misses the key detail that the proxy_config is passed through to the underlying AsyncPlaywrightCrawlerStrategy rather than being handled directly by AsyncWebCrawler.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "After creating an AsyncWebCrawler instance, the example calls its 'arun()' method to perform crawling, indicating direct usage of this method to initiate the crawl over the authenticated proxy.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions agree that the arun() method works with proxy configuration and is used to perform crawling with authenticated proxy support, even though the predicted text is less detailed about the implementation specifics",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy when no alternative is provided. This strategy processes the 'proxy_config' parameter to set up authenticated proxy settings, thereby supporting the functionality demonstrated in the snippet.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that authenticated proxy support exists but incorrectly implies AsyncPlaywrightCrawlerStrategy is a default strategy and doesn't clearly explain that the proxy authentication is implemented through ProxySettings during browser initialization.",
      "error_type": "inaccurate_implementation_details"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy to implement the required crawling methods. This establishes the abstract contract that underlies the proxy configuration functionality used indirectly by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions portray AsyncCrawlerStrategy as the abstract interface that enables proxy-based web crawling functionality. The predicted description accurately captures the core relationship, even though it mentions AsyncPlaywrightCrawlerStrategy which isn't explicitly in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'LLMExtractionStrategy' as the component used for extracting structured data from web pages using language models.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality that LLMExtractionStrategy uses language models to extract structured data from web pages, which matches the ground truth's explanation of the code's purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation mentions 'AsyncWebCrawler' as the asynchronous framework that enables the use of LLM extraction strategies.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role in LLM extraction but omits its core functionality as an asynchronous web crawler and oversimplifies it as just a 'framework'. The ground truth better captures its nature as a class implementing full web crawling with LLM extraction support.",
      "error_type": "oversimplification_and_omission"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the base class ExtractionStrategy, making ExtractionStrategy an implicit part of the extraction mechanism referenced in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy, but misses the crucial aspect of the parallel processing system and the core purpose of extracting structured data that is central to the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet directly instantiates CosineStrategy with parameters (semantic_filter, word_count_threshold, top_k, sim_threshold, max_dist) for different use cases such as article extraction, review analysis, and technical documentation. This clearly evidences an explicit reference to the CosineStrategy class.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between CosineStrategy and its configurable parameters for different use cases, aligning with the ground truth's explanation of how the class implements text extraction through cosine similarity with adjustable parameters for various scenarios.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is defined as a subclass of ExtractionStrategy, thereby inheriting its interface for extraction. This relationship is implicit in the design, ensuring that CosineStrategy conforms to the expected extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, but misses the crucial aspect of ExtractionStrategy's role in providing the foundation for different content extraction use cases and its parallel processing capabilities.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation usage example shows the crawling operation by calling 'crawler.arun()' with the extraction_strategy parameter set to an instance of CosineStrategy. This directly maps to the AsyncWebCrawler.arun() method responsible for handling such requests.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between arun() and extraction_strategy, but misses the crucial aspect that it implements multiple flexible configuration options shown in the use cases for different content extraction scenarios.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides a direct usage example by calling 'crawler.arun()' with the parameters 'simulate_user=True' and 'override_navigator=True'. This clearly indicates that the arun() method of AsyncWebCrawler is the entry-point for configuring anti-detection (anti-bot) features.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that arun() is the method for configuring anti-detection features through parameters. While using slightly different wording, both descriptions convey the same fundamental understanding that the method accepts anti-bot configuration options.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although 'AsyncPlaywrightCrawlerStrategy' is not directly called in the snippet, it is the default strategy used within AsyncWebCrawler. Its 'crawl' method processes the options 'simulate_user' and 'override_navigator' (along with 'magic') to trigger anti-detection behavior by injecting scripts that mask automation signals.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core functionality of implementing anti-bot options through parameters like simulate_user and override_navigator that inject scripts for masking automation signals. The predicted description correctly identifies the relationship between these parameters and their anti-detection behavior.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy serves as the abstract base class for crawler strategies. AsyncPlaywrightCrawlerStrategy extends this base class and inherits the public interface used to configure options like 'simulate_user' and 'override_navigator'.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy is an abstract interface defining methods that enable anti-bot features through configurable parameters. While the predicted description is less detailed, it correctly identifies the base class relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This clearly indicates that the AsyncWebCrawler class is directly used to enable verbose logging and manage the crawling session.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly convey that the verbose parameter in AsyncWebCrawler enables detailed logging functionality. While the predicted description uses a code example to illustrate this, and the ground truth provides more implementation details about the specific logging points, they both capture the same core relationship between verbose mode and logging capability.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, after instantiating AsyncWebCrawler, the arun() method is called with a URL parameter. This shows that the arun() method is used to perform the crawl operation, and it is an integral part of the public API as demonstrated in the example.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the existence and usage pattern of the arun() method as part of the public API, while the ground truth specifically describes the verbose logging functionality and how it enables detailed logging output when verbose=True. These are fundamentally different aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet shows a schema definition with keys like 'baseSelector' and 'fields', which directly mirrors the input expected by JsonCssExtractionStrategy. This class is designed to extract structured data using a provided JSON schema, and the advanced features (nested objects, lists, etc.) described in the snippet are exactly the capabilities implemented by this class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy uses a JSON schema with selectors to extract structured data, which aligns with the ground truth's description of using BeautifulSoup and CSS selectors for schema-based extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a concrete implementation that extends the abstract ExtractionStrategy, which defines the overall contract for extraction methods. Although not directly mentioned in the schema snippet, this inheritance chain is fundamental to how schema-based extraction is performed.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class, but erroneously focuses on JsonCssExtractionStrategy which isn't shown in the code. It misses describing the core schema-based extraction functionality mentioned in the ground truth.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates the JsonCssExtractionStrategy with a schema for pattern\u2010based content extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that JsonCssExtractionStrategy uses a schema for pattern-based content extraction, even though it doesn't detail the nested extraction functionality",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, thereby implementing its abstract extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the inheritance relationship but misses crucial aspects about the base class's core purpose in defining extraction interfaces and parallel processing capabilities",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet makes a call to crawler.arun() to perform the crawl with the extraction strategy, thus using the asynchronous method defined in AsyncWebCrawler.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the usage of crawler.arun() but misses the core functionality of processing the extraction strategy schema to extract structured data according to defined selectors",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows a code example that instantiates LLMExtractionStrategy with the parameters 'provider', 'api_token', and 'instruction'. This clearly demonstrates its direct use for customizing the LLM provider in Crawl4AI.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main relationship showing how LLMExtractionStrategy allows customization of LLM providers through the constructor parameters. Both descriptions emphasize the same core functionality of provider customization through provider name and API token parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the base class for LLMExtractionStrategy makes it implicitly relevant to the extraction customization mechanism.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the fundamental relationship between ExtractionStrategy and LLMExtractionStrategy, and this aligns with the ground truth's explanation of ExtractionStrategy serving as the foundation for implementing custom LLM providers.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows a call to 'crawler.arun(...)' with parameters for domain-based filtering. This directly maps to the AsyncWebCrawler.arun() method which is responsible for initiating the crawl while accepting custom keyword arguments such as 'exclude_domains', 'exclude_social_media_domains', and 'exclude_social_media_links'.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method accepts domain filtering parameters through **kwargs which aligns with the ground truth's explanation that these parameters get passed to crawler_strategy.crawl() for domain-based content control.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code example uses the variable 'crawler', implying that it is an instance of the AsyncWebCrawler class. Although the class name isn\u2019t explicitly mentioned in the snippet, the method call 'crawler.arun(...)' indicates that AsyncWebCrawler is the container class for the invoked method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the syntactic relationship of 'crawler' being an instance of AsyncWebCrawler, while missing the main functional relationship described in the ground truth about domain-based filtering capabilities.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Inside AsyncWebCrawler.arun(), the crawl operation is delegated to its crawler strategy, which by default is an instance of AsyncPlaywrightCrawlerStrategy. This strategy receives the extra keyword arguments including the domain filtering parameters and processes the crawl accordingly.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that the AsyncPlaywrightCrawlerStrategy handles domain-based filtering during crawling. While the predicted description mentions this happening through arun() delegation, and the ground truth focuses more on the parameters, they both convey the same fundamental relationship of the strategy implementing domain filtering functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is a concrete implementation that extends the abstract AsyncCrawlerStrategy. This base class defines the overall crawling interface, making it an integral part of the crawl execution chain even though domain filtering is handled downstream.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class defining the crawling interface, but misses the key point about it supporting domain-based filtering through its parameters. The mention of 'domain filtering is handled downstream' contradicts the ground truth which states the class directly provides this functionality.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'NlpSentenceChunking' class and shows its usage example by importing it, creating an instance, and calling its 'chunk' method. This directly corresponds to the implementation provided in the available artifacts.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the class and its basic usage pattern, it doesn't highlight the key relationship that NLTK's sent_tokenize is used for sentence boundary detection, which is a crucial aspect mentioned in the ground truth.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'NlpSentenceChunking' class extends the abstract base class 'ChunkingStrategy'. Although not explicitly mentioned in the documentation snippet, this dependency is inherent in its implementation as it conforms to the public interface defined by 'ChunkingStrategy'.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core inheritance relationship between ChunkingStrategy and NlpSentenceChunking, correctly indicating that NlpSentenceChunking extends and implements the abstract base class's interface.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'FixedLengthWordChunking' class and shows its usage in the example. The code sample imports and instantiates this class with a customizable 'chunk_size', directly demonstrating its purpose of splitting text into fixed-length word chunks.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the class existence and example usage but omits the core implementation detail of how the chunking actually works through word tokenization and list slicing",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, the 'FixedLengthWordChunking' class extends the 'ChunkingStrategy' class. This implies that 'ChunkingStrategy' is a fundamental part of the implementation, providing the abstract interface for chunking strategies.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that FixedLengthWordChunking extends/implements ChunkingStrategy and understands the inheritance relationship between them. Both descriptions convey the same core relationship of ChunkingStrategy being the abstract base that FixedLengthWordChunking implements.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with different browser_type parameters (e.g., 'firefox', 'webkit', and default for Chromium). This demonstrates its use as the primary interface for crawling with browser selection.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler allows for different browser selections, but it incorrectly suggests that browser_type is passed directly to AsyncWebCrawler when it's actually configured through the crawler_strategy parameter using PlaywrightCrawlerStrategy.",
      "error_type": "implementation_mechanism_error"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls the 'arun()' method on the AsyncWebCrawler instance to perform the crawl operation. This method is central to executing the crawling process as demonstrated in the example.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the core method for crawling but misses the key relationship about how it enables browser selection through crawler_strategy",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler internally creates an instance of AsyncPlaywrightCrawlerStrategy (when no specific crawler_strategy is provided). This strategy uses the 'browser_type' parameter to determine the browser engine (Firefox, WebKit, or Chromium) to launch.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy handles browser selection through the browser_type parameter to launch Firefox, WebKit, or Chromium browsers. Though it mentions AsyncWebCrawler rather than focusing directly on AsyncPlaywrightCrawlerStrategy, the core functionality relationship is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the interface for crawling operations. AsyncPlaywrightCrawlerStrategy inherits from it, thereby implementing the browser selection logic as part of its crawling methods.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that AsyncCrawlerStrategy is an abstract base class defining the interface that different browser implementations must follow. The predicted description correctly identifies its role in defining crawling operations, which aligns with the ground truth's explanation of it being the interface for browser type implementations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a usage example directly invoking 'crawler.arun' with content filtering parameters such as word_count_threshold and various exclusion flags. This directly maps to the public method AsyncWebCrawler.arun(), which is designed to accept such parameters and handle content filtering as part of its input.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that arun() accepts content filtering parameters as documented. Both descriptions align on the basic functionality of the method accepting and processing these parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet does not specify the class by name, the variable 'crawler' used in the usage example is implicitly an instance of the AsyncWebCrawler class, which provides the 'arun()' method. This indirect reference implies that AsyncWebCrawler is the underlying class facilitating the content filtering functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler class handles content filtering through its arun method, even though it focuses more on the class relationship than the specific filtering parameters. The core functionality relationship is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows usage of the 'arun' method on the crawler instance with the 'js_code' parameter. This call explicitly passes custom JavaScript commands (both as a single string and as a list) to be executed during the crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method's JavaScript execution capability, but incorrectly suggests that js_code is a direct parameter when it's actually handled through **kwargs and passed to crawler_strategy.crawl()",
      "error_type": "parameter_misunderstanding"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within its 'crawl' method, AsyncPlaywrightCrawlerStrategy retrieves the 'js_code' argument (or its alias 'js') and executes it using page.evaluate. Although not directly invoked in the snippet, it is the underlying strategy that processes the custom JavaScript commands provided in the arun() call.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship that the code handles JavaScript execution through the crawl() method by accepting js_code parameter and using page.evaluate() to execute the commands. The predicted description identifies the key functionality and implementation mechanism.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, which implements custom JavaScript execution, extends the abstract base class AsyncCrawlerStrategy. This base class defines the contract for crawling operations, thus indirectly supporting the execution of custom JavaScript.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawling, but incorrectly attributes JavaScript execution to a non-existent AsyncPlaywrightCrawlerStrategy rather than the base class itself",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler ('from crawl4ai import AsyncWebCrawler') and instantiates it using 'async with AsyncWebCrawler() as crawler:'. This shows a direct usage of the AsyncWebCrawler class for initiating a crawl.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that AsyncWebCrawler is used as shown in the documentation through async context management and crawling functionality. While it doesn't mention all implementation details like arun method specifics, it captures the essential usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the method arun() is directly called on the AsyncWebCrawler instance ('result = await crawler.arun(url=\"https://example.com\")'). This indicates that the crawl operation is executed via the arun() method.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called on the AsyncWebCrawler instance, but misses crucial aspects about what the method actually does (content extraction, caching, returning CrawlResult object)",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After crawling, the result is used to access the 'markdown' attribute ('print(result.markdown)'). This attribute, defined in CrawlResult, provides the cleaned markdown content of the crawled webpage.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the markdown attribute is accessed from the result object after crawling to get the cleaned markdown content, which aligns with the ground truth's explanation of the relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a call to 'crawler.arun()' with parameters simulate_user=True, override_navigator=True, and magic=True. This call directly maps to the 'arun()' method of the AsyncWebCrawler class, which is responsible for initiating a crawl with anti-detection features enabled.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles anti-detection features through keyword arguments passed to crawler_strategy.crawl(), even though it describes this from the usage perspective rather than the implementation perspective.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned in the snippet, AsyncWebCrawler uses the AsyncPlaywrightCrawlerStrategy as its default crawling strategy. Its 'crawl' method processes parameters such as simulate_user, override_navigator, and magic to inject stealth features (e.g., overriding navigator properties and simulating user interactions) that help avoid bot detection.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality - that the class implements anti-detection features by injecting stealth features and simulating user behavior when certain parameters are set. While it phrases things slightly differently, it describes the same essential relationship and purpose as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which serves as the abstract interface for all crawler strategies. This base class defines the contract (including the crawl method) that is implemented by AsyncPlaywrightCrawlerStrategy to support anti-detection features.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract interface class, but misses the key focus on anti-detection features and methods that enable stealth capabilities, which is the core relationship described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly states that 'Crawl4AI's AsyncWebCrawler class supports session-based crawling' through the use of the session_id parameter. This directly maps to the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the AsyncWebCrawler class supports session-based crawling through the session_id parameter, which matches the ground truth's explanation of how the class implements session management for persistent browser sessions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation instructs to use the same session_id across multiple 'arun' calls to maintain state, which clearly references the arun() method in AsyncWebCrawler.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that the arun() method enables session-based crawling through the session_id parameter to maintain state across requests. The predicted description accurately reflects the functionality shown in the code and described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly named in the documentation snippet, this class is the default crawler strategy used by AsyncWebCrawler. It implements key features like JavaScript execution via the js_code parameter, which supports the interactive crawling described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the class as implementing JavaScript execution functionality, it misses the core aspect of session-based crawling management which is central to the ground truth description. The prediction focuses on JavaScript execution while the ground truth emphasizes session management.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that AsyncPlaywrightCrawlerStrategy extends. It provides the interface for methods (like crawl) that support session-based crawling, forming an essential part of the inheritance chain.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncCrawlerStrategy is an abstract base class defining core crawling functionality through abstract methods, supporting session-based crawling capabilities. The predicted description, while more concise, correctly identifies its role in the inheritance structure and core purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy and shows its instantiation with parameters such as provider, schema, and instruction to configure LLM-based extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of LLMExtractionStrategy - that it's used for configuring and performing LLM-based extraction with parameters like provider, schema, and instruction. This aligns with the ground truth's explanation of structured content extraction using LLM completions.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet demonstrates the use of the 'arun' method, invoked via the crawler object, to perform a crawl operation while passing the previously instantiated extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() accepts an extraction strategy, but misses the key functionality of LLM-based extraction and content processing according to schemas/instructions",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl operation, the extracted LLM-processed content is accessed via the 'extracted_content' attribute of the CrawlResult, as shown by the json.loads() call.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea that extracted_content holds LLM-processed content that can be accessed via json.loads(), but misses the crucial aspect that it specifically matches a defined Pydantic schema for typed objects",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy, meaning that the extraction functionality is backed by the abstraction defined in ExtractionStrategy. This relationship is inherent in the design, although not directly mentioned in the usage example.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy and captures the inheritance relationship between them, which aligns with the ground truth's explanation of ExtractionStrategy being the foundation for specialized extractors like LLMExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and instantiates it with a schema and verbose parameter. This shows that the advanced schema extraction involves using this class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy uses a schema for extraction, but misses the core functionality of using BeautifulSoup for HTML parsing and structured data extraction according to field definitions",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the base class underpins the advanced extraction functionality used.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as a subclass of ExtractionStrategy and acknowledges the base/derived class relationship, which aligns with the ground truth's description of ExtractionStrategy providing the foundation for specialized strategies like JsonCssExtractionStrategy",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly imports AsyncWebCrawler and uses it within an asynchronous context (async with), indicating its role as the primary interface for performing web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the main interface for web crawling and notes its asynchronous nature, but omits crucial functionality around caching and the JsonCssExtractionStrategy support that are core to the class's purpose according to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The crawl operation is performed by calling the arun() method on the AsyncWebCrawler instance. This method executes the crawl and returns a result representing the page's content and extraction details.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic crawling functionality but omits key aspects mentioned in ground truth like handling extraction strategies, caching, and enabling JSON data extraction workflow.",
      "error_type": "important_omissions"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result returned from the arun() method is a CrawlResult object. The snippet uses properties of this object (like success and extracted_content) to determine the crawl outcome.",
    "ground_truth_relationship": "The CrawlResult class defines the structure that stores the extracted_content field used in the documentation example to validate successful crawling and store the parsed JSON product data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that CrawlResult is used to store crawled data (including extracted_content) and validate successful outcomes. The predicted description correctly identifies the key relationship between CrawlResult and the arun() method's return value.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks 'result.success' to assert that the crawl operation was successful. This attribute is integral to the public interface of CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that result.success is a boolean property used to verify successful completion of the crawl operation before proceeding with data processing",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses 'result.extracted_content' to load the structured JSON product data. This attribute is part of the CrawlResult interface that provides the extracted content.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies extracted_content as part of CrawlResult but misses the crucial detail that it's a raw string requiring JSON parsing, not directly structured data",
      "error_type": "omission_of_key_detail"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls the 'arun()' method on the crawler object with screenshot parameters (screenshot=True and screenshot_wait_for=2.0). This demonstrates its role in initiating the crawl process with screenshot capture enabled.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the arun() method handles screenshot functionality by taking a screenshot parameter and incorporating the screenshot data into its results. The predicted description demonstrates this through an example while the ground truth explains the implementation, but they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The snippet checks 'if result.screenshot:', clearly relying on the 'screenshot' attribute of the CrawlResult object to confirm that a screenshot has been captured and returned as a Base64 encoded string.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the screenshot attribute stores a base64-encoded image of the captured webpage that can be decoded",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned in the snippet, the underlying screenshot capture functionality is implemented by AsyncPlaywrightCrawlerStrategy via its 'take_screenshot' method. This class is implicitly used when the 'screenshot' parameter is set to True in the arun() method.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements screenshot functionality through the take_screenshot method when the screenshot parameter is True. While it doesn't mention the error handling and black error image generation, these are minor implementation details that don't affect the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of CosineStrategy with parameters that match its __init__ signature. The provided configuration options such as 'semantic_filter', 'word_count_threshold', 'sim_threshold', 'max_dist', 'linkage_method', 'top_k', 'model_name', and 'verbose' are directly defined in the CosineStrategy class.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation parameters match the CosineStrategy class implementation and its __init__ signature, including all key parameters and their default values.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is implemented as a subclass of ExtractionStrategy, meaning it inherits base functionality and structure from ExtractionStrategy. Although the base class is not explicitly mentioned in the snippet, it is an implicit part of the chain since CosineStrategy extends it.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the core relationship that CosineStrategy inherits from/extends ExtractionStrategy as a base class that provides configuration through kwargs. The predicted description accurately captures this inheritance relationship, even if it doesn't detail all the configuration parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler with a 'headers' keyword argument to add security-related headers. This directly demonstrates how the class is used for configuring custom headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts custom headers through its instantiation and demonstrates this with an example, which aligns with the ground truth's explanation that headers are supported via **kwargs and passed to AsyncPlaywrightCrawlerStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet makes an explicit call to 'arun()' on the AsyncWebCrawler instance to perform a web crawl. This demonstrates the usage of the arun() method as part of the crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for web crawling, but misses the key relationship about custom headers being accepted through the crawler_strategy component, which is the main focus of the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncPlaywrightCrawlerStrategy is implicitly employed by AsyncWebCrawler (via its default crawler_strategy) to handle parameters such as custom headers. In its constructor, it sets self.headers from the provided kwargs, thus processing the custom headers configuration.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that the AsyncPlaywrightCrawlerStrategy handles custom headers through its initialization and setting mechanisms, allowing for configuration of headers like X-Forwarded-For and Cache-Control. While the predicted version is less detailed, it correctly identifies the core functionality and relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a session cleanup best practice by calling 'await crawler.crawler_strategy.kill_session(session_id)'. This call directly maps to the 'kill_session' method implemented in AsyncPlaywrightCrawlerStrategy, which is responsible for cleaning up resources associated with a session.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that kill_session() implements resource cleanup by closing browser resources (contexts and pages) to manage sessions properly, which aligns with the documented best practices.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes usage examples that call 'crawler.arun(...)' with parameters such as session_id, js_code, and wait_for. This indicates that the arun method of AsyncWebCrawler is used to manage the state of a session while performing a crawl operation.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() method handles session management functionality through its parameters, which aligns with the ground truth's explanation that it stores session_id to enable stateful crawling across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler (via 'from crawl4ai import AsyncWebCrawler' and 'async with AsyncWebCrawler(verbose=True) as crawler'), demonstrating its use as the central component for session-based crawling.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler is used for session-based crawling and shows its instantiation/usage pattern. While it's less detailed than the ground truth about specific methods, it conveys the same core relationship between the class and its session-based crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code sample explicitly calls the 'arun()' method on the AsyncWebCrawler instance to perform crawling with session parameters, JavaScript execution for content loading, and a CSS selector for content extraction.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the core relationship where arun() handles session-based crawling by accepting session parameters. The predicted description captures the main functionality including session handling, JavaScript execution, and CSS selector usage, which aligns with the ground truth's emphasis on session state management.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The sample demonstrates session management by closing an ongoing session through a call to 'kill_session(session_id)' on the crawler's strategy. Since AsyncWebCrawler instantiates AsyncPlaywrightCrawlerStrategy by default, this kill_session() method is the one being used.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session() manages session cleanup through the crawler strategy, which aligns with the ground truth's explanation of closing page/context objects and removing sessions when crawling is complete.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates JsonCssExtractionStrategy with a JSON schema for pattern-based extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy uses a schema for extraction, but misses the core pattern-based functionality of iterating through matching elements and extracting specified fields",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, meaning that the abstract extraction interface is utilized indirectly when JsonCssExtractionStrategy is instantiated.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends/inherits from ExtractionStrategy as a base class implementation, which aligns with the ground truth's description of ExtractionStrategy providing the core framework for derived classes like JsonCssExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example calls 'await crawler.arun(...)' to perform the crawl with the provided extraction strategy, indicating that the arun() method of AsyncWebCrawler is implicitly employed to execute the extraction process.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and extraction, but misses the key relationship that it specifically processes extraction_strategy implementations like JsonCssExtractionStrategy to extract structured data using CSS selectors",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After invoking arun(), the example accesses 'result.extracted_content' to retrieve the extracted product listings, thereby implicitly referencing the extracted_content attribute of CrawlResult.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that extracted_content is accessed for results, but misses the crucial aspect that it specifically stores JSON-formatted pattern matches based on the schema structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet demonstrates the initialization of the crawl session using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This clearly shows that AsyncWebCrawler is the entry point for setting up and managing the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted text correctly identifies AsyncWebCrawler as an important class for crawling, it focuses only on initialization syntax and misses the core relationship described in the ground truth - that AsyncWebCrawler supports the wait_for parameter through its arun method for handling page load conditions.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, 'await crawler.arun(...)' is used to perform the crawling operation with parameters such as 'wait_for', 'js_code', and others. This method is central to executing the crawl while handling dynamic content and extraction.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling with parameters including wait_for, but misses the key aspect of how wait_for is specifically implemented through kwargs to control page loading conditions via crawler_strategy.crawl()",
      "error_type": "incomplete_implementation_detail"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet creates an instance of JsonCssExtractionStrategy using a given schema designed to extract commit details. This shows that the extraction strategy for parsing HTML content is explicitly being set up by the user.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy uses a schema for extraction, but misses the crucial implementation detail that it uses BeautifulSoup to parse HTML elements according to the schema's selectors",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "In the example, after completing the crawl, the session is properly terminated by calling 'await crawler.crawler_strategy.kill_session(session_id)'. This ensures that session resources are cleaned up. The kill_session() method, implemented in AsyncPlaywrightCrawlerStrategy, is explicitly invoked.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that kill_session is used to clean up browser resources (page and context) after crawling is complete, and that it's called at the end of the crawling process",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates CosineStrategy with parameters (semantic_filter, word_count_threshold, sim_threshold) to set up a content extraction strategy.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of CosineStrategy and its parameters, but misses the key functionality aspect of it being used for web content extraction and clustering that is emphasized in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy inherits from ExtractionStrategy, indicating that the functionality used in the snippet is based on the abstract extraction behavior defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, which is the core aspect described in the ground truth. While it doesn't mention the parallel processing functionality, this omission doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet shows the creation of an AsyncWebCrawler instance using an async context manager, which initiates the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the async context manager aspect but omits the crucial functionality of executing crawls with extraction strategies, which is a key part of the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method is explicitly called on the AsyncWebCrawler instance to perform the asynchronous crawling while applying the provided extraction strategy.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic async crawling functionality but omits crucial aspects about processing thresholds, content extraction and returning CrawlResult with extracted content mentioned in ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation snippet accesses the 'extracted_content' field of the result (a CrawlResult object) to retrieve the processed content after crawling.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that extracted_content is accessed from the result object to get the processed content after crawling, which aligns with the ground truth's explanation of it holding the gathered text content after filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the async context manager (i.e. 'async with AsyncWebCrawler(verbose=True) as crawler:'), which shows its role as the primary crawler for session-based crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as handling session-based crawling, but misses the crucial aspect of supporting dynamic content crawling through JavaScript injection and wait conditions for pagination/updates, which is a core functionality described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun(...)' to initiate crawling for a given URL, demonstrating the direct usage of the arun method of AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality of arun() but misses the crucial aspect of enabling session-based dynamic crawling with JavaScript execution and content update handling that is core to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "Within the snippet, an extraction strategy is defined by instantiating JsonCssExtractionStrategy with a schema, used to extract commit information from dynamic content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of JsonCssExtractionStrategy using a schema to extract commit information from dynamic content, which aligns with the ground truth's explanation of the class implementing structured data extraction using schema components.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.crawler_strategy.kill_session(session_id)' to terminate a session after crawling a page, demonstrating session management within the crawling strategy.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of kill_session - terminating browser sessions after crawling - even though it doesn't explicitly mention Playwright page/context objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy as its crawler_strategy when no alternative is provided. Although not mentioned by name explicitly in the snippet, its methods (such as kill_session) are invoked.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy as a default strategy, but misses the core functionality of session-based crawling and dynamic content handling that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is derived from ExtractionStrategy. Its instantiation in the snippet implies the use of the extraction strategy pattern defined by the ExtractionStrategy base class.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface needed to support the document's dynamic content extraction example, particularly through its extract method that processes HTML blocks into structured data as shown in the documentation's schema-based extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class and mentions inheritance, but misses the key purpose of HTML processing and structured data extraction that's central to the ground truth relationship",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun()' with a 'wait_for' parameter (both 'css:' and 'js:' forms). This makes an explicit link to the 'AsyncWebCrawler.arun()' method, which is designed to accept parameters that control waiting for dynamic content.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method supports wait conditions through its parameters, and accurately links this to the documented CSS and JavaScript wait functionality. While it doesn't explicitly mention kwargs, the core relationship between the method and wait functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the documentation, the underlying mechanism for processing the 'wait_for' parameter is implemented in the 'crawl' method of AsyncPlaywrightCrawlerStrategy. This method interprets strings with prefixes 'css:' and 'js:', invoking wait logic (via smart_wait and csp_compliant_wait), which is what the documentation examples demonstrate.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through smart_wait method, handling both CSS and JavaScript-based waiting. It accurately describes the core functionality matching the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class defining the required crawler interface, including methods like 'crawl'. The AsyncPlaywrightCrawlerStrategy (which implements the waiting functionality) extends this base class, thereby making it a foundational part of the implementation chain for wait conditions.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that AsyncCrawlerStrategy is the base class that enables/supports wait functionality, with the predicted focusing on inheritance and the ground truth on the kwargs parameter. The core relationship is preserved.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun' with parameters such as 'word_count_threshold', 'exclude_external_links', 'exclude_external_images', and 'excluded_tags'. These parameters are passed to the AsyncWebCrawler.arun() method (artifact id: 5), which implements the core crawling and content filtering functionality.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the arun() method handles content filtering through parameters that control how the crawled content is processed. The predicted description mentions specific parameters while the ground truth provides a more general description, but they describe the same core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet uses the variable 'crawler' without explicitly stating its class, it is implicitly understood that 'crawler' is an instance of the AsyncWebCrawler class (artifact id: 4). This class contains the 'arun()' method, thereby linking the usage to the overall crawler implementation.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the code relates to the AsyncWebCrawler class and its arun() method, but misses the main point about the filtering options and parameters that are the focus of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it in an async context manager to instantiate a crawler object.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler usage with async context manager, but misses the crucial aspect of customizable extraction strategies and the main purpose of content extraction demonstrated in the example",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on the AsyncWebCrawler instance to perform the crawling process.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function of calling arun() for crawling, but misses the crucial aspect of content extraction and filtering based on specific criteria using LLMExtractionStrategy, which is a key focus of the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates LLMExtractionStrategy with parameters (provider, api_token, instruction) to extract technology related content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that LLMExtractionStrategy handles content extraction with specific parameters (provider, api_token, instruction) for targeted extraction, which aligns with the ground truth's explanation of how it processes HTML content through LLM models with specific instructions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the abstract base class ExtractionStrategy, thereby inheriting its interface for content extraction.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy inherits from ExtractionStrategy, but misses the crucial aspect of how this enables custom content extraction functionality demonstrated in the example code",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses the 'extracted_content' property on the crawl result, which holds the JSON content extracted by the strategy.",
    "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that extracted_content holds content extracted by the strategy, which aligns with the ground truth showing it stores filtered content that is later parsed as JSON.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly references 'JsonCssExtractionStrategy' as the strategy that extracts structured data from web pages using CSS selectors. This directly indicates that the documented functionality is implemented by this class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy implements functionality to extract structured data using CSS selectors. While it omits implementation details about BeautifulSoup and schema structure, it correctly identifies the main purpose and relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is implemented as a subclass of ExtractionStrategy. Even though ExtractionStrategy is not explicitly mentioned in the text, its role as the base abstraction for all extraction strategies implicitly underpins the documented functionality.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify ExtractionStrategy as the base class providing core functionality for extraction strategies, with the predicted description accurately noting its role as the foundational abstraction even if not explicitly mentioned in documentation",
      "error_type": "none"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation indicates how to use JsonCssExtractionStrategy with AsyncWebCrawler, thereby explicitly showing that AsyncWebCrawler is the consumer of this extraction approach in Crawl4AI.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler uses JsonCssExtractionStrategy as an extraction approach. While it's less specific than the ground truth about implementation details, it captures the core relationship between these components.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")'. This directly corresponds to the AsyncWebCrawler.arun() method, which is responsible for executing a crawl.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on calling arun() but misses the core error handling relationship described in the ground truth. However, it does correctly identify the arun() method being used.",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The method 'arun()' returns a CrawlResult object that encapsulates the outcome of the crawling operation. Although the documentation does not explicitly mention 'CrawlResult', its properties (success, error_message, status_code) are used in the snippet.",
    "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult object is returned by arun() and contains the key fields (success, error_message, status_code) needed for error handling, matching the ground truth's core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation code checks 'if not result.success:' to determine whether the crawl was successful. This directly uses the 'success' attribute of the CrawlResult class.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the success boolean is used to check if a crawl operation was successful, as shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet prints the error message by accessing 'result.error_message'. This directly ties to the error_message attribute defined in the CrawlResult class.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that error_message is an attribute used to display failure messages, which aligns with the ground truth's explanation of how it stores failure reasons for unsuccessful crawls.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "In the sample code, the status code of the crawl is printed using 'result.status_code', directly indicating the use of the CrawlResult.status_code attribute.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that status_code is used and accessed via result.status_code, but misses the crucial purpose of storing HTTP response codes for error handling/diagnosis",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions and imports LLMExtractionStrategy and shows its instantiation with parameters like provider, schema, and instruction. This indicates that LLMExtractionStrategy is directly used for LLM-based extraction.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies LLMExtractionStrategy's basic usage and parameters, it misses the crucial aspect of how it processes HTML content through language models and extracts structured data according to schemas/instructions",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the abstract base class for extraction strategies is implicit in the design.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of how ExtractionStrategy provides the infrastructure for content extraction through its abstract methods that enable LLMExtractionStrategy's functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls the arun() method on the crawler object with the extraction_strategy parameter set to an instance of LLMExtractionStrategy. This indicates that AsyncWebCrawler.arun() is the method responsible for integrating and utilizing the provided extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is responsible for integrating and utilizing the extraction strategy, which aligns with the ground truth's explanation that it handles the core crawling logic and passes content to the extraction strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates the usage of the arun() method by calling 'crawler.arun(url=\"https://example.com\")'. This shows that the method is explicitly invoked to initiate the crawling process and return crawl results.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() performs crawling, but misses the key media selection functionality described in the ground truth - specifically the ability to access different media types through the media dictionary.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code snippet accesses 'result.media' to retrieve lists of media (images, videos, audios). Although the attribute is not named explicitly in the text, its usage is implicit as part of the CrawlResult returned by arun(). This attribute holds the media details used by the documentation example.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that result.media provides access to categorized lists of media elements like images, videos, and audios, which matches the ground truth relationship. The minor difference in wording does not change the core understanding.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows the use of 'crawler.arun()' to initiate a crawl. This method is responsible for fetching the webpage and returning a CrawlResult that contains, among other things, media information.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the core crawling method that returns CrawlResult, but misses crucial functionality around lazy-loading, caching, and screenshot capabilities that are central to the method's media handling purpose.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code example iterates over 'result.media[\"images\"]', demonstrating that the media attribute of the CrawlResult object contains rich metadata for each media element such as source URL, alt text, description, context, and relevance score.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the media dictionary contains metadata about images including source, alt text, description, context, and relevance scores. The predicted description accurately captures the core functionality and data structure relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly referenced in the usage example, the 'AsyncPlaywrightCrawlerStrategy' is implicitly involved because 'AsyncWebCrawler.arun()' delegates its crawling task to an instance of this strategy. Its crawl method processes parameters like 'wait_for' and 'delay_before_return_html' to properly handle lazy-loaded content and ensure that media elements are fully rendered.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy handles lazy-loaded content through parameters like wait_for and delay_before_return_html, which aligns with the ground truth's explanation of using smart_wait() for handling lazy-loaded elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses 'async with AsyncWebCrawler() as crawler:' to start a crawling session. This directly instantiates the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the async context manager usage but misses the crucial functionality around extraction strategies and content processing that the class provides, which is a core part of the relationship shown in the documentation example.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet makes two calls to 'crawler.arun(...)' with different extraction strategies. This demonstrates explicit invocation of the arun() method on an AsyncWebCrawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() in the example but misses its core functionality as a crawler that processes URLs and returns structured results for different extraction types. It focuses only on the surface-level method calls rather than the underlying functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet directly instantiates JsonCssExtractionStrategy with an 'article_schema' to extract structured content from HTML.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy uses article_schema for structured HTML content extraction, which aligns with the ground truth's explanation of implementing structured extraction using CSS selectors and schema.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "An instance of LLMExtractionStrategy is created with parameters (provider, schema, instruction) to perform semantic analysis on the article content. This is explicitly shown in the snippet.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy being used for semantic analysis with configurable parameters (provider, schema, instruction), which aligns with how it's used in the documentation example",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both JsonCssExtractionStrategy and LLMExtractionStrategy extend the abstract ExtractionStrategy, meaning the extraction logic follows a common interface defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship correctly - that both JsonCssExtractionStrategy and LLMExtractionStrategy inherit from and follow the interface defined by the abstract ExtractionStrategy class. While it doesn't mention the parallel processing functionality, this omission doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses the 'extracted_content' attribute on the crawl results returned by arun() to obtain the structured and semantic extraction output.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that extracted_content is accessed from crawl results to obtain extraction output. While it's less detailed than the ground truth, it accurately represents the main relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The 'media' attribute of the CrawlResult is accessed to retrieve media-related data from the crawled page, as is evident by the use of 'pattern_result.media' in the snippet.",
    "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that the media property from CrawlResult is used to store and return media-related data from crawled content, capturing the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(url=\"https://example.com\")', thereby explicitly demonstrating the usage of the AsyncWebCrawler.arun() method which returns a CrawlResult object.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() returns a CrawlResult object, but misses the crucial aspect of how the method processes the webpage content through extraction/chunking strategies to populate the CrawlResult's properties. It only focuses on demonstrating the usage rather than explaining the core functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation explicitly refers to a CrawlResult object (with a link [CrawlResult](../api/crawl-result.md)) as the return type of the arun() method.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that a CrawlResult object is returned by arun() method, which aligns with the ground truth showing CrawlResult class definition containing all the properties demonstrated in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The code sample prints 'result.html' to display the raw HTML, directly indicating that the 'html' attribute is part of the CrawlResult public interface.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that 'html' is a property/attribute of CrawlResult that contains raw HTML content. The predicted description captures the core relationship even though it focuses on the print statement example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet prints 'result.cleaned_html' to show the cleaned HTML content, indicating direct usage of this attribute from the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on printing/displaying cleaned_html, while the ground truth correctly identifies it as an optional string property of the CrawlResult class. The predicted misses the core nature of the attribute.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation explicitly prints 'result.markdown' to illustrate the markdown version of the crawled content, demonstrating its role as a public attribute of CrawlResult.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown property is part of CrawlResult and represents the Markdown version of the crawled content, accessible via result.markdown",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The code sample accesses 'result.fit_markdown' to retrieve the most relevant markdown content, thereby directly implying its inclusion in the CrawlResult class.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown contains the most relevant markdown content, which aligns with the ground truth's explanation of it being an optional string with the most relevant content in markdown format.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The sample code prints 'result.success' to indicate whether the crawl was successful, directly utilizing this public attribute from CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that result.success is a boolean field indicating whether a crawl operation was successful, with the predicted description accurately reflecting the core relationship shown in the documentation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The snippet prints 'result.status_code' to display the HTTP status code (e.g., 200, 404), thus it is used directly from the CrawlResult interface.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that status_code is used to display HTTP status codes from the CrawlResult interface, which aligns with the ground truth's explanation of it being a property that stores HTTP status codes for verifying crawl success",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation sample includes a call to 'result.media' to retrieve media elements (images, videos, audio) from the crawl response, highlighting its role as an accessible attribute.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies media as containing media elements, but describes it from the perspective of retrieving/accessing the media (result.media) rather than the ground truth's focus on it being a dictionary property for storing media elements.",
      "error_type": "perspective_mismatch"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The sample code prints 'result.links' to illustrate the dictionary of internal and external links extracted during crawling, showing direct usage of this attribute.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on printing/showing the links dictionary, while the ground truth describes the actual implementation of the links dictionary property. However, both correctly identify that it stores links from crawling.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of CosineStrategy using parameters such as 'linkage_method', 'max_dist', and 'model_name' in the 'Custom Clustering' section. This demonstrates its direct usage in configuring a custom clustering strategy.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy's clustering configuration capability but misses its key content filtering and semantic similarity features that are crucial to its overall functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, it forms the base class for CosineStrategy, thereby indirectly contributing to the behavior demonstrated.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is a subclass of ExtractionStrategy and captures the core inheritance relationship. While it's more concise than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet shows an asynchronous context manager 'async with AsyncWebCrawler() as crawler:' which explicitly creates an instance of AsyncWebCrawler for crawling purposes.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as an async context manager but fails to mention its core functionality for content filtering, extraction strategies, caching and clustering support which are central aspects highlighted in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the context of AsyncWebCrawler, the method arun() is explicitly called to perform the crawling operation with an extraction strategy. This is shown in the line 'result = await crawler.arun(url=url, extraction_strategy=strategy)' in the snippet.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling with an extraction strategy, but misses crucial aspects like custom clustering configurations, content filtering pipeline handling, and the broader role in managing the asynchronous execution engine described in the ground truth.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the snippet uses 'result.extracted_content' to access the JSON content extracted by the crawling process. This indicates reliance on the public interface of CrawlResult for accessing extracted content data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the CrawlResult, but misses the crucial aspect that it specifically contains JSON-serialized clustering and pricing feature data with similarity scores",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks 'result.success' to verify whether crawling was successful. This direct reference to the success attribute of CrawlResult confirms its role as part of the public interface utilized in the content filtering pipeline.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that result.success is used as a boolean check to validate successful extraction before processing results. The predicted description correctly identifies its role in the content filtering pipeline.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using an async context manager. This demonstrates its role in managing crawling sessions and handling setup/teardown operations.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship showing AsyncWebCrawler's compatibility with async context managers for setup/teardown, which aligns with the ground truth's explanation of the __aenter__ and __aexit__ implementations supporting this pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates instantiating AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:', thereby directly invoking this class as the entry point for Magic Mode operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as handling Magic Mode but misses that it's implemented through the crawler_strategy parameter and AsyncPlaywrightCrawlerStrategy. It focuses on the instantiation syntax rather than the actual mechanism.",
      "error_type": "incomplete_mechanics"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example in the documentation calls 'await crawler.arun(url=\"https://example.com\", magic=True)', explicitly invoking the arun() method of AsyncWebCrawler to trigger Magic Mode features.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method is called to enable Magic Mode features. While it doesn't explicitly mention the magic parameter being passed through kwargs, this is a minor implementation detail that doesn't change the core relationship between the method and its anti-bot functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Internally, when no crawler strategy is provided, AsyncWebCrawler defaults to using an instance of AsyncPlaywrightCrawlerStrategy. Its crawl() method checks for the 'magic' flag (among others) to inject anti-detection scripts, effectively enabling Magic Mode.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that Magic Mode is implemented through the magic=True parameter in the crawl method and includes anti-bot features like stealth configurations and navigator property overrides. The predicted description correctly explains the internal mechanism while the ground truth focuses on the implementation details, but they describe the same core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract interface for crawler strategies. This inheritance relationship underpins the implementation of Magic Mode features in the concrete strategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining an interface for crawler strategies, but incorrectly focuses on AsyncPlaywrightCrawlerStrategy inheritance which isn't mentioned in the ground truth. The core purpose related to Magic Mode's anti-bot features is somewhat obscured.",
      "error_type": "focus_shift_and_omission"
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation snippet uses 'result.media' to process video and audio elements. Although the snippet does not name 'CrawlResult' explicitly, 'result' is inferred to be an instance of CrawlResult, which provides the public interface that includes the media attribute.",
    "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core relationship - that CrawlResult's media dictionary field stores video and audio metadata that can be accessed as shown in the documentation. The predicted description correctly infers the relationship between the class and the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code snippet directly references 'result.media' to access media details such as videos and audios, demonstrating the explicit usage of the 'media' attribute defined in CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that media is a dictionary containing video and audio metadata that can be accessed and processed. The predicted description describes the same high-level relationship as the ground truth, even if it doesn't include all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet presents a CSS strategy for product listings by showing a schema with a 'baseSelector'. Although the class name is not explicitly mentioned, JsonCssExtractionStrategy is designed to extract data based on CSS selectors and a provided schema, making it an implicit match for this use case.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that JsonCssExtractionStrategy implements CSS-based data extraction using a schema with selectors, matching the e-commerce scraping example from the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates LLMExtractionStrategy for news article extraction, demonstrating its function in handling content via an LLM model and utilizing a schema defined from a BaseModel.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that LLMExtractionStrategy implements schema-based article extraction using LLM models, with the predicted description accurately capturing the core functionality shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation provides an explicit example where CosineStrategy is used for content analysis through cosine similarity, making it a direct example of its use for topic analysis.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for content analysis through cosine similarity, which aligns with the ground truth's explanation of how it clusters and analyzes text content using cosine similarity and semantic filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True)' to initiate a crawl session that supports custom execution hooks.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler instantiation and usage with 'async with', but it focuses only on demonstrating hook usage rather than explaining the crawler_strategy's fundamental role in enabling hook functionality that the ground truth emphasizes.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example explicitly calls the arun() method on the AsyncWebCrawler instance to perform the crawling operation after the custom hook is set, ensuring content is retrieved according to the defined behavior.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() is called to perform crawling operations with custom hooks, while the ground truth expands on how arun() enables this through crawler_strategy. The core relationship and functionality is consistently described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The snippet directly uses the set_hook() method to assign a custom 'on_execution_started' hook that waits for new content, thereby customizing the execution flow.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of the set_hook method - it enables setting custom hooks that get executed at specific points during crawling, as described in the ground truth. Both descriptions emphasize the hook's role in customizing execution flow.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example explicitly calls kill_session() on the crawler strategy to terminate the session identified by 'commit_session', ensuring resources are properly released after crawling.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of kill_session() as a cleanup method that terminates and cleans up browser resources for a specific session. The predicted description accurately reflects the relationship shown in the ground truth, even if it doesn't detail every specific cleanup action.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the snippet, AsyncWebCrawler initializes its crawler_strategy with AsyncPlaywrightCrawlerStrategy by default, which provides the implementations for set_hook() and kill_session() used in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements hooks functionality, but incorrectly states this is provided by AsyncWebCrawler by default rather than being implemented directly in the AsyncPlaywrightCrawlerStrategy class.",
      "error_type": "incorrect_class_attribution"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, meaning that the foundational behavior for custom hook support is inherited from AsyncCrawlerStrategy even though it is not directly referenced in the snippet.",
    "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted text correctly identifies AsyncCrawlerStrategy's role in hook functionality, it incorrectly focuses on inheritance relationships with AsyncPlaywrightCrawlerStrategy which isn't mentioned in the ground truth. The core hook registration functionality is present but obscured by this incorrect focus.",
      "error_type": "incorrect_focus_and_scope"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using a context manager with proxy and headers parameters. This directly demonstrates its intended usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly explains that AsyncWebCrawler can be used with proxy and headers parameters through a context manager. While it doesn't mention the kwargs implementation detail from the ground truth, it accurately describes the high-level usage pattern shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance, with the parameter 'magic=True' enabling all anti-detection features (Magic Mode).",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method implements the Magic Mode functionality through parameters, even though it doesn't explicitly mention proxies. The core relationship between the code and documentation is accurately captured.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Even though not mentioned directly in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler_strategy. The 'magic=True' argument eventually propagates to its internal crawl method, triggering anti-detection measures.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on magic mode and AsyncWebCrawler's usage of AsyncPlaywrightCrawlerStrategy, while the ground truth specifically focuses on the proxy configuration implementation. While both discuss anti-detection features, they emphasize different aspects of the relationship.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, meaning that the anti-detection and proxy functionalities\u2014enabled via the 'magic' parameter\u2014are part of the broader crawling strategy abstraction.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that AsyncCrawlerStrategy is an abstract base class that enables advanced crawling features like Magic Mode and proxy support through its abstract methods. The predicted description focuses on inheritance while the ground truth focuses on the abstract methods, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager. This shows that the class is used as the main crawler to perform operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the main crawler class used via async context manager, which is correct, but misses the crucial functionality around multi-format extraction and processing that's central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows multiple calls to the arun() method on the AsyncWebCrawler instance to perform crawling and extraction. This method retrieves the full result from the crawling operation.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() performs crawling operations, but misses the key aspect that the ground truth emphasizes - the ability to use different extraction strategies for different purposes. The prediction focuses only on general crawling while omitting this crucial functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly instantiates LLMExtractionStrategy with parameters including provider, schema, and instruction. This strategy is used to extract structured data using an LLM.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as a way to extract structured data using LLM with parameters like provider, schema, and instruction, which aligns with how it's used in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of JsonCssExtractionStrategy (with a provided schema) to extract repeated patterns from the content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for extracting repeated patterns using a schema, which aligns with the ground truth's explanation of using CSS selectors for structured data extraction from repeated HTML elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation accesses the 'fit_markdown' attribute of the crawl result to obtain the formatted main content extracted from the page.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that fit_markdown is used to access/store the main extracted content from a webpage as part of the crawl result",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code snippet accesses 'extracted_content' from the results (for both LLM and pattern extractions) and processes it using json.loads, indicating its role in holding structured extraction data.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that extracted_content stores crawled data as a string that gets parsed as JSON, and both mention its use with LLM and pattern-based extractions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet returns the 'media' attribute from the crawl result to provide associated media data, indicating its utility in handling multiple output formats.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions indicate that media data is returned as part of the function output, with the predicted description correctly capturing the basic relationship of media being a returned attribute from the crawl result.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both LLMExtractionStrategy and JsonCssExtractionStrategy extend ExtractionStrategy. Although not mentioned directly in the snippet, this base class underlies the extraction process and defines the interface used by the strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that ExtractionStrategy is the base class that underlies different extraction strategies (LLMExtractionStrategy and JsonCssExtractionStrategy) and defines their interface. The predicted description captures the same core relationship as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the code snippet, the crawl call is made using 'crawler.arun(url=...)'. Although the documentation does not explicitly name the class, this usage implies that the arun() method (defined in the AsyncWebCrawler class) is invoked to perform the crawl and return a CrawlResult. This method forms an implicit part of the example's workflow.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies the async crawling functionality but misses the crucial aspect of markdown extraction and content fitting that is central to the ground truth's description of the relationship between the code and documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation sample explicitly accesses 'result.fit_markdown' after calling arun(). This attribute is part of the CrawlResult public model (as defined in artifact 12) and is used to deliver the main content in markdown format, corresponding exactly to what the documentation intends to showcase.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown is an attribute containing markdown-formatted main content, matching the ground truth's core meaning of storing cleaned, extracted content in markdown format.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (async with AsyncWebCrawler(headless=True) as crawler), showing its direct use in handling protected sites.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the async context manager usage pattern, but misses the core functionality of handling protected sites through the specific parameters and features described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls crawler.arun(...) with parameters such as magic=True and remove_overlay_elements=True. This directly demonstrates the usage of the arun() method on the AsyncWebCrawler instance.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses crucial aspects of the implementation like error handling, caching, and customizable parameters that enable protected site crawling features.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy when no crawler_strategy is provided. Thus, when arun() is called with parameters like magic=True, the underlying crawling logic implemented in AsyncPlaywrightCrawlerStrategy is implicitly used.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection to AsyncWebCrawler but focuses on implementation inheritance rather than the core functionality of handling protected site crawling described in the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the crawling contract, which is implemented by AsyncPlaywrightCrawlerStrategy. This relationship is indirectly relevant because AsyncPlaywrightCrawlerStrategy (used implicitly) extends AsyncCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables implementations like the documented crawl_protected_site function to perform automated web crawling with customizable behavior for handling protected sites through methods like crawl() and set_hook().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawling, but focuses too heavily on an implementation detail (AsyncPlaywrightCrawlerStrategy) that isn't mentioned in the code/docs, while missing the key relationship with protected site handling shown in the documentation.",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'SlidingWindowChunking' class, demonstrates its instantiation with parameters (window_size and step), and shows its method 'chunk()' being used. This directly maps to the implementation provided in the SlidingWindowChunking artifact.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on superficial class/method presence and parameters but misses the core algorithm relationship - how sliding windows create overlapping chunks for context preservation",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although the documentation does not mention 'ChunkingStrategy' by name, the 'SlidingWindowChunking' class extends 'ChunkingStrategy'. This makes ChunkingStrategy an integral part of the implementation chain that underlies the sliding window chunking functionality.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that SlidingWindowChunking is an implementation of ChunkingStrategy, though it uses different wording. Both descriptions convey the core inheritance/implementation relationship between the abstract base class and its concrete implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly names RegexChunking in the header and demonstrates its usage in the example code where it is imported, instantiated with a 'patterns' parameter, and its method 'chunk' is called to split text.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies RegexChunking and its basic purpose of text splitting, but doesn't capture the key implementation detail of iterative pattern-based splitting using re.split() that the ground truth emphasizes",
      "error_type": "incomplete_core_mechanics"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "RegexChunking is implemented as a subclass of ChunkingStrategy. Although the snippet does not directly mention ChunkingStrategy, RegexChunking inherits from it, making it an integral part of the functionality chain.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that RegexChunking is a subclass of ChunkingStrategy, which aligns with the ground truth's explanation of ChunkingStrategy being the abstract base class that RegexChunking implements.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly names 'JsonCssExtractionStrategy' as the focus for advanced extraction scenarios, particularly for handling complex, nested HTML structures. This directly ties the documented functionality to this class.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is designed for handling complex nested HTML structures, which aligns with the ground truth's explanation of the class implementing recursive HTML processing with BeautifulSoup and schema-based selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, 'JsonCssExtractionStrategy' extends 'ExtractionStrategy'. Thus, the underlying abstraction provided by ExtractionStrategy is implicitly involved in achieving the advanced extraction capabilities detailed in the document.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship - that ExtractionStrategy serves as the foundation for JsonCssExtractionStrategy by providing the base architecture and abstract methods. The predicted description correctly identifies the inheritance relationship and underlying abstraction, even if it's more concise.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation advises to 'Start with CSS for structured data'. This implies using an extraction strategy that leverages CSS selectors. The JsonCssExtractionStrategy implements extraction based on CSS selectors, thereby matching the guideline.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that the JsonCssExtractionStrategy implements the CSS-based extraction approach recommended in the documentation for structured data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation states 'Use LLM for complex interpretation' which directly suggests employing a large language model based extraction. The LLMExtractionStrategy fulfills this role by applying LLMs to process and interpret complex content.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM usage for complex interpretation, while the ground truth describes error handling implementation using ThreadPoolExecutor and try-catch blocks. These are completely different aspects of the code.",
      "error_type": "wrong_feature_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The snippet advises 'Try Cosine for content relevance'. CosineStrategy implements extraction based on cosine similarity computations, making it suitable for ensuring content relevance as the documentation recommends.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy implements content relevance functionality through cosine similarity computations, aligning with the documentation's recommendation. Both descriptions convey the same core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The provided usage example in the documentation explicitly calls the 'arun' method on a crawler instance. This shows that AsyncWebCrawler.arun() is the method responsible for initiating crawls and processing extraction.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the method calling aspect, while missing the core error handling and best practices implementation shown in the ground truth. However, it does correctly identify arun() as the key method.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "After calling AsyncWebCrawler.arun(), the returned object is of type CrawlResult. The documentation sample then accesses its attributes to handle success, errors, and extracted data.",
    "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is the return type from arun() and contains the key attributes referenced in the documentation for error handling and data extraction. While it's less detailed than the ground truth, it captures the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In the usage example, the code checks if the result was unsuccessful by evaluating 'result.error_message'. This directly references the error_message attribute of the CrawlResult class.",
    "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that error_message is used for error checking, but misses the key point about it working in conjunction with result.success and providing descriptive error information when success is False",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The sample code uses 'result.extracted_content' to obtain the data for further processing (via json.loads). This shows that the extracted_content attribute is a key output of the extraction process.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that extracted_content is used to store/access the crawled data for further processing, which aligns with the ground truth's explanation of its role in storing successfully crawled data that can be parsed as JSON.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code snippet explicitly checks 'if not result.success' to determine whether the crawl was successful. This shows that the success attribute of CrawlResult is a critical flag for error handling in the extraction process.",
    "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship - that the success boolean is used for error handling in the extraction process, which aligns with the ground truth's description of it supporting error handling best practices.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the usage of CosineStrategy by showing examples of how to instantiate it with various parameters (sim_threshold, word_count_threshold, top_k). This clearly maps to the CosineStrategy class constructor that accepts these parameters.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between the documentation and CosineStrategy's parameters, but misses explaining how these parameters are functionally implemented in the class methods, which is a key aspect highlighted in the ground truth.",
      "error_type": "incomplete_functionality_coverage"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy, which is directly instantiated in the snippet, extends ExtractionStrategy. Although ExtractionStrategy is not mentioned explicitly in the text, it is an integral part of the inheritance chain that defines the extraction interface for CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class, but incorrectly focuses on CosineStrategy which isn't shown in the code or documentation. It misses the core purpose of ExtractionStrategy as a template for implementing various extraction configurations.",
      "error_type": "incorrect_focus_and_omission"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation example directly calls 'crawler.arun()' with keyword arguments for excluding certain links. This shows that the 'arun()' method of AsyncWebCrawler is the entry point for smart link filtering.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is the entry point for link filtering and accepts filtering parameters, which aligns with the ground truth explanation that arun() accepts filtering parameters through **kwargs that get passed to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "By default, AsyncWebCrawler instantiates an AsyncPlaywrightCrawlerStrategy to perform crawling operations. The keyword arguments for link filtering provided in the documentation are forwarded through AsyncWebCrawler.arun() to this strategy.",
    "ground_truth_relationship": "The documentation describes URL filtering options that are implemented through parameters passed to the crawler's arun() method, which are then processed within the crawl() method of the AsyncPlaywrightCrawlerStrategy class to control which links are included in the crawling results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description gets part of the relationship right by mentioning that AsyncPlaywrightCrawlerStrategy handles crawling operations, but incorrectly suggests that AsyncWebCrawler instantiates it by default and forwards the filtering parameters. The ground truth shows these parameters are processed within the crawl() method directly.",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. This establishes a uniform interface for crawling, including handling additional filtering parameters like those controlling link inclusion.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing the smart link filtering functionality through its crawl method that accepts kwargs, which allows passing filter parameters like exclude_external_links and exclude_domains.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that AsyncCrawlerStrategy provides an interface for crawling with filtering capabilities through kwargs/parameters. While the predicted description mentions a specific implementation (AsyncPlaywrightCrawlerStrategy), the core relationship and functionality alignment is accurate.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:'. This demonstrates that the AsyncWebCrawler class is the entry point for the crawling process and proxy rotation example.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used as an async context manager and serves as the entry point for the crawling functionality, which aligns with the ground truth's explanation of the class providing the foundational structure with async context manager support.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example explicitly calls 'crawler.arun(url=url)' to perform the crawling task after updating the proxy. This invocation shows that the 'arun' method of AsyncWebCrawler is used to execute the crawl operation with rotating proxies.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method is used for performing the crawling operation and is called with a URL parameter, which aligns with the ground truth's description of it being the core crawling functionality. The minor details about proxy configuration through crawler_strategy being omitted does not affect the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a usage example where the 'arun' method is called on a crawler instance with a 'css_selector' parameter. This matches the signature and functionality of the AsyncWebCrawler.arun() method, which accepts a 'css_selector' argument to extract targeted HTML content using CSS selectors.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of the css_selector parameter in the arun method for targeted content extraction, matching the ground truth's explanation of how CSS selectors are used to filter and extract specific elements from crawled webpages.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and instantiates it with a defined schema to perform pattern\u2010based extraction from dynamic page content.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that JsonCssExtractionStrategy uses a schema to perform pattern-based extraction from HTML content using CSS selectors",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of LLMExtractionStrategy by passing parameters such as provider, schema, and instruction to analyze dynamic content.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy - using LLM for content analysis with configurable provider, schema, and instruction parameters, which aligns with the ground truth's description of processing HTML content through LLM models with these configurations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation uses the 'crawler.arun' call to trigger the extraction process after performing page interactions; this maps directly to the AsyncWebCrawler.arun() method.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for extraction, but oversimplifies by not mentioning the key functionality of handling different extraction strategies and structured data extraction with JavaScript interactions and wait conditions",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, ExtractionStrategy is the abstract base class from which both JsonCssExtractionStrategy and LLMExtractionStrategy derive, thereby underpinning the extraction functionality demonstrated.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the abstract base class for the strategy implementations, but misses the crucial aspect of how it provides the foundational extract() and parallel run() methods that enable the functionality shown in the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy from 'crawl4ai/extraction_strategy', passing parameters such as provider, api_token, schema, and instruction to set up an LLM-based extraction strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic initialization of LLMExtractionStrategy with key parameters, but misses crucial functionality aspects mentioned in the ground truth about chunking, rate limiting, and parallel processing for different LLM providers.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'crawler.arun()' with the extraction_strategy parameter. Although the class instance is not directly shown, this indicates that the 'arun()' method from AsyncWebCrawler is used to execute the crawling process with the given extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() method accepts and uses an extraction_strategy parameter for the crawling process, which aligns with the ground truth's explanation of how AsyncWebCrawler.arun() implements structured data extraction through configurable extraction strategies.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl completes, the resulting structured data is accessed via the 'extracted_content' attribute of the CrawlResult object. This attribute holds the output data generated by the extraction strategy.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that extracted_content holds the structured data output from the extraction process, with the ground truth adding some implementation details about JSON parsing that don't change the core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'JsonCssExtractionStrategy' in its header and code example. It shows how the class is instantiated with a schema to extract product data using CSS selectors.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the class and its purpose for CSS-based extraction, but omits the crucial implementation detail that it uses BeautifulSoup's select() method to perform the actual extraction, which is a key part of how the relationship works.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly mentioned in the snippet, this inheritance relationship is crucial as it provides the abstract interface for all extraction strategies, including the CSS-based one.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, and acknowledges that ExtractionStrategy provides the abstract interface. While it doesn't mention the parallel processing capabilities, this omission doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage example, the snippet demonstrates invoking 'crawler.arun()' with the extraction_strategy parameter set to an instance of JsonCssExtractionStrategy. This shows how the extraction strategy is integrated into the crawling process.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship of how crawler.arun() accepts and integrates the extraction strategy parameter, which aligns with the ground truth's explanation of how arun() implements the execution logic for extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")', demonstrating the use of the AsyncWebCrawler.arun() method to perform a crawl. This call initiates the crawling process and returns a CrawlResult object.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() performs crawling and returns a CrawlResult, but misses the crucial aspect that it specifically powers the link analysis functionality and produces categorized link data as described in the documentation.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "After calling arun(), the documentation example accesses result.links to analyze both internal and external links. This indicates that the 'links' attribute of the CrawlResult object is used for link classification.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that links are accessed via result.links and used for classification, but misses the key structural detail that links is a dictionary with type-based keys containing lists of link details",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler from crawl4ai and creates an instance using the 'async with AsyncWebCrawler(verbose=True)' construct, demonstrating its use in asynchronous crawling.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler supports async context manager functionality through 'async with' syntax as shown in the documentation, which aligns with the ground truth's explanation of __aenter__ and __aexit__ implementation enabling this functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage example, the 'arun' method is called on the AsyncWebCrawler instance to crawl a specific URL. This directly demonstrates how to initiate a crawl using Crawl4AI.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of arun() for crawling URLs, but misses crucial aspects about its asynchronous nature and comprehensive functionality (content extraction, caching, error handling) that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example prints 'result.markdown', implicitly indicating that the crawl result object (an instance of CrawlResult returned by arun()) exposes a public 'markdown' attribute which holds the extracted content.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the markdown attribute/property of CrawlResult stores extracted content that is displayed in the example. The predicted description correctly identifies the relationship between the result object and its markdown attribute.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates CosineStrategy with parameters such as semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k, demonstrating its intended use for similarity\u2010based clustering and content extraction.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code, correctly identifying that CosineStrategy uses the documented parameters for similarity-based clustering and content extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'crawler.arun' with the extraction_strategy parameter set to an instance of CosineStrategy. This implies that the AsyncWebCrawler.arun() method is responsible for invoking the crawling process that uses the provided extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts and uses an extraction_strategy parameter in the crawling process, which aligns with the ground truth's explanation of how arun() implements crawling logic that can apply CosineStrategy for content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy; although the base class is not directly instantiated in the snippet, its role as the abstract interface for extraction strategies is implicit in the design and use of CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy as well as the basic architectural pattern where ExtractionStrategy serves as an abstract interface. While it's more concise than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it with an async context manager to initiate a crawl of the Coinbase explore page.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used in an async context manager to crawl the Coinbase page, which aligns with the ground truth's explanation of how the AsyncWebCrawler implements the core crawling functionality for the cryptocurrency price extraction example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on the AsyncWebCrawler instance to perform the crawling and extraction process. This demonstrates the use of the method to fetch the webpage content.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for crawling and fetching webpage content, but misses crucial aspects about applying the extraction strategy and returning structured cryptocurrency data shown in the example",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation creates an instance of JsonCssExtractionStrategy using a custom schema to extract cryptocurrency prices, which shows its direct role in structuring the output data.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core idea that JsonCssExtractionStrategy uses a schema to extract structured data, but misses key details about CSS selectors and the specific structure of crypto data extraction (baseSelector and field selectors) that are important to understanding its functionality.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not explicitly instantiated in the snippet, ExtractionStrategy underpins the extraction functionality demonstrated.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy as a base class, which aligns with the ground truth's explanation of ExtractionStrategy providing core functionality through abstract methods that concrete strategies like JsonCssExtractionStrategy inherit.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the code accesses the 'extracted_content' attribute of the CrawlResult object to retrieve the structured data. This attribute is a part of the public interface defined for crawl results.",
    "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions extracted_content is part of the CrawlResult object, but misses the key aspect that it specifically contains cryptocurrency data with name/symbol/price fields that gets parsed from a string to JSON.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct asynchronous call to 'crawler.arun(...)' with various parameters such as 'word_count_threshold', 'remove_overlay_elements', and 'process_iframes'. This explicitly maps to the 'AsyncWebCrawler.arun()' method in the available artifacts.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship between the documentation and code - that the arun() method is an asynchronous crawler that accepts the documented configuration options through its parameters. While the ground truth provides more implementation details, the predicted description captures the essential mapping.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet only demonstrates a call on 'crawler.arun()', it implies that 'crawler' is an instance of the 'AsyncWebCrawler' class. This class encapsulates the crawling process and provides the 'arun()' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between AsyncWebCrawler and the arun method, but misses the crucial aspect that the class implements the specific configuration options shown in the documentation through its parameters. The description is too general.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler.arun()' method internally delegates crawling operations to a crawler strategy. By default, this strategy is implemented by 'AsyncPlaywrightCrawlerStrategy', which processes parameters like 'remove_overlay_elements' and 'process_iframes' as shown in the documentation snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship between the AsyncPlaywrightCrawlerStrategy class and the documented crawling options, explaining how it processes parameters like remove_overlay_elements and process_iframes through its crawl method. The predicted description accurately describes the delegation of functionality, even if it mentions AsyncWebCrawler.arun() which isn't a crucial detail.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The usage snippet assigns the result of 'crawler.arun(...)' to a variable 'result'. This return value is an instance of the 'CrawlResult' class, which encapsulates details of the crawled page.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that stores all crawled outputs and processing flags shown in the documentation's basic options, including the cleaned content, extracted media, and success status.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CrawlResult as the return type, but misses the crucial aspect that it specifically stores crawl outputs and processing flags related to the documented basic options",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun(...)', indicating that the AsyncWebCrawler.arun() method is being invoked to execute a crawl and retrieve content. This call is explicitly demonstrated in the usage example provided.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method for crawling and retrieving content, but misses the crucial markdown conversion functionality highlighted in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet shows 'print(result.markdown)', which explicitly accesses the 'markdown' attribute of the crawl result. This attribute provides the markdown formatted output that is described in the documentation snippet.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately explains that the markdown attribute provides markdown formatted output, which aligns with the ground truth's explanation that it stores HTML-to-Markdown converted text as an optional string",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates RegexChunking, demonstrating its use as a chunking strategy that splits text based on a regex pattern.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that RegexChunking is a chunking strategy using regex patterns, but misses the crucial detail about handling multiple patterns iteratively and the default pattern behavior",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "RegexChunking is built on top of the abstract ChunkingStrategy, making ChunkingStrategy an implicit dependency that defines the contract for all chunking strategies.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ChunkingStrategy is an abstract base class that RegexChunking extends/implements, defining the contract for chunking functionality",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly creates an instance of AsyncWebCrawler using an async context manager, indicating its direct use in the example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the async context manager usage, while missing the core RegexChunking functionality relationship described in the ground truth. However, it does correctly identify one aspect of the AsyncWebCrawler's usage.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation code sample calls the 'arun()' method on an AsyncWebCrawler instance, showing how the crawl operation is executed.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic method call but misses the core focus on RegexChunking strategy implementation and validation that is central to the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a usage example that directly calls 'crawler.arun(url=\"https://example.com\")'. This indicates that the arun() method of the AsyncWebCrawler is responsible for performing the crawl and returning a result that contains the raw HTML.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is responsible for crawling and returning results containing raw HTML, which aligns with the ground truth's explanation that it retrieves unmodified webpage content and stores it in CrawlResult.html",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "In the provided code example, 'print(result.html)' accesses the 'html' attribute of the CrawlResult object, which holds the complete, unmodified HTML response from the crawler. This attribute exactly fulfills the description of 'Raw HTML' in the documentation.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the html attribute contains the complete, unmodified HTML from the crawled webpage, matching the core relationship described in the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation sample clearly instantiates an AsyncWebCrawler via the 'async with AsyncWebCrawler(verbose=True) as crawler:' statement. This shows the direct use of the AsyncWebCrawler class in the screenshot example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class usage but focuses only on instantiation, missing the core screenshot functionality relationship described in the ground truth. The ground truth more accurately describes how the class implements and handles screenshot capabilities through its arun method.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code sample explicitly calls the arun() method on the AsyncWebCrawler instance to perform a crawl with the screenshot parameter set to True. This method is responsible for triggering the screenshot capture process.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() handles screenshot functionality but misses key aspects like caching and the full flow of screenshot data through the process that are important to understanding the relationship",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "After the crawl, the sample checks 'if result.success and result.screenshot:', implicitly accessing the screenshot attribute of the CrawlResult object. This attribute holds the Base64\u2010encoded screenshot data produced during the crawl.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the screenshot attribute stores base64-encoded screenshot data. The predicted description correctly identifies the core relationship and functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, AsyncWebCrawler by default instantiates an AsyncPlaywrightCrawlerStrategy (as its crawler_strategy) that implements the take_screenshot method. This method is critical for capturing the screenshot used in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the screenshot functionality by using the take_screenshot method that captures a full-page screenshot of the webpage and returns it as a base64-encoded string, which directly corresponds to the documented screenshot capture and saving process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy provides screenshot functionality used by AsyncWebCrawler, but incorrectly suggests this relationship is not directly mentioned in the documentation when it actually is explicitly shown.",
      "error_type": "minor_misunderstanding"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, the abstract base class that defines the take_screenshot() method. This inheritance ensures that the screenshot functionality adheres to a standard interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods, including take_screenshot(), that enable the screenshot capture functionality demonstrated in the documentation's capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncCrawlerStrategy is an abstract base class that defines the interface including the take_screenshot method, which enables screenshot functionality. The predicted description focuses on inheritance while the ground truth focuses on interface definition, but they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun(...)', clearly invoking the arun() method of AsyncWebCrawler to perform the extraction process.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between the code and documentation by noting the arun() method usage, but misses the core focus on error handling which is the main relationship described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code snippet explicitly checks 'if result.success:' to determine whether the extraction was successful.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the code uses the success property to check if extraction succeeded, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet shows that 'result.extracted_content' is used as input to json.loads(), indicating that this attribute holds the extracted content from the crawling operation.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content contains crawled content that gets parsed as JSON, but misses the crucial semantic matching aspect and that it's part of CrawlResult structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The documentation snippet prints 'result.error_message' when the extraction fails, directly accessing this CrawlResult attribute.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of error_message in error handling, but misses the key aspect that it's specifically used to store failure details, not just display them.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions the 'Cosine Strategy' as being particularly effective for handling inconsistent content structure and for semantic understanding, indicating its role as an extraction strategy.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies CosineStrategy's main purpose for handling inconsistent content and enabling semantic understanding, which aligns with the code's implementation of cosine similarity and hierarchical clustering for semantic-based extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a usage example by calling 'crawler.arun()' with timing parameters (page_timeout and delay_before_return_html). This directly maps to the AsyncWebCrawler.arun() method which is designed to initiate a crawl with controlled timeouts and delays.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method handles timing controls through method parameters, which aligns with the ground truth explanation that these controls are implemented via **kwargs and passed to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, the AsyncWebCrawler.arun() method uses its internal attribute 'crawler_strategy', which, by default, is an instance of AsyncPlaywrightCrawlerStrategy. This class implements the crawl() method that utilizes the 'page_timeout' and 'delay_before_return_html' parameters to control timing.",
    "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the AsyncPlaywrightCrawlerStrategy implements timing control through page_timeout and delay_before_return_html parameters. The predicted description captures the essential relationship even though it mentions AsyncWebCrawler.arun() rather than focusing directly on the crawl method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. This inheritance establishes the contractual interface for crawl methods including timing controls. Thus, AsyncCrawlerStrategy forms a foundational part of the overall timing control mechanism indirectly utilized by the documentation example.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables timing control through its crawl method's **kwargs parameter, which can accept the documented page_timeout and delay_before_return_html parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that AsyncCrawlerStrategy provides the interface/foundation for timing control functionality through its abstract methods, particularly the crawl method that accepts kwargs parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy from the crawl4ai.extraction_strategy module and shows usage examples with different LLM provider strings (such as 'openai/gpt-4o', 'huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct', and 'ollama/llama3.2'). This indicates that the class is used directly to perform structured data extraction via various LLM providers.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship showing that LLMExtractionStrategy supports multiple LLM providers through its constructor parameters and can be used for structured data extraction with different services. This aligns with the ground truth's explanation of provider flexibility and authentication handling.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although the documentation does not mention ExtractionStrategy explicitly, it underpins the functionality of LLMExtractionStrategy by providing the abstract extraction interface that the LLM extraction strategy fulfills.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify ExtractionStrategy as the base class that provides the foundation for LLM-based extraction implementations. The predicted description captures the core inheritance relationship, and while it's less specific about the different LLM providers, it correctly describes the fundamental class relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example clearly shows the instantiation of the crawler using 'async with AsyncWebCrawler() as crawler:', which directly references the AsyncWebCrawler class as the starting point of the content extraction process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role in content extraction but focuses only on instantiation, missing the crucial implementation details about how fit_markdown actually works through aprocess_html method and the scraping strategy",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The provided code snippet calls 'await crawler.arun(url=\"https://example.com\")', clearly indicating that the 'arun()' method of AsyncWebCrawler is used to perform the crawling job and return a CrawlResult.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and returns a CrawlResult, but misses the crucial aspect that it implements content extraction and filtering functionality through extraction/chunking strategies to enable fit_markdown feature",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation explicitly accesses the 'fit_markdown' property via 'result.fit_markdown' to obtain the main article content. This property is presented as a smart extraction that focuses on the most relevant content, excluding boilerplate elements.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is a property that stores extracted main content after applying content filtering to remove boilerplate elements. The predicted description correctly emphasizes its role in smart content extraction and focusing on relevant content.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The sample code compares 'result.fit_markdown' with 'result.markdown', indicating that 'markdown' holds the initially extracted content. This highlights how the complete markdown output is available for further processing or comparison with the refined 'fit_markdown'.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the key relationship that markdown contains all content while fit_markdown provides filtered content, and both emphasize the comparison/contrast between these two properties",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using the 'async with AsyncWebCrawler(...)' syntax. This makes AsyncWebCrawler the primary entry point for crawling in this example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the entry point for crawling, which is true, but misses the core relationship about user simulation functionality described in the ground truth. The ground truth focuses on how AsyncWebCrawler implements user simulation through specific parameters.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the method 'arun()' is explicitly called on the AsyncWebCrawler instance. This method is responsible for performing the crawl with various options such as 'simulate_user' and 'override_navigator'.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called on AsyncWebCrawler with various options, but misses the key relationship that these options specifically enable user simulation behaviors through kwargs parameter.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncPlaywrightCrawlerStrategy is the default crawling strategy used by AsyncWebCrawler. The parameters 'simulate_user' and 'override_navigator' passed in the example are processed within AsyncPlaywrightCrawlerStrategy.crawl(), where user simulation (random mouse movements, clicks, and navigator overrides) is implemented.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncPlaywrightCrawlerStrategy implements user simulation features when simulate_user or override_navigator parameters are passed. Both descriptions agree on the key functionality including mouse movements, clicks, and navigator overrides.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit call to 'crawler.arun()' with the extraction_strategy parameter, directly mapping to the 'AsyncWebCrawler.arun()' method in the available artifacts.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts the extraction_strategy parameter and allows for different strategies, which aligns with the ground truth's explanation of how strategies can be combined and passed in sequentially.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet indicates 'First use CSS strategy for initial structure'. Although 'css_strategy' is not explicitly named, it is implicitly represented by an extraction strategy that leverages CSS selectors \u2013 corresponding to the 'JsonCssExtractionStrategy' artifact.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that JsonCssExtractionStrategy implements CSS-based extraction as part of a possible multi-strategy approach, with the predicted description correctly identifying its role in initial structure extraction via CSS selectors",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet further demonstrates a call for semantic analysis via LLM by passing 'llm_strategy' to the same method. This implies the usage of an extraction strategy based on a language model, which is implemented by the 'LLMExtractionStrategy' artifact.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the LLM-based extraction functionality but misses the crucial aspect of how it works in combination with CSS strategy as part of a multi-strategy approach. It only focuses on the LLM part without acknowledging the combined strategy capability.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows direct usage of 'crawler.arun()' with a 'js_code' parameter. This method is explicitly invoked in the example to execute JavaScript commands in the crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used to execute JavaScript code, but fails to mention that it works through kwargs and crawler_strategy.crawl(). It describes the interface but misses the implementation mechanism.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although the snippet does not name it, the 'js_code' parameter is ultimately processed by the crawl() method within AsyncPlaywrightCrawlerStrategy. This class is used as the default crawler strategy within AsyncWebCrawler, handling the execution of supplied JavaScript commands.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the code handles JavaScript execution through the js_code parameter, which can accept single or multiple commands. The predicted description mentions the key components (js_code parameter, crawl method, AsyncPlaywrightCrawlerStrategy) while the ground truth adds implementation details about page.evaluate(), but the core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, inheriting the abstract 'crawl' interface that includes the js_code processing. While the base class is not directly invoked in the snippet, it is a necessary part of the underlying implementation chain.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class with abstract methods, but focuses on inheritance aspects rather than its core purpose of enabling JavaScript execution as described in the ground truth",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', making it the entry point for the crawling example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler instantiation, but misses the key relationship about how it implements integrated JavaScript execution and content extraction through the arun method.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the 'arun()' method of AsyncWebCrawler is explicitly called to perform the crawl for each page, retrieving commit data.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that arun() is used for crawling pages, it misses the crucial aspect that arun() specifically implements integrated JavaScript execution and waiting functionality through its js_code parameter",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly creates an instance of JsonCssExtractionStrategy with a schema to extract commit titles from the page, serving as the extraction strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that JsonCssExtractionStrategy uses a schema to extract structured data, which aligns with the ground truth's explanation of implementing structured data extraction using baseSelector and field selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example directly calls 'kill_session()' via 'crawler.crawler_strategy.kill_session(session_id)' to terminate the crawling session, indicating explicit control over session management.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text captures that kill_session is called to terminate the session, but misses the key aspect of resource cleanup (closing page and context objects) that the ground truth emphasizes",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy when no crawler_strategy is provided. Its implementation handles the integrated JavaScript execution via the 'js_code' parameter, which is key to the example's functionality.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies JavaScript execution functionality but misses the key aspect of the csp_compliant_wait method's polling loop implementation and timeout handling that is central to the ground truth",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, inheriting the abstract crawling interface that defines methods such as 'crawl'. This forms part of the underlying architecture for the crawler used in the example.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class defining the crawler interface, but misses the key point about its role in enabling integrated JavaScript execution and waiting functionality through the set_hook() method.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a concrete implementation derived from the ExtractionStrategy base class, which defines the interface for any extraction strategy. This relationship underpins the behavior of the extraction process in the example.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the inheritance relationship between ExtractionStrategy and its implementation, it doesn't capture the key purpose of processing HTML into structured data as described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=...)' to obtain a crawl result. This directly maps to the AsyncWebCrawler.arun() method that performs the crawl and returns a CrawlResult object.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called to get crawl results, but misses the crucial aspect that it's responsible for generating the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation.",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The snippet accesses 'result.html' to retrieve the original raw HTML. This corresponds directly to the 'html' attribute in the CrawlResult class.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions describe the html attribute/property as storing raw HTML content, with the predicted description accurately reflecting the documented relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation shows 'result.cleaned_html' to obtain the sanitized version of the HTML. This attribute is defined in the CrawlResult class.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that cleaned_html provides sanitized HTML output through the result.cleaned_html attribute, which aligns with the ground truth's core meaning of storing sanitized HTML as an optional string value.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The documentation example uses 'result.markdown' to access the standard markdown output. This attribute is implemented as part of the CrawlResult class.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that result.markdown provides standard markdown output as part of the CrawlResult class, which aligns with the ground truth's explanation of the markdown field storing HTML converted to standard markdown format.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet includes 'result.fit_markdown' to retrieve the most relevant markdown content. This maps to the 'fit_markdown' attribute in the CrawlResult class.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used to retrieve the most relevant content in markdown format, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides a usage example where the 'arun' method is directly called on the 'crawler' instance with the keyword 'html2text' to configure markdown conversion options. This explicitly maps to the 'AsyncWebCrawler.arun()' method, which is responsible for initiating the crawl and processing of HTML content.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method handles HTML processing with html2text options passed as kwargs parameters. Both descriptions explain the same core relationship between the documentation example and the code's functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler via 'async with AsyncWebCrawler(verbose=True) as crawler', demonstrating its role as the entry point for crawling.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler is used as the main entry point for crawling via an async context manager, which aligns with the ground truth's description of its implementation through __aenter__/__aexit__ and core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code example, the method arun() is directly called on the AsyncWebCrawler instance to perform the crawling task with the provided URL.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method being called with a URL parameter, but omits crucial functionality like HTML extraction, caching, and processing that is core to the relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The arun() method returns a result which is expected to be a CrawlResult instance, as evidenced by later access to its 'markdown' attribute. Although not directly mentioned by name in the snippet, this type is implied.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that stores the crawled webpage content including markdown output shown in the basic usage example's print statement.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method returns a CrawlResult instance, which contains the crawled data including markdown content, aligning with the ground truth's explanation of CrawlResult as the data structure for storing crawled webpage content.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example directly accesses 'result.markdown' (printing its first 500 characters) to show part of the crawl output, demonstrating explicit use of this attribute.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute contains/stores the crawled text content that can be accessed and printed from the crawl result",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly discusses the schema for extraction using CSS selectors, detailing keys like 'baseSelector', 'name', 'selector', and 'type'. This exactly matches the purpose of the JsonCssExtractionStrategy which implements extraction based on a provided JSON schema and uses CSS selectors to extract data from web elements.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - both describe a schema-based extraction strategy using CSS selectors to extract structured data from HTML elements",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although the documentation does not mention ExtractionStrategy by name, the extraction process described adheres to the contract defined by ExtractionStrategy, thereby indirectly linking the documentation to this base class.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, and acknowledges that implementations must adhere to the base class's contract. While it doesn't mention the parallel processing capabilities, this omission doesn't change the core relationship being described.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet directly instantiates JsonCssExtractionStrategy with a defined schema to extract structured data from dynamic pages. This explicit reference indicates that this class is intended for handling JSON/CSS based extraction when combined with JavaScript execution.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy handles structured data extraction using a schema and works with JavaScript execution for dynamic pages. While the ground truth provides more implementation details, the high-level relationship understanding is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet demonstrates creating an instance of AsyncWebCrawler using an async context manager to perform the crawling task. This explicit usage shows that AsyncWebCrawler is a key entry point for dynamic data crawling in the system.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as a key component for crawling, but misses the crucial functionality around JavaScript execution and dynamic data extraction that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun()' method on the AsyncWebCrawler instance to start the crawling process and trigger extraction. Although not mentioned by name in the text, its invocation is essential to perform the operations described in the example.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the key method for crawling and extraction, but misses its specific support for dynamic content execution via js_code and other advanced capabilities that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the extracted data is accessed via the 'extracted_content' attribute of the CrawlResult object. In the example, the content is parsed using json.loads(result.extracted_content), making this attribute a key part of the public interface used to obtain structured extraction output.",
    "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that extracted_content is used to store/access the scraped data after extraction, with the ground truth providing more specific context about crypto pricing data",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls the 'arun' method on the crawler instance (i.e. 'await crawler.arun(...)'). This directly maps to the 'AsyncWebCrawler.arun()' artifact, which exposes parameters such as 'wait_for', 'js_code', 'process_iframes', and 'delay_before_return_html' for handling dynamic content.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method handles dynamic content through parameters that enable wait conditions and JavaScript execution, matching the ground truth's explanation of how the method implements dynamic content handling via kwargs.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although the documentation does not directly mention 'AsyncPlaywrightCrawlerStrategy', this artifact is implicitly used. 'AsyncWebCrawler.arun()' internally delegates its crawling tasks to its crawler strategy, which is typically an instance of 'AsyncPlaywrightCrawlerStrategy'. This class implements the handling of dynamic content by processing parameters like 'wait_for', 'js_code', 'process_iframes', and 'delay_before_return_html'.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that the code handles dynamic content through features like JavaScript execution, iframe processing, and configurable delays. The predicted description correctly identifies the core functionality of AsyncPlaywrightCrawlerStrategy in implementing these dynamic content handling capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncCrawlerStrategy' is the abstract base class that defines the crawl interface. 'AsyncPlaywrightCrawlerStrategy' extends this base class. Although not directly mentioned in the snippet, its role is critical in ensuring that the crawling implementation follows the required contract for dynamic content handling.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class that defines the crawl interface, but it fails to highlight its role in enabling dynamic content handling capabilities through its methods, which is a crucial aspect mentioned in the ground truth.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the arun() method by showing 'await crawler.arun(...)' with js_code and wait_for parameters to handle dynamic content such as infinite scrolling and form submission.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between arun() and dynamic content handling through js_code and wait_for parameters, showing the same fundamental functionality as described in the ground truth",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not explicitly named in the snippet, the 'crawler' instance used to call arun() is implicitly an instance of AsyncWebCrawler, which orchestrates crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class but misses the core functionality of handling dynamic content through js_code and wait_for parameters, which is the key relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy. This strategy processes parameters like js_code and wait_for to manage dynamic content operations such as scrolling and form interactions.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncPlaywrightCrawlerStrategy handles dynamic content loading through JavaScript execution and waiting mechanisms, including scroll functionality and form interactions. The predicted description covers the core functionality, even if it's less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy, establishing the required interface for crawling methods that support dynamic content interactions.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the essence of AsyncCrawlerStrategy as an abstract base class that provides foundational methods for dynamic content interaction and crawling functionality. The predicted description, while more concise, correctly identifies the key relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes a direct usage example where the 'arun' method is called on the crawler (i.e. 'crawler.arun(...)'). This demonstrates how the crawler cleans content, aligning directly with the documented functionality.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that arun() is used for content crawling/cleaning, it focuses on just showing an example call rather than explaining how the method implements the cleaning features through its parameters, which is the key relationship described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "In the provided usage example, after calling 'crawler.arun()', the cleaned HTML content is accessed via 'result.cleaned_html'. This attribute is part of the CrawlResult public interface and represents the output of content cleaning.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that cleaned_html is an attribute containing cleaned HTML content accessible after running the crawler. While it doesn't detail all the cleaning steps mentioned in the ground truth, it correctly describes the basic relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Similarly, the usage example prints 'result.markdown' to display a clean markdown version of the content. This attribute is part of the CrawlResult public interface and reflects the clean content processed by the crawler.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that result.markdown provides a clean markdown version of the content, which aligns with the ground truth's explanation of the markdown property providing cleaned content after noise removal.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler with parameters headless=True, verbose=True, and sleep_on_close=False.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that AsyncWebCrawler can be instantiated with those configuration parameters, but misses that these parameters are actually passed through kwargs to the underlying AsyncPlaywrightCrawlerStrategy rather than being directly used by AsyncWebCrawler itself.",
      "error_type": "implementation_detail_omission"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform a crawl operation.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic use of arun() for crawling but misses the crucial aspect that the code implements the method and its configuration parameters rather than just calling it",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when instantiated without an explicit crawler_strategy, defaults to creating an instance of AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description makes a claim about AsyncWebCrawler's default behavior, while the ground truth describes how AsyncPlaywrightCrawlerStrategy implements specific browser configuration options mentioned in the documentation.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is a subclass of AsyncCrawlerStrategy, meaning that the abstract crawling interface is implemented via inheritance.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining an abstract interface, but focuses only on inheritance relationships while missing the key connection to browser configuration and crawling functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the use of AsyncWebCrawler by instantiating it with proxy settings (using the 'proxy' and 'proxy_config' keyword arguments) to enhance access via proxies.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality that AsyncWebCrawler can be configured with proxy settings, which aligns with the ground truth's explanation of proxy configuration being passed through kwargs to AsyncPlaywrightCrawlerStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not mentioned explicitly in the snippet, AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy when no crawler_strategy is provided. This class processes the 'proxy' and 'proxy_config' parameters to set up the proxy in the browser session.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly states that proxy support is handled, it incorrectly suggests this is done through AsyncWebCrawler defaulting to AsyncPlaywrightCrawlerStrategy. The ground truth shows proxy support is directly implemented in AsyncPlaywrightCrawlerStrategy through browser_args configuration.",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, thereby inheriting the common crawling interface. This abstract base sets the stage for implementations\u2014including proxy configuration logic\u2014ensuring a uniform API across strategies.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that AsyncCrawlerStrategy is an abstract interface defining crawling functionality with proxy capabilities. The predicted description focuses on inheritance while the ground truth focuses on interface definition, but both accurately convey the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation explicitly shows a use case where the formatted article content is accessed via 'result.fit_markdown'. This maps directly to the public attribute 'fit_markdown' in the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that fit_markdown is used for formatted article content and specifically relates to Best Practice #1 from the documentation. The predicted description captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet demonstrates filtering images based on a relevance score from 'result.media'. This directly uses the 'media' attribute in the CrawlResult class that holds media information.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately describe filtering images based on relevance scores using the media dictionary attribute. The predicted description captures the core functionality shown in the documentation's Best Practices example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation shows an example where internal content links are filtered from 'result.links'. This directly corresponds to the 'links' attribute in the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions express the same core relationship - that the links dictionary structure allows filtering of internal content links as demonstrated in the documentation example. The predicted description captures this key functionality, even if it's slightly less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation example shows invoking 'crawler.arun()' with parameters such as word_count_threshold, keep_data_attributes, and process_iframes. This maps directly to the AsyncWebCrawler.arun() method, which orchestrates crawling and content cleaning.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the parameter passing and basic crawling functionality but misses key aspects mentioned in the ground truth about media processing and link extraction capabilities",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation header 'Tips for Using JsonCssExtractionStrategy' explicitly names this class. It is meant to be used for extracting content based on CSS selectors and handling extraction errors, which aligns with the guidance provided in the document.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship between the code and documentation - that the class implements CSS-based extraction using selectors, which aligns with the documentation's guidance on selector usage and testing",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly mentioned in the documentation snippet, understanding the extraction strategy interface is implicit in using JsonCssExtractionStrategy correctly.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies inheritance relationship but misses core aspects about error handling and parallel processing capabilities mentioned in ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation advises always checking the 'result.success' flag to handle failures. This flag is implemented as the success attribute of the CrawlResult class, making it directly relevant to error handling as described.",
    "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the success boolean flag and its role in error handling as described in the documentation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows the direct instantiation of AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:', which clearly indicates its use for session management.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles session management, even though it focuses on the instantiation syntax rather than the session_id parameter specifically. The core concept of session handling is captured in both descriptions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls the arun() method on the AsyncWebCrawler instance to perform crawl requests while passing a 'session_id'.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of arun() with session_id but misses the key aspects of session management and state persistence that are central to the ground truth relationship",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet explicitly calls the kill_session() method on the crawler_strategy attribute (which by default is an instance of AsyncPlaywrightCrawlerStrategy) to clean up the session, using the same 'session_id'.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly indicates kill_session is used for cleanup, but incorrectly specifies it's called on crawler_strategy attribute, which isn't shown in the code or documentation.",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation snippet is titled 'TopicSegmentationChunking' and includes a usage example that explicitly imports and instantiates TopicSegmentationChunking with the 'num_keywords' parameter. This directly maps to the class in the code.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the class name and parameter, it fails to mention the core TextTiling algorithm functionality and keyword extraction based on term frequency, which are crucial aspects of the implementation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not mentioned by name in the documentation snippet, TopicSegmentationChunking extends ChunkingStrategy. This base class defines the abstract contract for chunking strategies, making it an implicit part of the design.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that TopicSegmentationChunking extends ChunkingStrategy and implements its abstract contract. While it's more concise than the ground truth, it captures the essential inheritance relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates the AsyncWebCrawler class using an asynchronous context manager (async with AsyncWebCrawler(verbose=True) as crawler:) to manage crawling sessions for dynamic content.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's use of async context manager, but misses crucial functionality around dynamic content crawling, JavaScript execution, and custom extraction strategies that are core to the class's purpose as described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'await crawler.arun(...)' to perform the crawling operation, making use of the arun() method of AsyncWebCrawler to fetch, process, and extract content from the target URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality of arun() but misses the crucial aspect of session management and dynamic content handling that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example explicitly calls 'await crawler.crawler_strategy.kill_session(session_id)' for session cleanup, which corresponds to the kill_session() method of the AsyncPlaywrightCrawlerStrategy class.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session() is called to clean up the session at the end of the crawling example, which aligns with the ground truth's description of the method cleaning up browser resources.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet defines an extraction schema and creates an instance of JsonCssExtractionStrategy using that schema. This extraction strategy is used to parse commit information from the crawled pages.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that JsonCssExtractionStrategy uses a schema to extract data from HTML pages. While it omits some implementation details about BeautifulSoup that are mentioned in the ground truth, it correctly identifies the main purpose and relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler by passing a custom 'user_agent' and custom 'headers'. This demonstrates how the crawler\u2019s identity is configured.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of identity management through customizing user agent and headers in AsyncWebCrawler, which aligns with the ground truth's explanation of how the class implements identity management via its arun method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example clearly calls the 'arun' method on the AsyncWebCrawler instance to perform a crawl on a given URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality but misses the core identity management aspect (user agent and header customization) that is central to the ground truth relationship",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the documentation snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy. This strategy handles the underlying browser configuration including identity management (user-agent and headers).",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncPlaywrightCrawlerStrategy handles identity management through user agents and headers. The predicted description mentions this is done via browser configuration, while the ground truth adds implementation details about the specific methods, but the core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, which is used by AsyncWebCrawler, extends AsyncCrawlerStrategy. This base class defines the abstract interface for crawler strategies, including identity management.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base abstract class that defines the interface for crawler strategies, including identity management functionality, which aligns with the ground truth's description of it defining the interface for crawler identity management.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(...)' with the parameters process_iframes=True and remove_overlay_elements=True. This directly maps to the AsyncWebCrawler.arun() method (artifact_id 5) which is responsible for handling the crawl operation as demonstrated in the usage example.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as handling crawl operations, but describes a direct mapping of iframe parameters when they are actually handled through kwargs parameter passing to crawler_strategy.crawl()",
      "error_type": "minor_implementation_detail"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler (artifact_id 4) by default initializes its crawler_strategy as an instance of AsyncPlaywrightCrawlerStrategy. When arun() is called with process_iframes=True and remove_overlay_elements=True, the underlying crawl() method of AsyncPlaywrightCrawlerStrategy handles these flags by invoking its process_iframes and remove_overlay_elements functionalities. Thus, AsyncPlaywrightCrawlerStrategy is implicitly involved.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that AsyncPlaywrightCrawlerStrategy handles iframe content processing through its process_iframes functionality when process_iframes=True is passed. While the predicted text is less detailed about the specific implementation, it accurately describes the high-level relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy to extract structured data from the OpenAI pricing page. The strategy is constructed with parameters such as provider, api_token, schema, extraction_type, and instruction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that LLMExtractionStrategy is used to extract structured data using provider-based LLM models, and the specific example of extracting OpenAI pricing data aligns with the ground truth. While it doesn't explicitly mention Pydantic schema, it does reference the schema parameter and structured data extraction, which conveys the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy, making ExtractionStrategy the base interface for all extraction strategies. This linkage is implicit in the documented usage because the extraction strategy conforms to the interface defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the base class, but misses the crucial aspect of parallel processing logic and structured data extraction functionality that is core to the relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example uses AsyncWebCrawler within a context manager (async with) to initiate the web crawling process, indicating that this class is central to the crawling workflow documented.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling and its usage with context managers, but misses key functionality around HTML processing and structured data extraction that are central to the class's purpose as described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform the crawling and extraction. This method is responsible for fetching the page and processing it using the provided extraction strategy.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that arun() is used for crawling and extraction, it misses significant aspects covered in the ground truth like caching, error handling, and the specific purpose of extracting OpenAI pricing data using LLMExtractionStrategy",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example accesses 'result.extracted_content' to retrieve the structured data extracted from the crawled page. This attribute of the CrawlResult model holds the final output of the extraction process.",
    "ground_truth_relationship": "The extracted_content property stores the LLM-extracted structured data (model fees) as a JSON string which is then parsed and saved to a file in the example documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that extracted_content contains the structured data extracted from the crawled page in JSON format. The predicted description captures the core functionality even if it doesn't mention the specific use case of model fees.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls the 'arun()' method (via 'crawler.arun(url=...)') on the crawler object to retrieve the crawl result. This method is responsible for initiating the crawl process and returning the result containing metadata.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called to retrieve crawl results, but misses the crucial aspect that it processes the webpage content and returns metadata fields through the CrawlResult object's metadata property",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "After obtaining the result from arun(), the snippet accesses the 'metadata' attribute (result.metadata) to retrieve metadata such as title, description, keywords, author, published date, modified date, and language. This attribute is part of the CrawlResult public interface.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that metadata is accessed via result.metadata and contains various page metadata fields, aligning with the ground truth's explanation that it's an optional dictionary storing extracted page metadata.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (async with AsyncWebCrawler() as crawler), which serves as the entry point for the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage but misses its core functionality regarding extraction strategies and content processing, which is a crucial aspect highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the arun() method on the AsyncWebCrawler instance (e.g., pattern_result = await crawler.arun(...)) to perform asynchronous crawling for both structured and semantic extractions.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of arun() being used for asynchronous crawling and extraction, which aligns with the ground truth's description of it executing core crawling functionality for both pattern-based and LLM-based extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet demonstrates structured extraction by creating an instance of JsonCssExtractionStrategy with a defined article_schema to extract the 'title' and 'content' from an article.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship of JsonCssExtractionStrategy implementing structured extraction using a schema with selectors, even though it doesn't mention all implementation details",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet shows semantic analysis by instantiating LLMExtractionStrategy with parameters (provider, schema, instruction) to perform LLM-based extraction on the article content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as a semantic analysis tool that uses LLM-based extraction with provider, schema, and instruction parameters, which aligns with the ground truth's description of implementing semantic analysis functionality with these same key components.",
      "error_type": null
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both JsonCssExtractionStrategy and LLMExtractionStrategy extend the abstract base class ExtractionStrategy. Although not directly mentioned in the snippet, ExtractionStrategy is an essential part of the extraction functionality as it defines the extraction interface used by these strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the abstract base class that both JsonCssExtractionStrategy and LLMExtractionStrategy extend, establishing the core interface relationship. While it doesn't explicitly mention the parallel processing functionality, it captures the essential inheritance and interface relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses the 'extracted_content' attribute from the crawl result objects (pattern_result and analysis_result) to retrieve and parse the structured and semantic extraction results as JSON.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that extracted_content stores and provides access to the extracted data from both pattern and LLM extraction results, which is then parsed as JSON. While more concise than the ground truth, it conveys the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet accesses the 'media' attribute from one of the crawl results (pattern_result.media) to obtain media-related information from the crawl, which is then included in the final returned JSON.",
    "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the media property stores/contains media assets from crawling and is included in the final returned results of the extract_article_content function",
      "error_type": ""
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(...)' in its example. This clearly maps to the 'AsyncWebCrawler.arun()' method which is responsible for performing the crawl and returning results.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic relationship between the documentation and arun() method, but misses the key functionality of HTML-to-markdown conversion which is a crucial aspect mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example prints 'result.markdown', explicitly accessing the 'markdown' attribute of the CrawlResult object that is returned by the arun() method. This confirms that the markdown conversion is part of the documented output.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main concept that markdown is an output attribute of the CrawlResult that contains converted text, even though it focuses more on the example usage rather than the type definition.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with 'verbose=True', indicating that this class is directly used to enable detailed logging. The use of this parameter makes it clear that verbose mode is an intended feature of AsyncWebCrawler.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the verbose parameter in AsyncWebCrawler enables detailed logging functionality. The predicted description captures the essential relationship that verbose=True enables detailed logging, which aligns with the ground truth's explanation of how verbose controls conditional print statements for tracking crawler status.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "After instantiating AsyncWebCrawler, the snippet calls its 'arun()' method to perform crawling on the specified URL. This explicitly demonstrates the use of the arun method as part of the public interface for executing a crawl operation.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the general usage of arun() method for crawling, while the ground truth specifically describes the verbose logging functionality implemented in the code for debugging purposes. These are different relationships/functionalities.",
      "error_type": "wrong_functionality"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly shows the instantiation of an AsyncWebCrawler object with a 'proxy' parameter, demonstrating how to configure proxy settings. This is explicit because the class is directly referenced in the code example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler handles proxy configuration, but incorrectly states it's through a direct 'proxy' parameter when it's actually handled through the crawler_strategy parameter as noted in the ground truth.",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet shows a direct call to the 'arun' method on the AsyncWebCrawler instance (e.g., 'result = await crawler.arun(url=\"https://example.com\")'), indicating that this method is used to perform the crawling action after the proxy configuration is applied.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses the key aspect of how it integrates with proxy configuration through the crawler_strategy component",
      "error_type": "missing_key_component"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (async with AsyncWebCrawler(verbose=True) as crawler:), making it a key entry point for the session-based crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on AsyncWebCrawler's context manager usage, while the ground truth emphasizes its role in supporting custom execution hooks through crawler_strategy. While both discuss AsyncWebCrawler, the predicted misses the core functionality relationship.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "Within the sample code, the set_hook() method is directly called on crawler.crawler_strategy to register a custom 'on_execution_started' hook. This hook ensures that new page content is detected prior to further processing.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality - the set_hook method is used to register custom hooks for execution during crawling, which matches the ground truth's explanation of storing hook callbacks for execution at specific points.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The code snippet calls kill_session() on crawler.crawler_strategy to terminate the crawling session with a given session_id, representing an essential cleanup or session management operation.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly indicate that kill_session() is used for cleanup/termination of browser sessions, with the ground truth providing more implementation details but maintaining the same core concept.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The arun() method is invoked within the loop (result = await crawler.arun(...)) to perform the actual crawling of pages. This method fetches the crawl result which is then processed to extract commit elements.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a crawling method but misses the crucial aspect of enabling custom hook execution through crawler_strategy that is central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The extracted_content attribute of the CrawlResult is accessed in the sample code (result.extracted_content.select(...)) to further process the crawl results, even though this property is not explicitly described in the documentation text.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the scraped HTML content that the hook functions in the documentation wait for and validate before proceeding with further crawling operations.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that extracted_content contains the scraped HTML content that is processed after crawling. The predicted description focuses on its usage while the ground truth provides more context about hooks, but the core relationship understanding is aligned.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the snippet, the crawler_strategy property of AsyncWebCrawler defaults to an instance of AsyncPlaywrightCrawlerStrategy. This concrete class implements the hook setting and session management methods used in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles hooks and session management, but mischaracterizes it as being accessed through AsyncWebCrawler's crawler_strategy property rather than focusing on how the class itself implements the hook functionality directly.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This base class defines the interface and abstract methods (like set_hook) upon which the advanced hook functionalities are built, even though it is not directly invoked in the snippet.",
    "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class with abstract methods including set_hook, but misses the core purpose of the set_hook method for enabling custom hook functionality during crawling stages as described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun()' with the 'screenshot=True' parameter. This indicates that the 'arun' method of the AsyncWebCrawler is being used to initiate a crawl that supports screenshot capture.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method handles screenshot functionality through the screenshot parameter. While it doesn't mention all implementation details like storing in screenshot_data, it captures the core relationship of how screenshots are enabled through the method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The snippet accesses 'result.screenshot' after the crawl, expecting a Base64 encoded image. This makes use of the 'screenshot' attribute defined in the CrawlResult data model.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage image as a base64-encoded string that can be decoded into a PNG file as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the screenshot attribute stores a base64-encoded image that can be accessed via result.screenshot, which aligns with the ground truth's explanation of the screenshot functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned in the snippet, the 'screenshot=True' parameter in the call to 'arun()' triggers the underlying screenshot functionality. This functionality is implemented in AsyncPlaywrightCrawlerStrategy, which defines the 'take_screenshot' method with enhanced error handling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core functionality: the AsyncPlaywrightCrawlerStrategy class implements screenshot capability through its take_screenshot method, handling both successful captures and errors, with base64 encoding of the output. The predicted description is less detailed but doesn't contradict the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")', which directly maps to the AsyncWebCrawler.arun() method. This method is responsible for initiating the crawled request and processing the response.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method being called, but misses the crucial aspect that it processes and returns metadata fields through the CrawlResult object, which is a key part of the relationship shown in the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "After calling 'crawler.arun()', the returned result's 'metadata' attribute is accessed. This attribute holds the extracted page metadata (title, description, keywords, author, published date, modified date, language) as shown in the snippet.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the metadata attribute stores extracted page metadata from a crawled page, matching the ground truth's explanation. Both descriptions align on the key fields available in the metadata dictionary.",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'RegexChunking' class and shows a usage example where it is instantiated and its 'chunk' method is called. This indicates a direct relationship between the documentation and the RegexChunking implementation.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the existence of RegexChunking and its basic purpose, but fails to describe the key implementation detail of iterative pattern splitting that is central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Since 'RegexChunking' extends 'ChunkingStrategy', the abstract base class 'ChunkingStrategy' is indirectly involved in the functionality described in the documentation. It defines the public interface for chunking strategies that 'RegexChunking' implements.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture that ChunkingStrategy is an abstract base class that defines the interface/contract that RegexChunking must implement through the chunk() method. The predicted description accurately conveys this inheritance and implementation relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy from 'crawl4ai.extraction_strategy' and demonstrates its usage for extracting structured data across multiple LLM providers. This direct reference shows that LLMExtractionStrategy is the class responsible for LLM-based extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's role in extraction and provider support, but misses the crucial aspect of how it implements provider flexibility through constructor parameters and authentication handling",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the documentation snippet, it forms the foundation for LLMExtractionStrategy, thereby playing an implicit but essential role in the extraction framework.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship that LLMExtractionStrategy is a subclass of ExtractionStrategy and accurately implies its role as a foundation for extraction implementations. While more concise than the ground truth, it captures the essential hierarchical relationship without contradicting the ground truth's elaboration on specific LLM providers.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun(...)' with the parameters simulate_user=True and override_navigator=True. This explicitly ties the documentation to the AsyncWebCrawler.arun() method which is designed to accept such anti-detection options.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that the arun() method accepts anti-detection parameters like simulate_user and override_navigator as shown in the documentation example. While it focuses on the example call rather than explaining the full parameter flexibility, it conveys the same essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the documentation snippet, the parameters simulate_user and override_navigator passed to AsyncWebCrawler.arun() are processed by the underlying crawler strategy. Specifically, AsyncPlaywrightCrawlerStrategy (which is instantiated by AsyncWebCrawler) checks for these options in its crawl() method to inject anti-detection scripts.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the code handles anti-bot detection through simulate_user and override_navigator parameters that trigger script injection and user interaction simulation. The predicted description correctly identifies that these parameters are processed in the crawl() method to implement anti-detection features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates the AsyncWebCrawler class (using 'async with AsyncWebCrawler(verbose=True) as crawler:') to create a crawler object.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncWebCrawler enables async with syntax, but misses explaining that this functionality comes from the implementation of __aenter__ and __aexit__ methods. While it shows how to use the class, it doesn't explain the underlying mechanism.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the arun() method is explicitly called on the AsyncWebCrawler instance ('result = await crawler.arun(url=\"https://www.nbcnews.com/business\")'), indicating its role in performing the web crawl operation.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function call relationship but misses significant aspects of the implementation details - particularly the async functionality, content extraction, and error handling that are core to the method's purpose as described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example prints the 'markdown' attribute with 'print(result.markdown)'. This indicates that after crawling, the result object (an instance of CrawlResult) exposes the extracted content through its markdown attribute.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute/property of the CrawlResult object contains the extracted content that gets printed in the example.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a usage example where the 'arun' method of the crawler is called to start a crawl with an extraction strategy. This directly maps to the AsyncWebCrawler.arun() method.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method usage but misses the core focus on error handling patterns and caching that are central to the ground truth relationship",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "In the code example, the 'success' attribute is checked to determine if the crawl result indicates a successful extraction.",
    "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic use of the success attribute for checking extraction results, but misses the key connection to error handling best practices and its role in supporting conditional verification that is emphasized in the ground truth",
      "error_type": "incomplete_context"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet prints 'result.error_message' when the extraction fails, indicating its purpose in error reporting.",
    "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies error_message's role in error reporting, but misses that it's an optional field enabling the error handling practice shown in docs. Also focuses only on printing rather than the broader error handling purpose.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The extracted content is further processed by passing it to json.loads, which shows its role in holding the parsed extraction results.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that extracted_content holds extraction results that can be parsed as JSON, with the predicted description capturing the key relationship even if it's less detailed",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation advises using CSS for structured data extraction. This recommendation maps to the JsonCssExtractionStrategy, which applies CSS selectors to extract data.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the JsonCssExtractionStrategy implements the documentation's recommendation of using CSS selectors for structured data extraction",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The recommendation to 'Use LLM for complex interpretation' clearly directs users to an LLM-based extraction approach, implemented by the LLMExtractionStrategy.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM-based extraction functionality, while the ground truth describes error handling implementation. These are different aspects of the code with no meaningful overlap.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The guideline 'Try Cosine for content relevance' directly implies the use of a cosine similarity\u2013based extraction method, as implemented in CosineStrategy.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that CosineStrategy uses cosine similarity, but misses that it specifically uses this for filtering documents based on semantic relevance scores. The implementation detail about filtering is a crucial aspect of how the strategy works.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned, ExtractionStrategy is the abstract base that underlies all extraction strategies. Its role is implicit in the design of the specialized strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes ExtractionStrategy as an abstract base class but misses its crucial role in implementing the documented strategy selection framework and parallel processing functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows usage of AsyncWebCrawler by calling 'async with AsyncWebCrawler(proxy=...) as crawler'. This demonstrates that AsyncWebCrawler is the primary interface for users to configure proxies directly.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler is used to configure proxies, even though it focuses on usage examples rather than implementation details. The core relationship of AsyncWebCrawler accepting and handling proxy configuration is accurately conveyed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when not provided with a specific crawler_strategy, instantiates AsyncPlaywrightCrawlerStrategy by default. This strategy class processes the 'proxy' and 'proxy_config' keyword arguments, configuring the underlying browser via its proxy logic.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the class handles proxy configuration, but incorrectly states it's part of AsyncWebCrawler's default instantiation rather than focusing on the strategy class's actual proxy implementation through ProxySettings objects",
      "error_type": "misattribution_of_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. Although the base class itself does not implement proxy configuration, it defines the abstract interface for crawling strategies. This makes it part of the overall chain that handles proxy configurations indirectly.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that AsyncCrawlerStrategy enables proxy functionality through its abstract interface, even though they phrase it slightly differently. The predicted description acknowledges the class's role in handling proxy configurations indirectly through its interface, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and uses it to create an extraction_strategy instance with the advanced schema. This demonstrates its direct usage for extracting structured product data.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for extraction, but misses the core schema-based BeautifulSoup processing mechanism described in the ground truth. It focuses more on usage than the actual mechanism.",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly imported in the snippet, this base class is implicitly involved in the advanced extraction process.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, and acknowledges its role in the extraction process. While more concise than the ground truth, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses AsyncWebCrawler as an asynchronous context manager to perform the crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's use as an async context manager, but misses crucial aspects about its primary functionality for web crawling, caching, and schema extraction that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the AsyncWebCrawler context, the arun() method is explicitly called to execute the crawl and retrieve a CrawlResult.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic call relationship with arun() but misses crucial aspects about the method's full functionality including extraction strategies, caching, and structured JSON output that are core to its purpose as shown in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet verifies the crawl's success by asserting result.success. This direct attribute access indicates it is part of CrawlResult's public interface.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that result.success is a boolean property used to verify successful completion of the crawling operation, with the predicted description capturing the core relationship even if less detailed",
      "error_type": null
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation snippet accesses the extracted_content attribute of the CrawlResult to obtain the structured JSON output containing product data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content contains product data, but misses the crucial point that it's a raw string requiring JSON parsing rather than directly accessible structured data",
      "error_type": "omitted_key_step"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and creates an instance by passing a schema. This demonstrates its use for pattern\u2010based extraction of repeated elements such as news articles.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of JsonCssExtractionStrategy - using schema-based pattern extraction for repeated elements. While it doesn't mention the recursive/nested aspect, it correctly describes the main purpose and usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code calls the arun() method on the crawler instance (an AsyncWebCrawler object) passing the extraction_strategy parameter. This method is responsible for executing the crawl and processing the extraction strategy.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() accepts and uses the extraction_strategy parameter, but misses the key aspect of processing structured data extraction according to the schema's selectors and field patterns",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although the base class is not explicitly referenced in the snippet, its presence is implied by the inheritance relationship, defining the contract for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the crucial aspects about parallel processing and pattern-based content extraction that are core to the base class's functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows 'async with AsyncWebCrawler() as crawler:', explicitly instantiating the AsyncWebCrawler class to initiate a crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core async context manager usage with AsyncWebCrawler but misses mentioning the key proxy-related functionality and crawler_strategy integration that enables proxy rotation as highlighted in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code calls 'result = await crawler.arun(url=url)', directly using the 'arun' method to perform the actual crawling operation.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is the core method being used for crawling operations, which aligns with the ground truth's explanation of arun() as implementing the core crawling functionality. While the ground truth provides more detail about proxy support, this is a minor detail that doesn't affect the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although the snippet explicitly calls 'crawler.update_proxy(proxy)' (a method not directly defined in the provided artifacts), the functionality to handle proxy settings is managed internally. AsyncWebCrawler instantiates AsyncPlaywrightCrawlerStrategy, which holds and applies proxy configurations via its 'proxy' attribute during browser startup.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the core relationship that the AsyncPlaywrightCrawlerStrategy class handles proxy configuration through its internal mechanisms, enabling proxy support for web crawling. The predicted description accurately notes the proxy handling functionality, even though it mentions a method not directly shown.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows creating an instance of AsyncWebCrawler with the browser_type parameter set to either 'chromium', 'firefox', or 'webkit'. This directly demonstrates the public interface of AsyncWebCrawler for browser configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's browser configuration capability but describes it as a direct browser_type parameter, while the ground truth indicates it's implemented through the crawler_strategy parameter. This is a meaningful implementation difference.",
      "error_type": "interface_misunderstanding"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "When AsyncWebCrawler is instantiated without an explicit crawler_strategy, it defaults to creating an instance of AsyncPlaywrightCrawlerStrategy using the provided keyword arguments (including browser_type). This makes AsyncPlaywrightCrawlerStrategy an implicit part of the browser configuration chain.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncPlaywrightCrawlerStrategy's role in browser configuration but incorrectly focuses on its relationship with AsyncWebCrawler rather than its own internal browser type selection functionality described in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. This inheritance forms the interface that governs the crawling behavior and browser configuration, making AsyncCrawlerStrategy a foundational element in the configuration chain.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify AsyncCrawlerStrategy as an abstract base class that defines the interface/contract for browser-based crawling functionality. The predicted description captures the essential relationship even if it doesn't explicitly mention all browser types.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses AsyncWebCrawler in the async with block to create a crawler instance for crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used with async/await syntax, but misses the crucial aspect of customizable extraction strategies and the main purpose of content extraction shown in the documentation example.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun()' method on the AsyncWebCrawler instance to perform the asynchronous crawl operation, making its usage explicit in the example.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() method usage but misses the key aspect of content filtering/extraction using LLMExtractionStrategy that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The extraction_strategy parameter is set by instantiating LLMExtractionStrategy with specific arguments (provider, api_token, instruction) to extract technology\u2010related content, which is explicitly demonstrated in the example.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that LLMExtractionStrategy is used for content extraction with specific parameters, but misses the core implementation detail that it processes HTML through LLM models, which is a crucial aspect of how the functionality works",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy. Although not directly instantiated in the snippet, ExtractionStrategy is implicitly part of the extraction chain by serving as the base class for LLMExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that ExtractionStrategy serves as the base class that enables LLMExtractionStrategy to perform content extraction, with the predicted description accurately capturing the inheritance relationship and the base class's role in the extraction framework",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the example accesses 'result.extracted_content' to obtain the JSON data. This attribute usage is implicitly traced as it is a key part of the public interface of the crawl result.",
    "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that extracted_content stores the crawled and filtered content that can be accessed as JSON data. The predicted description focuses on the interface aspect while the ground truth elaborates more on the specific example, but they fundamentally describe the same relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows an instantiation of CosineStrategy with configuration parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, linkage_method, top_k, model_name, verbose) that match the constructor signature of CosineStrategy. This confirms that the snippet directly refers to this class.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that the CosineStrategy class constructor parameters match those shown in the documentation snippet, with the same parameter names and default values.",
      "error_type": null
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy. Although ExtractionStrategy is not explicitly mentioned in the snippet, the inheritance implies that the configuration settings for CosineStrategy are also governed by the contract defined in ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core relationship - that ExtractionStrategy is a base class that provides the foundation for configurable extraction strategies like CosineStrategy, with configuration happening through kwargs. The predicted description accurately captures this inheritance and configuration relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the 'async with AsyncWebCrawler(headless=True) as crawler:' syntax. This indicates direct usage of the AsyncWebCrawler class as the starting point for the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the syntax of creating an AsyncWebCrawler instance, while missing the core functionality described in the ground truth about handling protected sites. However, it does correctly identify that AsyncWebCrawler is the main class being used.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the arun() method of the AsyncWebCrawler instance is explicitly called with parameters such as 'magic=True', 'remove_overlay_elements=True', and 'page_timeout=60000'. This demonstrates the usage of the arun() method for crawling operations.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method usage correctly but misses key aspects like error handling, caching, and customizable parameters that are central to the ground truth's description of how it enables protected site crawling",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet directly accesses the 'markdown' attribute on the result object (i.e., result.markdown) to obtain the extracted content. This attribute is part of the CrawlResult class and forms part of its public interface.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the markdown attribute stores/provides the extracted content from crawling, and is accessed via result.markdown after a successful crawl",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks the 'success' boolean attribute (result.success) to determine if the crawling operation was successful before returning the markdown output. This attribute is a key part of the CrawlResult object's public interface.",
    "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the success boolean controls whether markdown content is returned or not when crawling protected sites",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates the AsyncWebCrawler class using 'async with AsyncWebCrawler(headless=False) as crawler:'. This direct usage confirms that AsyncWebCrawler is a primary component for handling crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as a key component for crawling operations, but misses the core functionality being discussed - the handling of overlay removal through the remove_overlay_elements parameter in arun()",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the 'arun()' method is explicitly called on the AsyncWebCrawler instance (i.e., 'result = await crawler.arun(...)'). This method orchestrates the crawl operation and processes parameters such as remove_overlay_elements and screenshot.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the arun() method is responsible for processing overlay removal along with other crawling configurations like screenshot and word_count_threshold. The predicted description accurately captures the core functionality relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler strategy. Although not directly mentioned in the snippet, the 'remove_overlay_elements' parameter is handled within this strategy's crawl() method, thereby indirectly implementing the overlay removal functionality described in the documentation.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncPlaywrightCrawlerStrategy handles overlay removal functionality as documented, even though it doesn't detail the specific implementation. The core relationship between the code and documented functionality is preserved.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, inheriting its abstract interface for methods such as crawl(), take_screenshot(), and set_hook(). This inheritance forms the contractual backbone of the crawling functionality used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract interface with core crawling methods, but it focuses on inheritance relationships rather than the strategy's role in overlay removal and content fitting emphasized in the ground truth",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates the AsyncWebCrawler class using an async context manager (async with AsyncWebCrawler(verbose=True) as crawler:). This directly demonstrates how to set up and utilize the crawler in an asynchronous environment.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that the AsyncWebCrawler class supports async context manager usage for setup/teardown, which is exactly what the ground truth describes via the __aenter__/__aexit__ implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the documentation snippet the user code directly calls 'crawler.arun(url=...)', which maps exactly to the 'arun' method in AsyncWebCrawler. This method initiates the crawling process and returns a CrawlResult object.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method and its basic crawling functionality, but misses the crucial markdown extraction aspect that is central to the ground truth relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation example prints 'result.fit_markdown', which directly accesses the 'fit_markdown' field defined in the CrawlResult class. This attribute holds the main content extracted and converted to markdown, as described in the document.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown stores main content extracted and converted to markdown format, which aligns with the ground truth's explanation of its purpose and functionality",
      "error_type": "none"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly shows an import and instantiation of JsonCssExtractionStrategy with a defined schema to extract pattern-based data from repetitive elements.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of JsonCssExtractionStrategy - using a schema to extract pattern-based data from repetitive elements, which aligns with the ground truth's explanation of iterating through elements matching selectors according to a schema.",
      "error_type": null
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy, meaning that by using JsonCssExtractionStrategy the contract defined by ExtractionStrategy is implicitly used.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy providing the core framework for derived classes like JsonCssExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code sample calls the arun() method on the crawler instance, which is responsible for executing the crawl using the provided extraction strategy.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() executes crawling functionality, but misses the key relationship with JsonCssExtractionStrategy for structured data extraction using CSS selectors, which is a crucial aspect highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet directly accesses 'result.extracted_content' after crawling, using it to load the JSON produced from the extraction.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic access of extracted_content but misses the crucial pattern-based nature of the extraction and how the JSON is structured according to the schema pattern",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates CosineStrategy with parameters (semantic_filter, word_count_threshold, sim_threshold). This shows direct usage of the CosineStrategy class, which implements an extraction strategy.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that CosineStrategy class is imported and instantiated with configurable parameters for content filtering, which matches the ground truth's explanation of the class implementation and usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Even though ExtractionStrategy is not directly mentioned in the snippet, CosineStrategy extends it, making ExtractionStrategy a critical part of the employed extraction strategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that CosineStrategy extends ExtractionStrategy, it misses the crucial point about the base class defining core interface and parallel processing functionality. The inheritance relationship is mentioned but the functional aspects are omitted.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet demonstrates the use of AsyncWebCrawler by entering an async context with 'async with AsyncWebCrawler() as crawler:', indicating its direct instantiation for crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core async context manager usage but misses the crucial aspect of extraction strategy implementation and customization that is central to the documented functionality",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet calls the 'arun' method on the AsyncWebCrawler instance to perform the crawl, passing in the URL and the extraction strategy. This indicates the usage of the 'arun()' method.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling with URL and extraction strategy, but misses crucial aspects like the asynchronous nature, configurable thresholds, and CrawlResult return type that are key parts of the relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet retrieves the extracted content by accessing the 'extracted_content' property on the result object returned by the 'arun()' method, showing explicit usage.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately describes how the content is accessed but misses the key aspect of semantic filtering and clustering strategy that produces the content",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(...)' with a 'css_selector' parameter. This explicitly demonstrates the use of the 'arun()' method to extract content based on CSS selectors.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the arun() method supports CSS selector-based content extraction through its css_selector parameter. While it's more concise than the ground truth, it conveys the same essential functionality without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet uses a variable named 'crawler' to call 'arun()'. Although the class name is not explicitly mentioned in the snippet, it is implicitly understood that 'crawler' is an instance of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the existence of a 'crawler' variable and arun() method usage, while missing the core relationship about CSS selector functionality being used to target specific HTML elements. However, it does correctly identify the class and method involved.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of an AsyncWebCrawler instance using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This indicates that the crawler is being created to manage a web crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the wrong aspect - it only describes the basic instantiation of the AsyncWebCrawler, while the ground truth describes the class's error handling implementation through the arun() method and its compatibility with retry decorators for API failures.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)', which directly invokes the arun() method on the AsyncWebCrawler instance to perform the crawling operation and retrieve results.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() method call but misses the core focus on error handling implementation that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "Within the usage example, LLMExtractionStrategy is instantiated with parameters (such as provider, api_token, and instruction) and passed as the extraction_strategy. This directly ties the LLM-based extraction approach to the crawling process.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on instantiation and usage of LLMExtractionStrategy, while the ground truth describes the error handling and parallel processing implementation details of the class",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl operation, the snippet accesses 'result.extracted_content' to retrieve the JSON string which is then processed by json.loads. This indicates that the extracted_content attribute of CrawlResult is used to hold the output of the extraction.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that extracted_content is used to store the output data from the extraction process, which aligns with the ground truth's explanation of it storing text data retrieved from web pages through LLM extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy, which is explicitly instantiated in the snippet, is a subclass of ExtractionStrategy. This base class sets the interface for extraction strategies, implicitly supporting the instantiated LLMExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class, but misses the crucial aspect of its role in the retry-based error handling system that the ground truth emphasizes. The prediction focuses more on inheritance structure while missing the key resilient processing functionality.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'arun' method on a crawler instance (i.e. 'crawler.arun(url=\"https://example.com\")'). This call is directly mapped to the AsyncWebCrawler.arun() method which initiates the crawling process and returns a CrawlResult containing analyzed page data.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is used to execute crawling and returns a CrawlResult, which aligns with the ground truth's explanation of how it powers the link analysis features. The main relationship and functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet accesses the 'links' attribute from the result returned by the crawler.arun() call. This attribute, defined in the CrawlResult class (artifact id 14), stores categorized links such as internal and external links, which aligns with the documented link classification functionality.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that links are stored as categorized data (internal/external) in the links attribute of the CrawlResult class, which aligns with the ground truth's explanation of the dictionary structure for storing categorized link data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(...)' to initiate the crawling process. This method is responsible for cleaning noise from web pages by applying parameters such as word_count_threshold, excluded_tags, and remove_overlay_elements.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method handles web page content cleaning through parameters like word_count_threshold and excluded_tags, which matches the ground truth's explanation of how the method implements the documented cleaning features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "In the provided code snippet the cleaned HTML content is accessed via 'result.cleaned_html'. This attribute is part of the CrawlResult class and holds the processed, noise-free HTML output.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that cleaned_html holds processed, cleaned HTML content after noise removal. While it doesn't list all cleaning steps mentioned in the ground truth, it conveys the main functionality correctly.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet also shows that the markdown version of the cleaned content is accessed via 'result.markdown'. This public attribute in the CrawlResult class holds a markdown representation of the cleaned page content.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies markdown as an attribute that provides a markdown version of content, but misses the key aspect that it's specifically a cleaned version with noise elements removed",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using an async context manager with a 'proxy_config' parameter, demonstrating its use for authenticated proxy scenarios.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler accepts and uses proxy configuration for authenticated proxy connections through the async context manager pattern, which aligns with the ground truth's explanation of the class's proxy handling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform crawling, making it a direct part of the usage example.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as part of AsyncWebCrawler but misses the core proxy support relationship described in the ground truth. While not wrong, it fails to capture the key functional relationship with proxy configuration.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy. The proxy configuration passed to AsyncWebCrawler is forwarded to AsyncPlaywrightCrawlerStrategy, which handles 'proxy_config' in its initialization, thereby supporting authenticated proxy usage.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the code handles authenticated proxy support by using proxy configuration (server, username, password) during browser initialization. While the predicted description takes a higher-level view mentioning AsyncWebCrawler's usage, it captures the core functionality of authenticated proxy support.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This inheritance relationship ensures that the authenticated proxy support implemented in AsyncPlaywrightCrawlerStrategy conforms to the interface specified by AsyncCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncCrawlerStrategy as defining an interface, but incorrectly focuses on a specific implementation (AsyncPlaywrightCrawlerStrategy) that isn't mentioned in the code or documentation. The core proxy authentication capability is present in both descriptions.",
      "error_type": "introduced_unmentioned_implementation"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with a 'headers' keyword argument, demonstrating its use for setting custom security headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts custom headers through instantiation, and demonstrates the usage through the headers parameter. While it doesn't explicitly mention the kwargs passing to AsyncPlaywrightCrawlerStrategy, this is an implementation detail that doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "After instantiating AsyncWebCrawler, the snippet explicitly calls the arun() method to perform the crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality of arun() but misses the key relationship regarding custom header configuration described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when instantiated without an explicit crawler_strategy, internally creates an instance of AsyncPlaywrightCrawlerStrategy. The custom headers passed to AsyncWebCrawler are forwarded to this strategy, making its use implicit in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on AsyncWebCrawler's implicit usage of AsyncPlaywrightCrawlerStrategy and header forwarding, while the ground truth specifically describes how AsyncPlaywrightCrawlerStrategy implements custom header functionality through specific methods. While related, they describe different aspects of the header handling mechanism.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows usage of the 'arun' method on a crawler instance with different wait_for parameters (e.g., 'css:.dynamic-content' and 'js:...'). This demonstrates that the method is designed to handle waiting conditions for page elements.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method handles waiting conditions for page elements through wait_for parameters, matching the ground truth's explanation of the method's waiting functionality implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not referenced by name in the documentation snippet, the wait condition functionality (both CSS-based and JavaScript-based waiting) is implemented in AsyncPlaywrightCrawlerStrategy. Its 'crawl' method invokes the 'smart_wait' helper to process the 'wait_for' parameter, which underlies the examples provided.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements the wait conditions functionality through its smart_wait method, handling both CSS and JavaScript-based waiting as shown in the documentation. The description captures the core relationship between the class and the documented functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the crawl interface (including the wait_for capability). AsyncPlaywrightCrawlerStrategy extends this base class and provides the actual implementation for waiting conditions, contributing indirectly to the functionality described in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that AsyncCrawlerStrategy provides the base interface that enables wait functionality through its crawl method, even if implemented differently by concrete classes. The predicted description adds detail about PlaywrightCrawlerStrategy but this doesn't contradict the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly names 'LLMExtractionStrategy' and shows its usage in the code snippet to extract structured data using language models.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the LLMExtractionStrategy and its basic purpose of extracting structured data using language models, but misses crucial aspects of its implementation like chunking, threading, and error handling that are central to how it actually processes HTML content",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy. Although ExtractionStrategy is not mentioned explicitly in the snippet, it is an essential base class that underpins LLMExtractionStrategy's functionality.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy and recognizes it as an essential base class relationship, which aligns with the ground truth's explanation of ExtractionStrategy providing the core infrastructure for LLMExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example calls 'crawler.arun()' to perform crawling with the extraction strategy. This implies that the extraction strategy (LLMExtractionStrategy) is used within the asynchronous crawling workflow.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() works with extraction strategies, but doesn't capture the full scope of its role in handling URL fetching, caching, and HTML processing that enables the extraction strategy to work",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates the usage of the 'arun' method by calling 'crawler.arun' with the 'html2text' parameter. This explicitly shows how to configure markdown conversion options when crawling.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that the html2text customization options are passed to the arun() method as parameters to control webpage content conversion. The predicted description captures this core relationship accurately, even if it doesn't mention all implementation details.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not directly named in the snippet, the 'crawler' variable is implicitly an instance of the 'AsyncWebCrawler' class. Its 'arun' method is invoked, indicating that the class is being used to perform the crawling and conversion operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler class and its arun method, but misses the key aspect of HTML-to-text customization functionality through kwargs parameters, which is the main point of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it in an async 'with' context to initialize a crawler, directly demonstrating its usage for web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler in an async context, but misses the core relationship where AsyncWebCrawler implements the specific cryptocurrency price extraction functionality through its arun() method and JsonCssExtractionStrategy integration",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the 'arun' method is called on the AsyncWebCrawler instance to perform the crawling operation, affirming its role as a key method in the workflow.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun as a key method for crawling but misses crucial aspects about its role in applying extraction strategies and returning structured data as shown in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The example code explicitly imports and instantiates JsonCssExtractionStrategy with a defined schema, indicating that it is used to extract structured data from the webpage.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept that JsonCssExtractionStrategy uses a schema to extract structured data, but misses crucial details about processing cryptocurrency data specifically using CSS selectors for table rows, names, symbols, and prices",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although the base class is not directly mentioned in the snippet, its role is fundamental as JsonCssExtractionStrategy inherits its interface and behavior.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is a subclass of ExtractionStrategy and inherits its interface. While it's more concise than the ground truth, it captures the core inheritance relationship that enables the functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The method 'arun' returns an object whose properties (such as 'success' and 'extracted_content') are utilized in the snippet to evaluate the crawling outcome. This indicates the usage of the CrawlResult class as part of the public interface.",
    "ground_truth_relationship": "The CrawlResult class stores the extracted cryptocurrency data from Coinbase in its extracted_content field, which is used in the example code to verify successful extraction and parse the crypto prices into a structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that CrawlResult stores and provides access to crawled data, with the extracted_content field being key for accessing the results. The predicted description focuses on the interface aspect while the ground truth gives a specific example, but the core relationship is consistent.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation snippet shows usage of 'result.media' which implies that the 'result' variable is an instance of CrawlResult. Although the snippet does not directly name the CrawlResult class, it is the container that exposes all crawl-related data including video and audio metadata.",
    "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that result.media field of CrawlResult stores media metadata including video and audio content, which aligns with the ground truth's explanation of the relationship between CrawlResult class and its media dictionary field.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example code explicitly accesses 'result.media' to iterate over video and audio content. This directly references the 'media' attribute of the CrawlResult class, which is defined to store a dictionary of media elements including videos and audios with their metadata.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that media is a dictionary storing video and audio metadata that can be accessed and iterated over. The predicted description correctly identifies the high-level structure and purpose.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'arun()' method by showing its invocation (result = await crawler.arun(url='https://example.com')). This makes it clear that the 'arun()' method is central to initiating a crawl and returning a CrawlResult object.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that arun() returns a CrawlResult object, but it focuses only on showing how to invoke the method rather than explaining that it processes webpage content through extraction/chunking strategies to populate the CrawlResult properties.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The snippet explicitly states that the 'arun()' method returns a 'CrawlResult' object. This object aggregates all response properties, making it the primary container for the crawl results.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is the container class that holds all the crawl response properties, which aligns with the ground truth showing CrawlResult as a class defining these properties through type-annotated fields.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The code sample demonstrates accessing 'result.html' to retrieve the raw HTML. Although not separately described in text, its usage in the snippet implies that this property is part of the CrawlResult object.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that html is a property of CrawlResult that contains raw HTML content. The predicted description correctly identifies the relationship from the code example and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet shows 'print(result.cleaned_html)' to display the cleaned version of the crawl's HTML output. Its inclusion in the code usage implies its relevance in the response schema.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that cleaned_html is a property/output that contains a cleaned/sanitized version of HTML content. The predicted description captures this core relationship through the example usage, while the ground truth provides the formal definition.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Access of 'result.markdown' in the code sample implies that the CrawlResult object also produces a markdown version of the content. This property offers an alternative content format.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that CrawlResult has a markdown property for accessing content in markdown format. While it's less detailed than the ground truth, it accurately describes the main functionality.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The code sample prints 'result.fit_markdown' to show the most relevant markdown content extracted. This property is implicitly referenced through its usage in the snippet.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that fit_markdown contains markdown content, but misses the key aspect that it specifically contains only the most relevant content and that it's an optional string property",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet includes a check 'print(result.success)' to verify if the crawl was successful. This boolean property is used to gauge the outcome of the operation.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that success is a boolean property indicating whether the crawl operation was successful, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The code sample demonstrates that 'result.status_code' is used to print the HTTP status (such as 200 or 404), which is an essential part of the response metadata.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that status_code represents the HTTP status code (like 200 or 404) returned from the web request, which matches the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "By printing 'result.media', the snippet shows that the CrawlResult object contains a dictionary of media elements like images, videos, and audio. Its usage is inferred from the example.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies media as containing media elements like images/videos/audio, but describes it as being accessed via result.media from CrawlResult rather than recognizing it's a dictionary being defined directly in the code",
      "error_type": "misunderstood_implementation_context"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation snippet shows 'print(result.links)' to access internal and external links found during crawling. This access implies that the CrawlResult object includes a structured representation of links.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the CrawlResult object provides access to internal and external links through the links property, which matches the ground truth's explanation of the implemented dictionary storing both types of links.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation example directly calls the arun() method on the crawler (i.e. 'crawler.arun(...)'). This method is explicitly used to initiate the crawl process with timing parameters like page_timeout and delay_before_return_html, thereby controlling timing of interactions.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation - that arun() is used for web crawling with timing control parameters. While it doesn't explicitly mention **kwargs, this is a minor implementation detail that doesn't affect the core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Internally, the AsyncWebCrawler.arun() method relies on an instance of AsyncPlaywrightCrawlerStrategy to perform the actual crawling. This class\u2019s crawl method uses parameters such as page_timeout and delay_before_return_html to control timing, which supports the documented behavior even though it is not directly mentioned in the usage example.",
    "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncPlaywrightCrawlerStrategy implements timing control through page_timeout and delay_before_return_html parameters in its crawl method, matching the core relationship described in the ground truth. While the predicted version includes extra context about AsyncWebCrawler.arun(), it doesn't contradict the main timing control functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of AsyncWebCrawler with a 'browser_type' parameter for different browsers (Firefox, WebKit, Chromium by default). This directly maps to the AsyncWebCrawler class, indicating its role in selecting the browser engine.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles browser selection and mentions the different browser types, but incorrectly states that browser_type is passed directly to AsyncWebCrawler when it's actually handled through the crawler_strategy parameter using PlaywrightCrawlerStrategy.",
      "error_type": "implementation_misconception"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage examples, the 'arun' method is called on an instance of AsyncWebCrawler to perform the crawl operation. This indicates that the arun() method is the interface used to initiate crawling, and it is explicitly referenced in the documentation snippet.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the method for crawling, but misses the key relationship about how browser selection actually works through the crawler_strategy object and browser_type parameter",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun()' with parameters, which explicitly maps to the 'AsyncWebCrawler.arun()' method. This method is responsible for initiating the crawl with an extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() accepts strategy parameters, but misses the key point about combining different strategies sequentially for enhanced extraction as demonstrated in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the example, the variable 'css_strategy' is passed as the extraction_strategy for the first call to 'arun()'. Although not named explicitly, the purpose of providing an initial CSS-based structural analysis strongly implies use of a CSS extraction strategy. 'JsonCssExtractionStrategy' fits this role as it extracts elements using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately identify that JsonCssExtractionStrategy implements CSS-based extraction as part of a potential multi-strategy approach, with the predicted description correctly inferring its role in the documented combination pattern",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The second call to 'arun()' uses 'llm_strategy' as the extraction_strategy parameter for semantic analysis. This implies the use of a strategy that leverages language models, corresponding to the 'LLMExtractionStrategy' class.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy uses language models for semantic analysis and corresponds to the 'llm_strategy' parameter shown in the documentation. While it doesn't mention all implementation details, it captures the core relationship between the class and its documented usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler with 'from crawl4ai import AsyncWebCrawler' and uses it to create an instance via an asynchronous context manager. This clearly indicates direct usage of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler and its instantiation, but misses key aspects about session handling and persistent browser sessions that are central to the ground truth's description of the relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the 'arun' method of AsyncWebCrawler is called to perform the crawling. The call includes parameters such as 'session_id', 'js_code', and 'css_selector', demonstrating explicit utilization of the arun() method.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is used for crawling and mentions its parameters, but misses the key relationship about session management and state maintenance across multiple requests that is central to the ground truth",
      "error_type": "key_concept_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example calls 'crawler.crawler_strategy.kill_session(session_id)' to close a session. Although AsyncPlaywrightCrawlerStrategy is not directly imported, AsyncWebCrawler creates an instance of it as the default crawler_strategy. This makes the kill_session() method an implicit but necessary component of session management.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that kill_session() is used for session cleanup/closure, which aligns with the ground truth's explanation of implementing session cleanup by closing page/context objects and removing sessions from memory.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation example prints the count of '.content-item' occurrences by accessing 'result.extracted_content'. This attribute, part of the CrawlResult class, is explicitly used to check the extracted content from the crawling process.",
    "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that extracted_content stores/contains the HTML content matching the CSS selector '.content-item' and is used to count found items after crawling. The predicted description captures the core relationship and usage shown in the example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly invokes 'crawler.arun()' with the 'js_code' parameter to execute JavaScript commands. This method, represented by the AsyncWebCrawler.arun() artifact, is the entry point for the example.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the entry point for JavaScript execution, but misses the crucial detail that it forwards js_code to crawler_strategy.crawl() which actually performs the execution",
      "error_type": "missing_key_mechanism"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within AsyncWebCrawler.arun(), the 'js_code' parameter is forwarded to the crawler strategy. The AsyncPlaywrightCrawlerStrategy artifact implements the 'crawl' method which evaluates the provided JavaScript code (via page.evaluate). This is an implicit dependency supporting JavaScript execution.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that JavaScript code can be executed through the js_code parameter and is evaluated using page.evaluate(). While it mentions this as an 'implicit dependency', it describes the same fundamental functionality as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation text explicitly introduces 'FixedLengthWordChunking' and describes its function to split text into chunks based on a fixed number of words. The usage example provided shows the instantiation and use of the FixedLengthWordChunking class, directly evidencing its role in the documentation.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the class's purpose of splitting text into chunks, but fails to explain the implementation details of using word tokenization and list slicing that are central to the ground truth description",
      "error_type": "crucial_implementation_omission"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "FixedLengthWordChunking is implemented as a subclass of ChunkingStrategy. Although the documentation does not explicitly mention ChunkingStrategy, the inheritance relationship implies that the base class defines the interface used by FixedLengthWordChunking, making it indirectly relevant.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that FixedLengthWordChunking is a subclass of ChunkingStrategy and implements its interface, which aligns with the ground truth's description of the inheritance relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet is titled 'Tips for Using JsonCssExtractionStrategy', explicitly naming this class. This class uses a CSS selector (via BeautifulSoup's select method) to extract blocks from HTML, which directly supports the advice to inspect and test selectors.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the class uses CSS selectors via BeautifulSoup to extract data, and both emphasize the connection to the documentation's focus on CSS selector inspection and testing.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although not mentioned by name in the documentation, its role as the foundational interface for extraction strategies makes it indirectly relevant to understanding and applying the tips for JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but incorrectly focuses on JsonCssExtractionStrategy specifics which aren't shown in the code. The ground truth better captures the actual functionality shown - parallel processing and error handling capabilities.",
      "error_type": "incorrect_focus"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation advises to check the 'result.success' flag as part of error handling. This flag is a member of the CrawlResult class, making it indirectly relevant for users to handle potential failures based on the extraction result.",
    "ground_truth_relationship": "The CrawlResult class implements error handling guidance from the documentation through its 'success' boolean flag and 'error_message' field, allowing developers to check crawl status and handle failures as recommended.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the success flag's role in error handling, but misses mentioning the error_message field which is an important part of the error handling functionality described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions JsonCssExtractionStrategy by displaying its instantiation with a CSS-based schema. The code sample shows the import and creation of a schema for product listings, which directly demonstrates how this class extracts data using CSS selectors.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses CSS selectors for data extraction, and shows how it's instantiated with a schema. While it doesn't mention BeautifulSoup specifically, it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the base class is inherent in the design of extraction strategies, linking it to the CSS-based extraction approach.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core inheritance relationship between ExtractionStrategy as the base class and JsonCssExtractionStrategy as a subclass, which aligns with the ground truth's description of the basic architectural relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the code sample, the asynchronous method 'arun' is called on the 'crawler' object with the extraction_strategy parameter. This indicates that the crawling process, which integrates the extraction strategy, is managed by AsyncWebCrawler.arun().",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the arun method accepts and integrates an extraction strategy parameter into the crawling process, which aligns with the ground truth's explanation of how arun() implements the execution logic for processing web pages using extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager, indicating its role as the primary crawler component for dynamic content extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncWebCrawler's role in content extraction and its async context manager functionality, but misses the crucial aspects of JavaScript execution capabilities and coordination with extraction strategies that are central to the ground truth description.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method on an AsyncWebCrawler instance to execute the crawling process, integrating JS execution and subsequent content extraction.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately mentions arun() and its crawling functionality, but misses the key aspect of handling dynamic content through JavaScript execution and LLM-based extraction described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly creates an instance of LLMExtractionStrategy, supplying parameters such as the provider, API token, and an instruction for summarization, to perform LLM-based content extraction.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy as a class that handles LLM-based content extraction with configurable parameters, which aligns with the ground truth's description of its implementation and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy internally as its crawling strategy for executing JavaScript and handling dynamic page content.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes that AsyncPlaywrightCrawlerStrategy handles JavaScript and dynamic content, but incorrectly states it's used internally by AsyncWebCrawler as a default when this is not shown in the code or documentation.",
      "error_type": "unsupported_assumption"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, the default strategy used by AsyncWebCrawler, is an implementation derived from AsyncCrawlerStrategy, which defines the abstract contract for asynchronous crawling.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class, but introduces an unverified implementation detail about AsyncPlaywrightCrawlerStrategy being the default strategy, which isn't shown in the code or documentation",
      "error_type": "unverified_implementation_detail"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy to provide a concrete implementation for LLM-based extraction. ExtractionStrategy defines the contract for extraction strategies used within the crawler.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship - that ExtractionStrategy is a base class that provides the foundation for different extraction strategies, including LLMExtractionStrategy. Both emphasize the inheritance relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses the 'extracted_content' attribute from the result returned by AsyncWebCrawler.arun(), which holds the LLM-extracted summaries of the crawled articles.",
    "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that extracted_content stores LLM-processed summaries of crawled articles that are then handled as JSON data. The predicted description captures the core functionality and relationship, even if it doesn't mention all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'crawler.arun()' method with parameters such as 'wait_for', 'js_code', 'process_iframes', and 'delay_before_return_html'. This direct call indicates that the AsyncWebCrawler.arun() method is responsible for handling dynamic content as shown in the example.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method handles dynamic content through its parameters, which aligns with the ground truth's explanation of how it implements dynamic content handling via kwargs to support wait conditions and JavaScript execution.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The parameters 'wait_for', 'process_iframes', 'js_code', and 'delay_before_return_html' used in the snippet are internally processed within the 'crawl' method of AsyncPlaywrightCrawlerStrategy. Although the documentation does not call this class directly, it is used by AsyncWebCrawler.arun() (as seen in its internal call to self.crawler_strategy.crawl()) to implement dynamic content handling and lazy-loaded content.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the code implements dynamic content handling through methods (smart_wait, crawl) that support JavaScript execution, iframe processing and configurable delays. The predicted description adds implementation details about AsyncWebCrawler.arun() but gets the core relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call in the usage example ('await crawler.arun(...)'), which explicitly references the arun() method from the AsyncWebCrawler class. This method is the entry point for performing a crawl operation and producing the result used in the snippet.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the main entry point for crawling, but misses the core HTML cleaning/sanitization functionality that is the focus of the ground truth documentation",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The usage example prints 'result.cleaned_html', directly accessing the 'cleaned_html' property of the CrawlResult instance. This property represents the sanitized HTML output as described in the documentation.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that cleaned_html contains sanitized HTML output after removing unwanted elements and attributes, with the predicted description accurately reflecting the property access shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'JsonCssExtractionStrategy' as it discusses advanced extraction features using JSON and CSS selectors, which directly relates to this class's functionality in parsing the HTML structure shown.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that JsonCssExtractionStrategy processes HTML using JSON and CSS selectors, but misses the crucial point about BeautifulSoup and schema-based field mapping for structured data extraction",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the text, 'ExtractionStrategy' is the abstract base class that 'JsonCssExtractionStrategy' extends, making it an essential part of the extraction functionality demonstrated by the documentation snippet.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class but focuses primarily on its relationship to JsonCssExtractionStrategy, while the ground truth more accurately describes its core purpose of extracting and processing structured HTML data.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun()' with parameters such as 'js_code' and 'wait_for' to handle dynamic content. This directly maps to the 'arun' method defined in the AsyncWebCrawler class (artifact_id 5).",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles dynamic content through js_code and wait_for parameters, which matches the ground truth's explanation of handling JavaScript execution for dynamic content and form interactions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The variable 'crawler' used in the code snippet is an instance of the AsyncWebCrawler class (artifact_id 4), which provides the 'arun' method. Although not explicitly named in the snippet, its usage is inferred from the method call.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and arun method relationship, but misses the crucial dynamic content handling functionality (scrolling, form interactions, wait states) that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within AsyncWebCrawler's constructor, if no alternate crawler_strategy is provided, an instance of AsyncPlaywrightCrawlerStrategy (artifact_id 1) is created. This strategy handles the execution of JavaScript (js_code) and waits for specific conditions (wait_for), which is the core of the dynamic content handling demonstrated in the snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncPlaywrightCrawlerStrategy handles JavaScript execution and wait conditions, but incorrectly frames it as part of AsyncWebCrawler's constructor rather than focusing on how the strategy itself implements dynamic content handling through smart_wait.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy (artifact_id 1) extends AsyncCrawlerStrategy (artifact_id 0), thereby inheriting the abstract definition of crawling methods. This relation is crucial as it defines the expected interface for handling dynamic content, including js_code execution and waiting logic.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining an interface for crawling methods, but incorrectly assumes a relationship with AsyncPlaywrightCrawlerStrategy which is not shown in the code or documentation.",
      "error_type": "unsupported_assumption"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with 'proxy' and 'headers' parameters to combine with Magic Mode. This shows that the user is directly using the AsyncWebCrawler class as the entry point for crawling with enhanced anti-detection features.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts proxy and headers parameters and is used for crawling with anti-detection features, but incorrectly suggests these parameters are passed directly to AsyncWebCrawler's constructor rather than through kwargs to AsyncPlaywrightCrawlerStrategy.",
      "error_type": "implementation_detail_misunderstanding"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows an explicit call to the 'arun' method on an AsyncWebCrawler instance with parameters such as 'url' and 'magic=True'. This indicates that the arun() method is the mechanism by which crawling is performed with anti-detection features enabled.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the main crawling method, but misses the key relationship about how it implements the proxy and magic mode functionality through crawler_strategy and kwargs parameters",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler automatically uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy. This strategy processes parameters like 'magic=True' to activate anti-detection behaviors such as simulating user interactions.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly mentions that the class handles magic mode for anti-detection, but misses the core relationship between proxy configuration and magic mode working together for enhanced protection that is central to the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that AsyncPlaywrightCrawlerStrategy extends. This base class defines the interface required for crawling strategies, supporting the implementation that enables Magic Mode anti-detection features.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class that defines the interface for crawling functionality, which enables the advanced features (like Magic Mode) mentioned in the ground truth through its abstract methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun' with parameters (process_iframes=True, remove_overlay_elements=True). This directly maps to the AsyncWebCrawler.arun() method which is intended to initiate a crawl and process such flags.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts and processes iframe-related parameters, even though it doesn't explicitly mention the kwargs implementation detail.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, when the 'process_iframes' and 'remove_overlay_elements' flags are set to True, the AsyncWebCrawler.arun() method internally delegates functionality to its strategy. AsyncPlaywrightCrawlerStrategy implements the crawling process, including the extraction of iframe content (via its process_iframes method) and removal of overlay elements (via its remove_overlay_elements method).",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship that AsyncPlaywrightCrawlerStrategy handles iframe content processing when process_iframes=True is set. While it adds some extra context about overlay removal, it accurately describes the core functionality of extracting iframe content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'arun' method by showing how a js_code parameter is passed to the crawler, thereby executing custom JavaScript commands before crawling.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for JavaScript execution, but incorrectly states that js_code is passed directly to arun() when the code shows it's actually handled through **kwargs and crawler_strategy.crawl()",
      "error_type": "implementation_detail_mismatch"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the snippet, the js_code parameter is handled by the 'crawl' method within AsyncPlaywrightCrawlerStrategy. This strategy is the underlying implementation used by AsyncWebCrawler.arun() to execute custom JavaScript.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of JavaScript execution through js_code parameter in the crawl method, matching the ground truth's explanation of how custom JavaScript can be executed. While it doesn't detail the ability to handle arrays of commands, this is a minor omission that doesn't change the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy, which defines the crawl interface that includes handling the js_code parameter. This inheritance is integral to the overall custom JavaScript execution workflow.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawling, but incorrectly refers to an AsyncPlaywrightCrawlerStrategy that isn't shown in the code and implies inheritance that isn't demonstrated in the context.",
      "error_type": "unsubstantiated_extension"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows the instantiation of AsyncWebCrawler using an asynchronous context manager (using 'async with AsyncWebCrawler() as crawler'). This explicitly demonstrates session-based usage where the 'session_id' is intended to persist state between multiple crawl requests.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions describe the same core functionality of session management in AsyncWebCrawler, where session_id is used to maintain state between multiple crawl requests. The predicted description accurately captures the key concept of using session_id for persistence across requests, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code snippet, the 'arun()' method is called on the AsyncWebCrawler instance, passing a session_id. This explicit call demonstrates how the crawler makes a crawl request while reusing the session state maintained by the instance.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that the arun() method handles session management by using a session_id parameter to maintain state across requests. The predicted description correctly identifies the basic relationship even though it doesn't mention all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet ends with a call to 'kill_session' on the crawler's strategy (accessed via 'crawler.crawler_strategy.kill_session(session_id)'). This explicit invocation cleans up the session associated with the given session_id, ensuring that session state is properly terminated.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that kill_session is used to clean up and terminate sessions, which aligns with the ground truth's explanation of how it closes page/context objects and removes session data.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy to configure an LLM-based extraction strategy for processing article content with a defined schema and instruction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of using LLMExtractionStrategy for content extraction, but fails to emphasize the key functionality of parsing web content into structured format based on the schema, which is a crucial aspect highlighted in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the abstract base class for all extraction strategies is implicit in the use of LLMExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy as base/child classes, and acknowledges ExtractionStrategy's role as the abstract base class. While it's more concise than the ground truth, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on a crawler object to initiate the crawling process using the specified extraction strategy, indicating direct usage of AsyncWebCrawler.arun().",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core crawling functionality of arun() but misses the crucial LLM-based extraction aspect described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After invoking the crawl operation, the snippet accesses the 'extracted_content' attribute from the result to obtain the JSON formatted extracted data, clearly demonstrating its use.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content contains JSON formatted data, but misses the crucial aspect that it specifically matches a defined Pydantic schema and is meant for typed object parsing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with custom identity parameters (user_agent and headers). This shows how the crawler's appearance (its identity) is controlled via its constructor arguments.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that AsyncWebCrawler handles identity management through customization parameters, it incorrectly states this is done via constructor arguments when the ground truth indicates it's actually implemented through the arun method parameters.",
      "error_type": "implementation_location_error"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code calls the 'arun' method on the AsyncWebCrawler instance to initiate the crawling process. This method is responsible for applying the provided identity parameters (such as user_agent and headers) during the crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of arun() method handling identity parameters (user_agent) during crawling, which aligns with the ground truth's description of identity management implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler internally defaults to using AsyncPlaywrightCrawlerStrategy when no custom crawler_strategy is provided. This strategy is responsible for handling low-level details including applying the custom user_agent and headers, thus realizing identity management.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that identity management is handled through custom user agents and headers in the AsyncPlaywrightCrawlerStrategy. The predicted description accurately mentions the core functionality, even if it doesn't detail all the specific methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, whose abstract method update_user_agent defines the contract for modifying crawler identity. This ensures that the custom identity parameters from AsyncWebCrawler are ultimately enforced.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions focus on the core relationship that AsyncCrawlerStrategy defines the interface for managing crawler identity through abstract methods like update_user_agent(), which enables customizing how the crawler appears to websites. The predicted description captures this key functionality, even though it mentions a specific implementation (AsyncPlaywrightCrawlerStrategy) that isn't in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit usage example calling 'crawler.arun(...);' with keyword parameters such as 'exclude_external_links' and 'exclude_social_media_links'. This indicates that the 'arun()' method is the primary entry point for initiating a crawl with smart link filtering control.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is the main entry point for crawling with link filtering parameters, and while it doesn't explicitly mention **kwargs, it correctly describes the core functionality of passing filtering parameters to control the crawl behavior.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides a schema in Python that defines a complex, nested extraction structure using keys such as 'baseSelector' and 'fields'. This schema directly mirrors the expected input format for the JsonCssExtractionStrategy, which uses such a schema to perform extraction on HTML using CSS selectors. Although the class name is not explicitly mentioned in the snippet, the structure and intent clearly imply the use of this strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the JsonCssExtractionStrategy class uses a schema with CSS selectors to extract data from HTML, which aligns with the ground truth's explanation of using BeautifulSoup's select() method to match CSS selectors defined in the schema structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The complex extraction schema shown in the documentation is intended to be consumed by an extraction strategy. ExtractionStrategy is the abstract base class that defines the interface for extraction strategies. JsonCssExtractionStrategy, which uses the provided schema format, extends this base class, making ExtractionStrategy an essential component in the extraction chain.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that ExtractionStrategy is the base class that enables schema-based extraction functionality, which aligns with the ground truth's explanation of it providing core functionality for implementing schema-based extraction patterns.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a call to the 'arun' method on the crawler instance using 'await crawler.arun(url=\"https://example.com\")'. This directly maps to the AsyncWebCrawler.arun() method listed in the available artifacts.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the async call syntax rather than the error handling functionality described in the ground truth. While it correctly identifies the arun() method, it misses the core relationship about error handling and status checking.",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The example code checks 'if not result.success:' to determine whether the crawl has failed. This condition explicitly uses the 'success' property from the CrawlResult.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the success boolean property is used for error handling by checking if result.success is false, which aligns with the ground truth's explanation of using the success property to check if a crawl completed successfully.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In the example, when the crawl is not successful, the code prints 'result.error_message'. This directly shows that error_message is expected to provide details about a failed crawl.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that error_message provides details about failed crawls, which matches the ground truth's explanation of its purpose in error handling",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The snippet prints out 'result.status_code' to indicate the HTTP status of the crawl response. This is an explicit dependency demonstrating the use of the status_code attribute from the CrawlResult.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that status_code is used to indicate the HTTP status of crawl responses, which aligns with the ground truth's explanation of using status_code for error handling and diagnosing failed crawls.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly describes a cosine-based extraction strategy that breaks content into chunks, converts them into vector representations, computes cosine similarities, and clusters similar chunks. The CosineStrategy class directly implements these functionalities.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted text correctly captures the core 5-step workflow of the Cosine Strategy as implemented in the code, identifying the key operations of chunking, vectorization, similarity computation, and clustering. While the ground truth provides more implementation specifics about which methods handle each step, the high-level relationship description is aligned.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends the ExtractionStrategy base class. ExtractionStrategy establishes the common interface for extraction strategies, and CosineStrategy fulfills that contract by providing a cosine similarity-based implementation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is an abstract base class that provides the foundation/interface for implementing extraction strategies. The predicted description mentions CosineStrategy as an implementation, while the ground truth provides more detail about the methods, but the fundamental relationship understanding is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example explicitly creates an instance of AsyncWebCrawler using a context manager (async with AsyncWebCrawler(verbose=True) as crawler) to start the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the context manager aspect of AsyncWebCrawler, while the ground truth describes its wait_for parameter functionality. While both describe valid aspects of the class, they're focusing on different core functionalities.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example invokes the arun() method on the AsyncWebCrawler instance to perform crawling, passing parameters such as wait_for, js_code, and extraction_strategy.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() accepts parameters including wait_for, but misses the crucial aspect of how wait_for is used to control page loading conditions through crawler_strategy.crawl()",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The extraction strategy is instantiated as JsonCssExtractionStrategy with a schema to extract commit data from the page. This directly corresponds to the extraction process described in the snippet.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that JsonCssExtractionStrategy implements schema-based extraction functionality. Both descriptions highlight that the strategy uses a schema to extract data from HTML elements, which is the key relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "After the crawling process is complete, the example explicitly calls kill_session() on the crawler's strategy to terminate the session, ensuring proper cleanup.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core idea that kill_session is called to terminate/cleanup the session after crawling, which aligns with the ground truth's explanation of cleaning up browser resources via closing page and context objects. While the ground truth provides more implementation details, the high-level relationship is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned by name in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler strategy. This strategy implements the handling of the 'wait_for' parameter (processing the wait_for condition via its internal smart_wait method) which is central to the documented technique.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship - that AsyncPlaywrightCrawlerStrategy implements the wait_for functionality through its smart_wait method to handle both CSS selectors and JavaScript functions for dynamic content loading. While the ground truth provides more implementation details, the core functionality relationship is correctly identified.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the e-commerce scraping use case, the documentation shows a CSS-based schema with keys such as 'baseSelector' and 'fields' for product listings. Although the snippet does not explicitly name JsonCssExtractionStrategy, its functionality\u2014extracting elements using CSS selectors based on a provided schema\u2014maps directly to what JsonCssExtractionStrategy implements.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that JsonCssExtractionStrategy implements the CSS-based scraping functionality shown in the e-commerce example, with schema fields and selectors. While it's slightly more verbose, it conveys the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation directly creates an instance of LLMExtractionStrategy for news article extraction, providing a provider and a schema from an Article class. This explicit instantiation clearly indicates the use of LLMExtractionStrategy for processing article content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that LLMExtractionStrategy implements schema-based article extraction by taking a provider and schema in its initialization. The predicted description accurately describes the key aspects shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The content analysis use case explicitly instantiates CosineStrategy with parameters such as 'semantic_filter' and 'top_k'. This direct usage indicates that CosineStrategy is employed for topic analysis based on cosine similarity.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for topic analysis using cosine similarity and correctly mentions key parameters like semantic_filter and top_k, which aligns with the ground truth's explanation of content analysis functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet directly references the 'fit_markdown' attribute on the crawl result as a feature that extracts the main content while excluding boilerplate elements. This attribute is showcased in the usage example where 'result.fit_markdown' is printed.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is a property that extracts main content while removing boilerplate elements, which aligns with the ground truth's description of it being an optional string property that stores extracted main content after applying content extraction heuristics.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the code snippet, 'result.markdown' is used to obtain the regular markdown output. This enables a comparison with 'fit_markdown', demonstrating the refinement achieved through smart content extraction.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that markdown is the regular/baseline text extraction compared to fit_markdown's refined extraction, even though it's less detailed than the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet demonstrates the usage of the 'arun()' method by showing an example call to 'crawler.arun(url=\"https://example.com\")'. This method triggers the crawling process, resulting in a CrawlResult containing extraction attributes such as 'fit_markdown'.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() performs crawling and returns a CrawlResult, but misses the core functionality of processing the HTML through extraction/chunking strategies to identify relevant content blocks while filtering boilerplate elements",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example use case shows the instantiation of AsyncWebCrawler using an 'async with' statement. This class provides the high-level interface for initiating the crawling process and ultimately retrieving a CrawlResult with content extraction features like 'fit_markdown'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately mentions AsyncWebCrawler and its connection to fit_markdown functionality, but focuses on instantiation rather than explaining how the class actually implements the fit_markdown feature through content processing and extraction as described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")', which maps directly to the AsyncWebCrawler.arun() method responsible for initiating the crawl and returning the crawl result.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called to perform a crawl, but misses the key functionality about media selection and returning structured media data that can be accessed through the media dictionary",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet accesses 'result.media' to retrieve images, videos, and audios. This directly corresponds to the 'media' attribute defined in the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the media dictionary stores and provides access to different types of media (images, videos, audios), which aligns with the ground truth. While it's less detailed, it doesn't misrepresent the relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet does not explicitly mention the class name 'AsyncWebCrawler', the usage of 'crawler.arun(...)' implies that the 'crawler' variable is an instance of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes media types through its aprocess_html method, where it scrapes and organizes images, videos, and audios into a structured media dictionary that matches the documented media selection interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description only mentions the existence of the AsyncWebCrawler class and arun method, while the ground truth specifically describes how the class handles media processing through aprocess_html and organizes different media types into a structured dictionary matching the documented interface.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet explicitly shows a session cleanup example using 'await crawler.crawler_strategy.kill_session(session_id)'. This call directly maps to the 'kill_session' method implemented in AsyncPlaywrightCrawlerStrategy.kill_session(), which is responsible for cleaning up session resources.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies kill_session as a cleanup method, but focuses more on the API call syntax rather than explaining its core purpose of resource management and memory leak prevention mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet provides usage examples where 'crawler.arun(...)' is called with session_id and other parameters, indicating state management across different crawling steps. This explicitly corresponds to the 'arun' method of the AsyncWebCrawler, documented as AsyncWebCrawler.arun().",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the arun() method handles session management by storing session_id from kwargs to enable stateful crawling across multiple requests. The predicted description accurately captures this core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the use of the AsyncWebCrawler class via the line 'async with AsyncWebCrawler(verbose=True) as crawler:', indicating that this class is directly invoked to create a crawler instance.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the core usage of AsyncWebCrawler class, but misses noting the significant functionality it implements including content filtering, processing, and cache control which are essential aspects mentioned in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the sample code, the method 'arun()' is explicitly called on an instance of AsyncWebCrawler (i.e., 'result = await crawler.arun(...)'), making it a directly referenced element of the API.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as part of AsyncWebCrawler's API but misses the crucial relationship between the implementation details in the code and how they support the documented usage patterns/parameters.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The example uses the return value from 'arun()' by checking properties such as 'result.success', 'result.markdown', 'result.media', and 'result.error_message'. This indicates that the CrawlResult class forms part of the public interface, even though it is not directly instantiated in the snippet.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that CrawlResult class serves as the data structure/return type for the crawler's outputs, including success status, content and other fields that are accessed in the example code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows two examples where CosineStrategy is instantiated with various parameters (such as word_count_threshold, top_k, verbose, semantic_filter, sim_threshold, and max_dist) for adjusting thresholds and optimizing performance. This direct usage in the example code links the documentation to the CosineStrategy class.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify the core relationship between the documentation and CosineStrategy class - that the class implements configurable parameters for controlling text filtering and clustering that align with the documentation's recommended use cases and best practices.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends the abstract base class ExtractionStrategy to implement its extraction logic. Although ExtractionStrategy is not directly mentioned in the documentation snippet, its role as the base class for CosineStrategy establishes an implicit trace relationship that is fundamental for the extraction strategy design.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but focuses on CosineStrategy inheritance rather than the core flexibility and configurability described in the ground truth",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', indicating that this class is directly used to perform the crawl.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is directly used for crawling through an async context manager, which aligns with the ground truth's explanation of the class's core functionality and context manager implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun()' method on the AsyncWebCrawler instance (i.e. 'result = await crawler.arun(url=\"...\")'), showing that this method is used to perform the crawl operation and retrieve results.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and getting results, but misses crucial aspects of the functionality like caching, HTML processing, and the CrawlResult return type that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The returned result from the 'arun()' method is used to access the 'markdown' attribute (via 'result.markdown[:500]'), demonstrating that the markdown content produced by the crawl is part of the public interface.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute stores/contains the extracted text content from the crawl that can be accessed through result.markdown",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with 'verbose=True' in an async with block, demonstrating how a user creates a crawler object to perform web crawling with caching enabled by default.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions the AsyncWebCrawler instantiation and verbose parameter, but misses the core caching functionality relationship described in the ground truth - namely the async_db_manager integration and bypass_cache parameter functionality",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to initiate a crawl, including one call with bypass_cache=True to force a fresh crawl, clearly demonstrating how the caching mechanism is controlled.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship of how caching works in the crawler - the predicted description correctly identifies the bypass_cache parameter's role in controlling caching, which aligns with the ground truth's explanation of caching functionality and demonstration through example code",
      "error_type": ""
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet prints a slice of 'result1.markdown', implicitly referencing the 'markdown' attribute of the CrawlResult object returned by the 'arun()' method. This attribute holds the markdown representation of the crawl result.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that markdown is an attribute/property that stores the crawled content in markdown format. The predicted description correctly captures the core relationship even if it doesn't mention the 100-character printing example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code sample explicitly calls 'await crawler.arun(...)', which directly maps to the AsyncWebCrawler.arun() method.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method but focuses only on its async signature, missing the core relationship about error handling implementation that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result from invoking 'arun()' is checked for properties like 'success', 'extracted_content', and 'error_message', signaling that it is an instance of CrawlResult.",
    "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult instances contain the key properties (success, extracted_content, error_message) used in error handling, which aligns with the ground truth's explanation of how these fields support the error handling flow.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet contains 'if result.success:' which directly accesses the 'success' attribute of the CrawlResult instance.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies the core relationship - that the code checks the 'success' attribute of the result object to determine the outcome. While it omits the error handling context, this is a minor detail that doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code uses 'json.loads(result.extracted_content)' to parse the extracted content, directly referencing this attribute of CrawlResult.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the parsing of extracted_content using json.loads, but misses the key semantic context that this is an optional field used for storing semantically-matched content found by the crawler",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "When the result indicates failure, the error is printed using 'result.error_message', directly linking to this attribute in CrawlResult.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that error_message is used to store and display failure information via result.error_message, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly names 'The Cosine Strategy', which corresponds to the CosineStrategy class used for semantic extraction.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the CosineStrategy class but only mentions semantic extraction in a surface-level way, missing crucial aspects about how it works through cosine similarity and hierarchical clustering that are core to its functionality",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, indicating that extraction strategies in the framework are based on this abstract class even though it is not directly mentioned in the snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies that CosineStrategy extends ExtractionStrategy, but incorrectly suggests this is only implied rather than explicitly shown. The ground truth better captures the base class's purpose in providing foundational structure for different extraction methods.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet directly instantiates JsonCssExtractionStrategy with a defined schema and verbose flag to extract structured data from dynamically loaded content. This class also extends ExtractionStrategy, defining the extraction rules via CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that JsonCssExtractionStrategy is used for structured data extraction using CSS selectors and works with dynamic content loading. The predicted description correctly identifies the core functionality and class inheritance, aligning with the ground truth's explanation of its purpose and usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly instantiated in the snippet, ExtractionStrategy forms the abstract base from which the extraction strategy is derived.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and JsonCssExtractionStrategy, and their roles as abstract base and specialized implementation. While more concise, it captures the same core architectural relationship described in the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet uses 'async with AsyncWebCrawler(verbose=True)' to create a crawler instance for dynamic page processing, indicating direct usage of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description mentions AsyncWebCrawler's basic async usage but misses the crucial functionality related to JavaScript execution, waiting conditions, and custom extraction strategies described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "After instantiating AsyncWebCrawler, the example explicitly invokes the arun() method to launch the crawling process with parameters such as js_code, extraction_strategy, and wait_for, thereby integrating JavaScript execution with content extraction.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that arun() method enables dynamic content extraction through JavaScript execution and extraction strategies, with the predicted description accurately capturing the core functionality even if it phrases it slightly differently",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The result returned from AsyncWebCrawler.arun() is a CrawlResult object from which the extracted_content attribute is accessed to parse the structured data via json.loads.",
    "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that extracted_content contains scraped data from the AsyncWebCrawler. While the predicted focuses on the technical flow and the ground truth emphasizes the specific use case, they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly names 'JsonCssExtractionStrategy' in the header, indicating that this class is intended for advanced extraction scenarios involving complex and nested HTML structures.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies JsonCssExtractionStrategy and its purpose for complex HTML extraction, it omits the crucial technical implementation details about using BeautifulSoup and schema-based selectors that are central to how it achieves this functionality",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the documentation text, 'JsonCssExtractionStrategy' is a subclass of 'ExtractionStrategy'. This base class provides the underlying abstraction for extraction strategies and is thereby implicitly linked to the advanced usage described.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, and acknowledges the base class's role in providing abstraction for extraction strategies. While it's more concise than the ground truth, it captures the essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation code snippet explicitly creates an instance of AsyncWebCrawler using an async context manager (async with AsyncWebCrawler(...)). This demonstrates that the user is directly instantiating this class with advanced browser configurations (e.g., browser_type, headless, verbose, user_agent, headers, and proxy).",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's instantiation and context manager usage, but misses describing the class's full implementation of configuration options and features mentioned in the documentation example. It focuses only on instantiation rather than the complete relationship between class capabilities and documentation usage.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the method arun() is explicitly invoked on the AsyncWebCrawler instance (result = await crawler.arun(...)), indicating its role in performing the crawl operation and retrieving the crawl results.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but it oversimplifies the relationship by only focusing on method invocation while missing the crucial implementation aspects mentioned in the ground truth regarding configuration processing, error handling, caching, and content extraction.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the text, AsyncWebCrawler internally defaults to using AsyncPlaywrightCrawlerStrategy when no custom crawler_strategy is provided. This strategy handles advanced browser configuration parameters such as proxy settings, JavaScript execution, and headless mode.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted text correctly notes that AsyncPlaywrightCrawlerStrategy handles browser configuration and advanced features, it incorrectly states an assumption about AsyncWebCrawler's default behavior which isn't shown in the documentation example. The ground truth more accurately describes the relationship as demonstrating the strategy's features through example usage.",
      "error_type": "unsupported_assumption"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is a subclass of AsyncCrawlerStrategy. Therefore, when AsyncPlaywrightCrawlerStrategy is used, the crawling interface and methods defined in AsyncCrawlerStrategy are inherited and utilized indirectly.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies inheritance but focuses on AsyncPlaywrightCrawlerStrategy which isn't mentioned in the code or documentation. The ground truth correctly describes AsyncCrawlerStrategy's role in defining the interface for browser configurations.",
      "error_type": "introduces_unmentioned_class"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result returned by the arun() method is a CrawlResult object, whose attributes (such as markdown, screenshot, and success) are used in the snippet to build the final returned dictionary.",
    "ground_truth_relationship": "The CrawlResult class defines the structured response object that captures all possible outputs from the crawl_with_advanced_config function, including the markdown, screenshot, and success fields shown in the example's return statement.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that CrawlResult is the return type of arun() and contains the attributes used in the return dictionary. This aligns with the ground truth's explanation of CrawlResult as the structured response object capturing crawl outputs.",
      "error_type": ""
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'SlidingWindowChunking' class through its import statement and usage example. The snippet details parameters (window_size and step) that directly correspond to the constructor parameters in the 'SlidingWindowChunking' class artifact.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the class and its parameters, it focuses mainly on the structural aspects (import and parameters) rather than explaining the core sliding window algorithm and context preservation functionality that the ground truth emphasizes",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'SlidingWindowChunking' class extends the 'ChunkingStrategy' abstract base class. Although the documentation does not explicitly mention 'ChunkingStrategy', this relationship is implicit in the implementation, as it defines the common interface for all chunking strategies.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core inheritance relationship between SlidingWindowChunking and ChunkingStrategy abstract base class, even though it doesn't detail the sliding window implementation specifics.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy from crawl4ai.extraction_strategy and uses it for pattern\u2010based extraction by providing a schema. The code shows its instantiation with a specific schema describing the base selector and field selectors.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy is used for pattern-based extraction using a schema with selectors, which aligns with the ground truth's explanation of how it parses HTML using BeautifulSoup and applies CSS selectors based on the schema",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although the base class is not directly instantiated in the snippet, its role is required as the interface that JsonCssExtractionStrategy extends.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the base class and its relationship to JsonCssExtractionStrategy, but misses the crucial aspect of parallel processing functionality and fails to mention LLMExtractionStrategy as another implementation shown in the documentation.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly imports LLMExtractionStrategy and uses it to instantiate an extraction strategy configured for LLM analysis. Parameters such as provider, schema, and instruction are provided to drive LLM-based content extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy as shown in the ground truth - both describe it as a strategy for LLM-based content analysis with configurable providers, schemas, and instructions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is built as a subclass of ExtractionStrategy, meaning it relies on the standardized extraction interface defined there. This relationship is inherent in its design even though it\u2019s not directly referenced in the snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is a subclass of ExtractionStrategy and uses its standardized interface. While it doesn't mention all the specific methods (extract() and run()) or the other strategy type, it captures the core inheritance and interface relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation example calls 'crawler.arun()' to perform the crawl operation while passing the extraction_strategy parameter. This indicates that the arun() method of AsyncWebCrawler underpins the workflow that integrates page interaction with structured extraction.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is used to perform crawling operations with extraction strategies, matching the core functionality shown in the ground truth. While it's less detailed, it captures the essential relationship between the method and extraction strategy integration.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and instantiates it using the async context manager ('async with AsyncWebCrawler() as crawler'). This clearly demonstrates the usage of the AsyncWebCrawler class for crawling a webpage.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship shown in the ground truth - that AsyncWebCrawler is used for asynchronous web crawling with an async context manager pattern. The implementation details in the ground truth confirm the basic usage pattern described in the prediction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the 'arun' method is called on the AsyncWebCrawler instance (result = await crawler.arun(url=\"https://example.com\")). This call is explicitly used to fetch or crawl the content of the webpage.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for web crawling, but misses crucial aspects like content extraction, caching, and returning a CrawlResult object that are key parts of the method's functionality as described in the ground truth",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After calling the 'arun()' method, the returned object (of type CrawlResult) has its 'markdown' attribute accessed (print(result.markdown)). This attribute is explicitly referenced to output the clean markdown content.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the markdown attribute of the CrawlResult object contains cleaned content and can be accessed after calling arun(), which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of CosineStrategy with parameters such as 'linkage_method', 'max_dist', and 'model_name' in the 'Custom Clustering' section. This demonstrates the use of CosineStrategy to provide custom clustering functionality.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy's clustering functionality and customization through parameters, but misses the important semantic filtering and word count threshold aspects that are crucial to the class's full functionality as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning that all functionality provided via CosineStrategy is built upon the abstract interface defined by ExtractionStrategy. Although not directly mentioned, this inheritance is critical to the extraction process and underpins the advanced clustering functionality.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship where ExtractionStrategy serves as the base class that enables specialized implementations like CosineStrategy, providing the fundamental structure for content extraction features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the 'Content Filtering Pipeline' example, the code snippet uses 'async with AsyncWebCrawler() as crawler:' to create a web crawler instance. This demonstrates that AsyncWebCrawler is explicitly used to initiate an asynchronous crawl, integrating the extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in asynchronous crawling but misses significant aspects like caching, clustering support, and the full range of customizable extraction strategies mentioned in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when instantiated without a specified crawler strategy, defaults to using AsyncPlaywrightCrawlerStrategy. This implicit link ensures that the asynchronous crawling behavior in the pipeline is executed by the concrete AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class enables advanced web content extraction by implementing browser automation features that support the documented custom clustering and content filtering pipelines through its sophisticated crawling capabilities and JavaScript execution support.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on AsyncWebCrawler's default use of AsyncPlaywrightCrawlerStrategy, while the ground truth describes AsyncPlaywrightCrawlerStrategy's core functionality in enabling advanced web content extraction and supporting custom clustering/filtering. The predicted misses the key capabilities.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example explicitly instantiates AsyncWebCrawler using an async context manager ('async with AsyncWebCrawler(verbose=True) as crawler'). This shows direct usage of the AsyncWebCrawler class to manage crawling sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage but misses the core functionality around dynamic content crawling, JavaScript execution, and custom extraction strategies that are central to the class's purpose.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code snippet, the 'arun' method is directly called on the AsyncWebCrawler instance (e.g., 'result = await crawler.arun(...)'). This indicates that the arun() method is responsible for executing the crawl operation with the provided parameters, including handling dynamic JavaScript and session information.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of arun() as the main method for executing crawl operations with dynamic features, which aligns with the ground truth's description of it being the core method for dynamic web crawling with session management.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example calls the 'kill_session()' method on the crawler's strategy (via 'crawler.crawler_strategy.kill_session(session_id)'), explicitly demonstrating how sessions are managed and terminated after completing the crawl.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that kill_session is used to clean up and terminate browser sessions after crawling, with the predicted description accurately capturing the core functionality even if it doesn't mention specific details about closing page and context objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the provided snippet, the extraction strategy is instantiated with a specific schema via 'extraction_strategy = JsonCssExtractionStrategy(schema)'. This clearly shows how content extraction is configured using this strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is instantiated with a schema, but misses the crucial aspect of how it actually implements the extraction logic using BeautifulSoup to parse HTML and apply selectors for extracting commit data.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly names 'NlpSentenceChunking' and shows its usage in a code example. The example creates an instance of NlpSentenceChunking and calls its chunk() method to split text into sentences, clearly demonstrating its role in handling sentence boundaries using NLP models.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that NlpSentenceChunking uses NLP models (specifically NLTK) to split text into sentences with accurate boundary detection, matching the implementation shown in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "While not mentioned by name in the documentation, NlpSentenceChunking extends the ChunkingStrategy abstract base class. This implicit relationship signifies that NlpSentenceChunking adheres to the interface and behavior defined by ChunkingStrategy for text chunking.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the inheritance relationship between NlpSentenceChunking and ChunkingStrategy, including the key point that NlpSentenceChunking implements the abstract interface defined by ChunkingStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates a usage example by calling 'crawler.arun(...)'. The method is invoked with parameters such as 'word_count_threshold', 'remove_overlay_elements', and 'process_iframes', making it the explicit target of the documentation.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that the code implements an async crawler method that accepts the documented options/parameters, even though it doesn't detail all the internal workings described in the ground truth",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not mentioned by its class name in the snippet, the identifier 'crawler' used in 'crawler.arun(...)' implies that it is an instance of 'AsyncWebCrawler', the class that provides the arun() method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between the crawler instance and AsyncWebCrawler class, but misses the main point about the implementation of configuration options through the arun method, which is the key relationship described in the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The options 'remove_overlay_elements' and 'process_iframes' passed to 'arun()' are processed internally by the crawler strategy. By default, 'AsyncWebCrawler' instantiates 'AsyncPlaywrightCrawlerStrategy', which is responsible for handling these options.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the AsyncPlaywrightCrawlerStrategy implements the documented crawling options (remove_overlay_elements and process_iframes) through its internal processing. While it's less detailed than the ground truth, it conveys the same fundamental relationship between the class and its handling of these options.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, establishing the foundational interface and behavior for crawling strategies in the system. This inheritance is part of the internal chain of responsibility for processing crawl options.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the crawler configuration options shown in the documentation through its crawl method's kwargs parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining crawler behavior, but incorrectly states it's about inheritance with AsyncPlaywrightCrawlerStrategy and chain of responsibility, when the ground truth focuses on its role in enabling configuration options through kwargs",
      "error_type": "misattributed_relationship"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler via the line 'async with AsyncWebCrawler() as crawler:', which indicates that it is the entry point for Magic Mode usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in Magic Mode but focuses only on instantiation syntax rather than explaining how it implements the functionality through crawler_strategy and arun() parameters.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example calls the 'arun()' method with 'magic=True' (i.e., 'result = await crawler.arun(url=\"https://example.com\", magic=True)'), demonstrating that this method is directly responsible for enabling Magic Mode anti-detection features.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method handles Magic Mode functionality through its parameters, even if it doesn't explicitly mention the delegation to crawler_strategy. Both descriptions convey that this method is responsible for enabling anti-detection features.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler constructs an instance of AsyncPlaywrightCrawlerStrategy by default. This strategy\u2019s 'crawl' method checks for the 'magic' (or related) parameter in its kwargs to trigger anti-bot protection mechanisms such as overriding navigator properties, simulating user behavior, and other stealth capabilities inherent in Magic Mode.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that Magic Mode is implemented through the magic parameter in the crawl method and combines various anti-bot protection features like navigator property overrides and user behavior simulation. While the predicted description is more tentative in tone, it describes the same core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet shows direct usage examples (e.g., 'strategy = CosineStrategy(sim_threshold=0.8)') and describes parameters such as 'semantic_filter', 'sim_threshold', 'word_count_threshold', and 'top_k'. These parameters are defined in the constructor of CosineStrategy, making it an explicit trace from the document.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the documentation parameters directly correspond to the CosineStrategy class's initialization parameters and provides code examples showing their usage. While it's less detailed about the implementation methods, it correctly identifies the core relationship between the documentation and code.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning it inherits the common interface and functionality for extraction strategies. Although ExtractionStrategy is not directly mentioned in the documentation snippet, its role as the base class is a necessary dependency for CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy serves as a base/template class that other strategy classes inherit from and implement with specific parameter configurations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates CosineStrategy with parameters such as 'semantic_filter', 'word_count_threshold', 'top_k', and 'sim_threshold/max_dist' to demonstrate its role in extracting article content, product reviews, and technical documentation.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy as a configurable text extraction class that uses different parameter combinations for various use cases, matching the ground truth's explanation of how semantic_filter, word_count_threshold, top_k, and sim_threshold parameters enable different extraction scenarios.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is a subclass of ExtractionStrategy. Although the documentation does not mention ExtractionStrategy by name, it is inherently part of the extraction functionality due to inheritance, establishing an implicit relationship.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy as a subclass of ExtractionStrategy, but misses the crucial aspect of ExtractionStrategy's role in providing the foundational structure and parallel processing capabilities for content extraction, which is a key part of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In one of the use cases, the documentation calls 'crawler.arun(...)'. Although the class of 'crawler' is not explicitly named, this method call corresponds to AsyncWebCrawler.arun(), indicating its role in processing the extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a key method being called in the examples, but misses the main point about how it implements the documented use cases through flexible extraction strategy parameters and configurations.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly discusses the schema for extracting data via CSS selectors and JSON output. It mentions advantages such as speed, precision, and structured output which directly correspond to the functionality provided by the JsonCssExtractionStrategy class.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly discusses the schema and advantages of JSON/CSS extraction, but misses the core implementation detail that the code actually uses BeautifulSoup to execute the schema configuration",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a concrete implementation that extends the abstract base class ExtractionStrategy. This base class defines the interface and expectations for extraction strategies, thereby indirectly supporting the schema described in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the core relationship of ExtractionStrategy being an abstract base class that enables concrete implementations to provide specific extraction logic while sharing common functionality. The predicted description correctly identifies the inheritance relationship and the role of the base class in defining the interface.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy to perform LLM\u2010based structured data extraction. This class is responsible for interfacing with LLM providers (e.g., Ollama, HuggingFace) and uses a provided schema to extract entities and relationships. It extends the base ExtractionStrategy, ensuring adherence to the extraction interface.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's basic role in interfacing with LLM providers and using schemas, but omits crucial functionality around chunking, rate limiting, and parallel processing that is core to its implementation according to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code calls 'await crawler.arun(...)' to initiate the crawl process, thereby integrating the extraction strategy into the crawling workflow. This method handles the process of retrieving and processing the webpage content.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic crawling functionality but misses the key aspect of structured data extraction through LLM providers that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling and extraction, the resulting structured data is accessed through the 'extracted_content' attribute of the CrawlResult object. The sample code uses this attribute to load the JSON-formatted structured output.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that extracted_content contains structured data output from extraction that can be parsed as JSON. The predicted description accurately conveys the main functionality without contradicting the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly instantiates AsyncWebCrawler via the 'async with AsyncWebCrawler(verbose=True) as crawler:' statement to perform advanced session-based crawling, serving as the entry point for the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its initialization with context manager syntax, but misses the core functionality of dynamic content crawling, pagination support, and JavaScript injection capabilities that are central to the class's purpose as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the 'arun' method of the AsyncWebCrawler instance is explicitly invoked with various parameters (such as session_id, css_selector, js_code, wait_for, bypass_cache, headless) to perform a crawl operation on a dynamic webpage.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that arun() is used for web crawling with various parameters, which aligns with the ground truth's explanation of it enabling dynamic crawling through those parameters. While the ground truth provides more specifics about JavaScript execution and content updates, the core relationship is correctly identified.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of JsonCssExtractionStrategy with a defined schema (for extracting commit titles) and 'verbose=True', linking it to the extraction of structured content from the crawled page.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that JsonCssExtractionStrategy implements schema-based structured data extraction from HTML using baseSelector and fields to extract commit information from pages. The predicted description captures this essence even if it focuses more on the instantiation rather than the implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet calls 'await crawler.crawler_strategy.kill_session(session_id)' explicitly to terminate an active session, ensuring proper session lifecycle management within the crawling process.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the core purpose of kill_session: terminating/cleaning up an active browser session. The predicted description focuses on explicit session termination while the ground truth provides more implementation details, but the fundamental relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation example explicitly shows the usage of 'result.fit_markdown' to obtain well-formatted markdown content, indicating that the fit_markdown attribute of the CrawlResult object is intended for presenting articles and similar content.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify fit_markdown as being used for formatted article/blog content output, capturing the same core relationship from Best Practice #1",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code snippet demonstrates handling media by filtering images from 'result.media[\"images\"]' based on a relevance score. This directly implies the usage of the 'media' attribute of the CrawlResult model.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the media dictionary property allows filtering images based on relevance scores, which aligns with the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation shows an example where content links are extracted by filtering 'result.links[\"internal\"]' based on their type. This demonstrates direct use of the 'links' attribute in the CrawlResult model.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately describe how the links dictionary enables filtering internal content links by type, capturing the same core relationship between the links structure and its usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls 'crawler.arun()' with specific parameters (word_count_threshold, keep_data_attributes, process_iframes) to clean and process content. This directly indicates the usage of the 'arun()' method from the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and some parameters for content cleaning, but misses key aspects from the ground truth like media processing (screenshots) and link extraction handling that are core to implementing the documented best practices.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation explicitly names and describes the 'TopicSegmentationChunking' class. It details that this class employs the TextTiling algorithm to segment text into topic-based chunks and provides a usage example (importing and instantiating the class with a num_keywords parameter).",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship - that the class uses TextTiling algorithm for topic-based text segmentation, which aligns with the ground truth's implementation description using NLTK's TextTilingTokenizer",
      "error_type": ""
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "While the documentation does not explicitly mention 'ChunkingStrategy', the 'TopicSegmentationChunking' class extends it. This inheritance relationship is implicitly relevant because 'ChunkingStrategy' is the abstract base class that defines the contract for all chunking strategies.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ChunkingStrategy and TopicSegmentationChunking, and accurately notes that ChunkingStrategy is the abstract base class that defines the contract (through the chunk method) that implementing classes must follow.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows the call to 'crawler.arun(url=\"https://example.com\")', which directly invokes the AsyncWebCrawler.arun() method to perform the crawl and return a result.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called to perform crawling, but misses the key aspect that it's responsible for processing and generating the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The code snippet assigns 'raw_html = result.html'. This directly accesses the 'html' attribute of the CrawlResult, which contains the original raw HTML output.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the html property/attribute contains raw HTML content, with the ground truth specifying it's a string type and the predicted describing how it's accessed",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation snippet shows 'clean_html = result.cleaned_html', which clearly maps to the 'cleaned_html' attribute of the CrawlResult. This attribute delivers a sanitized version of the HTML.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that cleaned_html represents a sanitized version of the HTML content, which aligns with the ground truth's explanation of it being a cleaned HTML version stored as an optional string",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet directly accesses 'markdown = result.markdown', mapping to the CrawlResult.markdown attribute which provides the standard markdown format of the crawled content.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown field/attribute provides standard markdown conversion of the crawled HTML content and how to access it via result.markdown",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "In the usage example, 'fit_md = result.fit_markdown' explicitly references the CrawlResult.fit_markdown attribute, which is intended to deliver the most relevant markdown content.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that fit_markdown contains the most relevant content from a webpage in markdown format",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun(...)' with various parameters, including 'word_count_threshold' and content filter options. This usage explicitly maps to the AsyncWebCrawler.arun() method, which is responsible for initiating the crawl and processing the content based on the provided parameters.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method handles content filtering through its parameters, which aligns with the ground truth's explanation of how the method processes and filters crawled content based on user-specified criteria.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the snippet, the variable 'crawler' is used to call the 'arun' method. This implies that the 'crawler' is an instance of the AsyncWebCrawler class. Although the class name is not mentioned explicitly in the snippet, its instance is implied, making AsyncWebCrawler an implicit part of the usage chain.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the existence of a 'crawler' instance variable, while the ground truth describes the filtering parameters implemented in the arun() method. These are completely different aspects of the code's functionality.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an example where the 'arun' method is invoked on a crawler instance with several keyword arguments (such as exclude_domains, exclude_social_media_domains, and exclude_social_media_links). This directly corresponds to the 'AsyncWebCrawler.arun()' method as implemented in the available artifacts.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method accepts keyword arguments and is called on a crawler instance, which aligns with the ground truth's explanation of how the method handles domain filtering options via kwargs. While the prediction is less specific about the domain filtering details, it correctly identifies the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates how to instantiate LLMExtractionStrategy by passing parameters such as provider, api_token, and instruction. This directly ties the documentation to the LLMExtractionStrategy class implementation.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that LLMExtractionStrategy can be customized by passing provider, api_token, and instruction parameters, which matches how the class is implemented.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the text, LLMExtractionStrategy extends ExtractionStrategy, making ExtractionStrategy an underlying dependency that influences the extraction process described in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is a base class that enables custom LLM provider implementation and flexibility. The predicted description correctly identifies the inheritance relationship and its role in the extraction process.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler with 'verbose=True' as seen in 'async with AsyncWebCrawler(verbose=True) as crawler', which clearly shows its direct usage in initiating a crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's instantiation with verbose=True, but misses the core functionality described in the ground truth about integrated JavaScript execution, extraction strategies, and browser session management.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform the page crawling and extraction by passing parameters like 'js_code', 'css_selector', and 'extraction_strategy'.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and extraction with parameters, but misses the core functionality of integrated JavaScript execution and waiting described in the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The example code instantiates a JsonCssExtractionStrategy by passing a schema (with 'baseSelector' and 'fields') along with 'verbose=True'. This clearly demonstrates its intended use for extracting commit information.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that JsonCssExtractionStrategy processes HTML content based on a schema with baseSelector and fields to extract structured data (commit information in this case). The predicted captures the essential functionality, even if it's slightly less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "Within the snippet, the method 'kill_session' is directly invoked on the crawler's strategy via 'crawler.crawler_strategy.kill_session(session_id)', signifying its role in cleaning up or terminating the session after crawling.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core purpose of kill_session being used to terminate/cleanup the session after crawling, which aligns with the ground truth's explanation of cleaning up browser resources by closing page and context objects.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler by using 'async with AsyncWebCrawler() as crawler:'. This shows that AsyncWebCrawler is the starting point for the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class and mentions its usage, but misses the crucial aspect of its multi-format extraction capabilities and different extraction strategies that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet directly calls the 'arun' method on the AsyncWebCrawler instance (e.g., 'result = await crawler.arun(...)') to perform the crawl and retrieve results.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of arun() but misses the crucial aspect that it supports different extraction strategies, which is a key point emphasized in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "Within the snippet, LLMExtractionStrategy is directly instantiated with parameters (provider, schema, instruction) to extract structured data from the crawled content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of LLMExtractionStrategy being used for structured data extraction with parameters (provider, schema, instruction), which aligns with how it's shown being used in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both LLMExtractionStrategy and JsonCssExtractionStrategy extend the abstract base class ExtractionStrategy, which defines the interface for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that ExtractionStrategy is an abstract base class that serves as the foundation for different extraction strategy implementations like LLMExtractionStrategy and JsonCssExtractionStrategy. The predicted description focuses on inheritance while the ground truth emphasizes its foundational role, but they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet instantiates JsonCssExtractionStrategy with a provided schema (represented as 'your_schema') to extract repeated patterns from the content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core purpose of JsonCssExtractionStrategy - using a schema to extract repeated patterns from content. While it's less detailed than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The code retrieves the 'fit_markdown' attribute from the result object (e.g., 'result.fit_markdown') to get the main content in a formatted markdown format.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is used to store/access the main content extracted from a webpage, with the predicted description correctly identifying it as an attribute that returns markdown-formatted content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses 'extracted_content' from the results (e.g., 'llm_result.extracted_content' and 'pattern_result.extracted_content') to obtain JSON string representations of structured and pattern data.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that extracted_content stores crawled data as strings that are parsed as JSON. The predicted description correctly identifies the usage pattern shown in the example code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example returns 'result.media' to include any media information extracted during the crawl, indicating the use of the media attribute from the CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the media information is extracted during crawling and returned via result.media, which aligns with the ground truth's explanation of the CrawlResult.media dictionary being returned in the function output.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly introduces RegexChunking as the chunking strategy to be added. The code example shows an import and instantiation of RegexChunking with a custom regex pattern (patterns=[\"\\n\\n\"]).",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of RegexChunking as a strategy that splits text based on regex patterns, and correctly notes its usage in the documentation example with the '\\n\\n' pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The usage example in the snippet creates an instance of AsyncWebCrawler (using an async 'with' statement) to perform web crawling. This shows that AsyncWebCrawler is the main crawler class used to trigger the operation that accepts a chunking strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler is the main class that handles web crawling operations and accepts a chunking strategy, which aligns with the ground truth's explanation of how AsyncWebCrawler implements RegexChunking functionality through its arun() method. While the predicted version omits some implementation details, it captures the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code example, the 'arun()' method of the AsyncWebCrawler instance is called with the URL and the chunking_strategy parameter set to an instance of RegexChunking. This demonstrates direct invocation of the arun() method to perform crawling.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the usage of the arun() method with chunking_strategy parameter, but misses the crucial point about RegexChunking being the default implementation and the type validation against ChunkingStrategy",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "RegexChunking extends ChunkingStrategy, meaning that it inherits its interface and behavior. Although the snippet does not explicitly mention ChunkingStrategy, this inheritance is inherent and forms the basis of the implementation of RegexChunking.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that RegexChunking extends ChunkingStrategy and inherits its interface, which aligns with the ground truth's description of the inheritance relationship and functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses 'async with AsyncWebCrawler() as crawler:' to create an instance of AsyncWebCrawler. This clearly demonstrates a direct instantiation and usage of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler can be instantiated and used with async context manager, but misses the crucial functionality of handling complex page interactions through parameters like session_id, js_code, and wait_for that are core to the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet calls 'crawler.arun(...)' to perform the crawling action. This shows that the 'arun' method of the AsyncWebCrawler class is directly invoked to load and process page content.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses crucial functionality around dynamic page interactions, JavaScript execution, session management and wait conditions that are central to the method's purpose",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "Within the code snippet, the attribute 'cleaned_html' of the result (which is a CrawlResult) is accessed via 'len(result.cleaned_html)'. This indicates that the returned CrawlResult object provides a cleaned HTML content attribute used for further processing.",
    "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that cleaned_html stores processed HTML content after crawling that can be used for further processing/analysis. The predicted description captures the core relationship even if it doesn't explicitly mention the dynamic page load context.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet demonstrates session cleanup by calling 'crawler.crawler_strategy.kill_session(session_id)'. Although the object's type isn\u2019t explicitly named in the code snippet, the method 'kill_session' directly maps to the implementation in AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that kill_session is used to clean up browser resources/sessions after multi-page interactions, with the predicted version capturing the core cleanup functionality even if it doesn't detail all the specific resources being closed.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example shows the asynchronous instantiation and use of AsyncWebCrawler via 'async with AsyncWebCrawler(...)', indicating that this class is directly involved in setting up the crawling context for user simulation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's async context manager usage, but misses the core relationship that user simulation is implemented through specific parameters (simulate_user and override_navigator) passed to the crawler_strategy",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls the 'arun()' method on the AsyncWebCrawler instance with parameters 'simulate_user=True' and 'override_navigator=True'. This directly demonstrates the use of the method to trigger simulated user behavior.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that the arun() method handles user simulation behavior through parameters. The predicted description focuses on the method call example while the ground truth explains how it works via kwargs, but they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler internally defaults to using AsyncPlaywrightCrawlerStrategy. This strategy\u2019s 'crawl' method processes flags like 'simulate_user' and 'override_navigator' to simulate real user behavior.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncPlaywrightCrawlerStrategy handles user simulation through simulate_user and override_navigator parameters. While it mentions AsyncWebCrawler rather than directly naming AsyncPlaywrightCrawlerStrategy, this doesn't change the fundamental understanding of the user simulation functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, ensuring that it adheres to the expected interface for crawler strategies. This relationship supports the overarching structure used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining an interface, but misses the key aspect about user simulation functionality that is central to the ground truth's description",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(...)' with parameters such as 'word_count_threshold'. This directly maps to the 'AsyncWebCrawler.arun()' method implementation.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic connection between the documented arun() call and its implementation, but misses the core relationship about how content filtering parameters are handled as kwargs and processed during extraction stages.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not directly mentioned by name in the snippet, the 'crawler' variable used in the example is an instance of the 'AsyncWebCrawler' class, which provides the 'arun()' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the crawler variable is an instance of AsyncWebCrawler with an arun method, but misses the core relationship about content filtering functionality that is the main focus of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler' class (artifact 4) initializes its crawler strategy with 'AsyncPlaywrightCrawlerStrategy' by default. This strategy handles the actual crawl operations, including processing of content that may be subject to filtering parameters passed in the 'arun()' call.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements content filtering by accepting configuration parameters like excluded_tags and thresholds that control what elements are included or excluded during web page crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles crawl operations, but misses the core focus on content filtering functionality that the ground truth emphasizes. The prediction focuses more on initialization and general crawling rather than the specific content filtering capabilities.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' (artifact 1) extends the abstract base class 'AsyncCrawlerStrategy', providing the contract for crawl operations. This inheritance structure underpins the crawling and content filtering workflow.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface through which the documented content filtering parameters can be passed as kwargs to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that AsyncCrawlerStrategy is an abstract base class that defines the interface for crawling functionality, including the ability to pass filtering parameters via kwargs. The predicted description captures this core relationship, even though it doesn't detail the specific filtering parameters.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as the tool to extract highly structured data from complex web pages. This indicates that users should employ this strategy for advanced extraction scenarios, where handling default field values and transforms in schemas is vital.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship between the code and documentation - that JsonCssExtractionStrategy implements a schema-based extraction system that aligns with the documented tips for gradual development and testing. The predicted description correctly identifies its role in structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends the abstract base class ExtractionStrategy. Although the documentation does not explicitly mention ExtractionStrategy, it underpins the extraction functionalities (such as handling defaults and transformation of extracted data) that JsonCssExtractionStrategy offers.",
    "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on JsonCssExtractionStrategy extending ExtractionStrategy and mentions data extraction features, while the ground truth specifically describes ExtractionStrategy's parallel processing implementation using ThreadPoolExecutor. These are completely different aspects of the relationship.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides examples for creating nested objects, simple lists, nested lists, and lists of objects using keys like 'type', 'selector', and 'fields'. JsonCssExtractionStrategy directly implements this functionality by accepting a schema (which includes these keys, such as a 'baseSelector' and nested 'fields') to extract structured data from HTML.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of JsonCssExtractionStrategy - its use of schemas with nested structures and CSS selectors to extract data from HTML, matching the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although the documentation snippet does not explicitly mention the ExtractionStrategy, the extraction configuration it describes is governed by the interface defined in ExtractionStrategy. JsonCssExtractionStrategy, a concrete implementation of this abstract base, relies on the contract set by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that ExtractionStrategy provides the foundational framework for different extraction patterns and implementations, with the predicted description focusing on JsonCssExtractionStrategy as an example while the ground truth elaborates more on the patterns. The core relationship understanding is aligned.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as a feature to extract structured data using CSS selectors. This directly maps to the 'JsonCssExtractionStrategy' class, which implements the extraction strategy interface using a CSS-based schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of JsonCssExtractionStrategy as a CSS-based data extractor, matching the ground truth's explanation of using CSS selectors to extract structured data via a schema. While it omits implementation details about BeautifulSoup, the high-level relationship is correctly described.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explains how to use the 'JsonCssExtractionStrategy' with the AsyncWebCrawler, indicating that the AsyncWebCrawler is the component responsible for crawling web pages and integrating extraction strategies. This usage is directly reflected in the 'AsyncWebCrawler' class.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler integrates with JsonCssExtractionStrategy, which aligns with the ground truth showing that the code checks for and executes JsonCssExtractionStrategy in the aprocess_html method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions Crawl4AI's AsyncWebCrawler class as supporting session\u2010based crawling through the session_id parameter. This class is introduced as the primary interface to maintain persistent browser sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the AsyncWebCrawler class implements session-based crawling functionality through the session_id parameter, allowing persistent browser sessions across multiple requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation refers to maintaining state across multiple arun calls by reusing the same session_id. The arun() method on AsyncWebCrawler implicitly handles this functionality as it passes the session_id parameter, ensuring continuity of the session.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that the arun() method handles session-based crawling through the session_id parameter to maintain state across requests. The predicted description accurately conveys this main relationship, even if it doesn't mention every implementation detail.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the documentation, AsyncPlaywrightCrawlerStrategy underpins many of the described features such as JavaScript execution (via the js_code parameter), CSS selector support, and session management. It is employed by AsyncWebCrawler to implement the behaviors outlined in the doc snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements core functionality described in the documentation including session management, JavaScript execution, and CSS selector support. While it doesn't elaborate on the session management details as much as the ground truth, it captures the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the interface for asynchronous crawling strategies. It is extended by AsyncPlaywrightCrawlerStrategy, thereby forming the foundational layer for the session\u2010based crawling functionality described in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify AsyncCrawlerStrategy as the abstract base class that defines the interface for session-based crawling functionality, with the predicted version capturing the essential relationship even if it doesn't detail all the specific methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using the async context manager (i.e., 'async with AsyncWebCrawler(...)') and passes browser configuration parameters (headless, verbose, sleep_on_close).",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how AsyncWebCrawler is instantiated with configuration parameters through the async context manager syntax, which aligns with the ground truth's explanation that these parameters are passed via kwargs to the AsyncPlaywrightCrawlerStrategy in __init__",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls 'crawler.arun(url=\"https://example.com\")', which demonstrates the direct usage of the arun() method from the AsyncWebCrawler class.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method usage but misses the crucial aspect of how the constructor parameters (headless, verbose, sleep_on_close) configure the crawler's behavior, which is a key part of the relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Internally, AsyncWebCrawler creates an instance of AsyncPlaywrightCrawlerStrategy when no crawler_strategy is provided. Configuration options like headless, verbose, and sleep_on_close are passed along to it.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented browser configuration options through its constructor parameters and start() method, specifically handling headless mode, verbose logging, and sleep_on_close functionality as shown in the example configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that AsyncPlaywrightCrawlerStrategy handles configuration options, but incorrectly states it's created by AsyncWebCrawler internally when no strategy is provided - this relationship is not shown in the ground truth.",
      "error_type": "scope_expansion"
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract AsyncCrawlerStrategy. As a result, using AsyncWebCrawler (which defaults to AsyncPlaywrightCrawlerStrategy) indirectly involves the AsyncCrawlerStrategy hierarchy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable the browser configuration settings shown in the documentation, including the crawl functionality used by AsyncWebCrawler's arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship involving AsyncCrawlerStrategy, but misses the key point about how it enables the browser configuration settings and crawl functionality shown in the documentation.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun(...)' with parameters like 'page_timeout', 'delay_before_return_html', and 'wait_for'. This explicitly demonstrates usage of the 'AsyncWebCrawler.arun()' method to control timeouts and waiting behavior during page loading.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method's role in handling timeouts and waiting, but fails to mention that these controls are implemented through kwargs being passed to crawler_strategy.crawl(), which is a crucial implementation detail mentioned in the ground truth.",
      "error_type": "omitted_implementation_detail"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, the 'AsyncWebCrawler.arun()' method internally delegates to the crawler strategy, which is an instance of 'AsyncPlaywrightCrawlerStrategy'. This class implements the 'crawl' method where the parameters such as 'page_timeout', 'delay_before_return_html', and 'wait_for' are processed to control page loading behavior.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation, noting how the AsyncPlaywrightCrawlerStrategy implements the timeout and waiting behavior described in the documentation through its crawl method. Though it's less specific about the smart_wait implementation, it accurately describes the overall relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class extends the abstract 'AsyncCrawlerStrategy'. This relationship signifies that the waiting and timeout mechanism is part of a defined contract in 'AsyncCrawlerStrategy' that is concretely implemented by 'AsyncPlaywrightCrawlerStrategy'.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that the AsyncCrawlerStrategy defines the contract/foundation for implementing crawling behavior with timeout and waiting capabilities through its abstract methods. While the predicted adds the specific example of AsyncPlaywrightCrawlerStrategy, this doesn't contradict the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'arun' method on the crawler object by showing the call: 'result = await crawler.arun(url=\"https://example.com\")'. This method initiates the crawl process and returns a CrawlResult containing the media information.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately describes arun() as the core crawling method but fails to mention its key media handling capabilities, caching functionality, and customizable parameters that are central to the ground truth relationship.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "Within the usage example, the code iterates over 'result.media[\"images\"]'. This directly leverages the 'media' attribute of the CrawlResult, which is designed to store rich media metadata including image source, alt text, description, context, and relevance score.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core concept that the media dictionary stores media metadata including images with properties like source, alt text, description, context, and relevance score. It aligns with the ground truth's explanation of the CrawlResult.media's purpose and structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The documentation mentions handling of lazy-loaded images using parameters such as 'wait_for' and 'delay_before_return_html'. Although not explicitly mentioned by name, these parameters are managed internally by the 'crawl' method of AsyncPlaywrightCrawlerStrategy, which is the default crawling strategy used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that lazy-loaded image handling is implemented via wait_for and delay parameters, but misses that this is specifically managed through the smart_wait() method which is a key implementation detail mentioned in the ground truth.",
      "error_type": "missing_key_implementation"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun()' with parameters simulate_user, override_navigator, and magic. This explicit usage demonstrates that the method AsyncWebCrawler.arun() is intended to handle anti-detection features.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as handling anti-detection features, but incorrectly suggests direct parameter usage rather than the actual **kwargs pass-through mechanism described in the ground truth",
      "error_type": "implementation_detail_mismatch"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within the crawl method of AsyncPlaywrightCrawlerStrategy, the code checks for the flags 'simulate_user', 'override_navigator', and 'magic'. When any of these flags are True, the strategy injects scripts to override navigator properties and simulates user behavior to avoid bot detection. This behavior, while not directly mentioned in the snippet, is implicitly enabled when the arun() method passes the anti-detection parameters.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the class implements anti-detection features through scripts that override navigator properties and simulates user behavior when certain flags are enabled. While it uses slightly different wording, it conveys the same fundamental functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is built upon the abstract base class AsyncCrawlerStrategy. This establishes a formal interface for crawling, which includes the anti-detection functionality handled by its subclass.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class but focuses on a generic crawling interface without acknowledging the specific anti-detection functionality that's central to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly recommends using a CSS Strategy for well-structured HTML and for faster performance with high structure accuracy. 'JsonCssExtractionStrategy' directly implements a CSS-based extraction mechanism using BeautifulSoup selectors, which aligns with the documented recommendation.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements CSS-based extraction using BeautifulSoup selectors and aligns with the documentation's recommendation for handling well-structured HTML with high performance and accuracy",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The text explicitly advises using an LLM Strategy for natural language text and for obtaining the best semantic understanding. 'LLMExtractionStrategy' implements this approach by leveraging language model based extraction methods, which fits the recommendation.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that LLMExtractionStrategy implements the LLM Strategy option for natural language processing and semantic understanding, which aligns with the ground truth's core explanation",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation clearly mentions using a Cosine Strategy for mixed or complex content as well as for achieving high content relevance. 'CosineStrategy' is designed to perform clustering based on cosine similarity, making it the appropriate extraction strategy for these scenarios.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that CosineStrategy is designed for mixed/complex content processing using cosine similarity, and both acknowledge its role in extracting relevant content. The predicted description mentions the key implementation aspects (clustering, cosine similarity) that align with the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', indicating that this class is the entry point for the screenshot functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies AsyncWebCrawler as the entry point for screenshot functionality, it omits the crucial detail about how the screenshot data is handled through the arun method and returned in base64 format. The focus on class instantiation only tells part of the story.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the snippet, the 'arun' method is explicitly called on the AsyncWebCrawler instance with parameters including 'screenshot=True'. This method initiates the crawling process that includes taking a screenshot.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles screenshot capture when screenshot=True, but misses key aspects about caching and the full data flow through screenshot_data to CrawlResult that are central to the functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks the 'success' attribute on the result object (i.e., 'if result.success and result.screenshot:'). This flag indicates whether the crawl operation succeeded.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the success boolean is used to verify if the screenshot operation completed successfully before proceeding with saving the screenshot data",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The code checks 'result.screenshot' to see if a screenshot was captured. This attribute, part of the CrawlResult object, holds the base64-encoded screenshot data which is then decoded and saved.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that result.screenshot contains base64-encoded screenshot data that gets decoded and saved to a file",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation\u2019s usage example explicitly calls 'crawler.arun(url=\"https://example.com\")', directly demonstrating the invocation of the 'arun' method on the crawler instance to obtain a crawl result containing the raw HTML.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the arun method is used to crawl a URL and retrieve raw HTML content, which matches the ground truth's explanation of the method's functionality. The minor implementation details omitted don't affect the fundamental understanding.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "In the provided usage example, the code prints 'result.html', showing that the 'html' attribute of the CrawlResult object is used to access the complete, unmodified HTML from the webpage.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that the html attribute provides access to complete, unmodified HTML from the webpage, which aligns with the ground truth's description of storing raw HTML content for access.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler as a context manager (using 'async with AsyncWebCrawler(verbose=True)') to perform the web crawling task.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage, but fails to capture the broader functionality of the class including its core web crawling, HTML processing, and data extraction capabilities mentioned in the ground truth.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet clearly calls the 'arun()' method on the AsyncWebCrawler instance to perform crawling with specific parameters, including the extraction strategy.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of using arun() for crawling, but misses key aspects about the extraction strategy specifically being used for OpenAI pricing data and omits the important functionality around caching, HTML processing, and error handling that are central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly creates an instance of LLMExtractionStrategy (imported from crawl4ai.extraction_strategy) to specify the schema-based extraction of structured data from the provided URL.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for schema-based extraction, but misses the crucial aspect that it uses provider-based LLM models to parse the content and omits the structured data parsing functionality according to Pydantic schema",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is derived from ExtractionStrategy, meaning that the abstract extraction interface (ExtractionStrategy) underpins the implementation used in the snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of the base class providing core parallel processing functionality and structured data extraction interface that concrete implementations utilize.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result of invoking AsyncWebCrawler.arun() is a CrawlResult object, whose 'extracted_content' property is used to retrieve the structured data extracted by the LLMExtractionStrategy.",
    "ground_truth_relationship": "The CrawlResult class defines the structure that holds the extraction results shown in the documentation example, particularly storing the extracted OpenAI model fees in the extracted_content field.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions agree on the core relationship - that CrawlResult class holds extraction results with extracted_content field storing the extracted data from LLMExtractionStrategy. The predicted description provides a more implementation-focused view while ground truth is more structural, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions 'AsyncWebCrawler' as the asynchronous crawler that enables the use of LLMs to extract structured data. This shows that the web crawler is central to the LLM extraction functionality demonstrated.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of AsyncWebCrawler as an asynchronous web crawler that supports LLM-based content extraction. The predicted description correctly identifies the main purpose and relationship between the crawler and LLM extraction capability.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation text explicitly references 'LLMExtractionStrategy', indicating that this strategy is used to extract structured data using LLMs. It is shown as a key example for applying language model based extraction.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core purpose of LLMExtractionStrategy as a means to extract structured data using LLMs, which aligns with the ground truth's explanation of implementing LLM-based web content extraction and structuring.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the abstract base class ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the documentation, it underpins the LLMExtractionStrategy by providing the required interface for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class that LLMExtractionStrategy extends, but misses the crucial aspect of parallel processing functionality and the core purpose of extracting structured data, which are fundamental aspects mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports CosineStrategy from its module and instantiates it with parameters such as semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k. This direct usage in the example code demonstrates that CosineStrategy is the extraction strategy being employed.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship by explaining that CosineStrategy is initialized with the same key parameters mentioned in the documentation, and both descriptions align on its purpose for similarity-based content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is defined as a subclass of ExtractionStrategy. Although the base class is not directly referenced in the snippet, the inheritance relationship implies that the extraction process leverages the interface and behavior defined in ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, but misses the crucial aspect of the base class providing parallel processing capabilities and the abstract extract() method interface that derived classes must implement",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example shows a crawl operation via the call 'crawler.arun(...)' with extraction_strategy set to an instance of CosineStrategy. This implies that the AsyncWebCrawler.arun() method is used to execute the crawl process that leverages the extraction strategy provided.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that arun() accepts and executes crawls using the extraction strategy parameter, which can be CosineStrategy. While it omits some implementation details about similarity-based clustering, it gets the main functional relationship correct.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet uses AsyncWebCrawler by instantiating it within an async context manager (async with AsyncWebCrawler() as crawler:), demonstrating its direct application for web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage but misses the crucial aspect of its core functionality supporting both structured and LLM-based content extraction methods",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on the AsyncWebCrawler instance twice to retrieve structured and semantic analysis content, making its invocation explicit.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called in the documentation example, but misses the core functionality of the method itself shown in the code (processing URLs, handling extraction strategies, returning structured results). It focuses only on how it's used rather than what it does.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The example creates an instance of JsonCssExtractionStrategy by passing an 'article_schema' dictionary. This indicates its direct use as the extraction strategy for structured content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for structured content extraction, but misses explaining its core functionality of using CSS selectors with baseSelector and fields properties to implement the extraction pattern.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "In the snippet, an instance of LLMExtractionStrategy is explicitly constructed with parameters such as provider, schema, and instruction, to perform semantic analysis on the article content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions describe the core functionality of LLMExtractionStrategy as handling semantic analysis with custom parameters like provider, schema, and instructions. The predicted description captures the essential relationship even though it's more concise.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both JsonCssExtractionStrategy and LLMExtractionStrategy extend the abstract base class ExtractionStrategy. Although not directly instantiated in the snippet, it is part of the underlying framework that supports the extraction functionalities used in the example.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that both JsonCssExtractionStrategy and LLMExtractionStrategy inherit from ExtractionStrategy as the base class, which aligns with the ground truth's explanation of the core interface and functionality relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with custom parameters (user_agent and headers) to control how the crawler identifies itself to websites. This shows explicit usage of the AsyncWebCrawler class for identity management.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the AsyncWebCrawler class handles identity management through customizable user agent and headers parameters, with the predicted description accurately reflecting how the class is used for this purpose, even if it doesn't mention the specific method implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the arun() method on the AsyncWebCrawler instance (e.g., 'result = await crawler.arun(url=\"https://example.com\")'), demonstrating the use of this method to perform crawling after configuring the crawler's identity.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for crawling but misses the core identity management functionality through user_agent and headers that is central to the ground truth relationship",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler defaults to instantiating AsyncPlaywrightCrawlerStrategy when no alternate strategy is provided. This internal dependency handles the actual crawling operations and applies custom identity parameters (like user_agent and headers), thus supporting the documented identity management functionality.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that the code handles identity management through user agents and headers, but incorrectly states this is done through AsyncWebCrawler defaulting to AsyncPlaywrightCrawlerStrategy rather than directly describing AsyncPlaywrightCrawlerStrategy's implementation of these features.",
      "error_type": "implementation_misattribution"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates a CosineStrategy with parameters (linkage_method, max_dist, model_name) to configure custom clustering functionality.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions custom clustering with CosineStrategy parameters, but misses the crucial content filtering pipeline aspect and semantic similarity features that are core to the class's functionality",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is implemented as a subclass of ExtractionStrategy. Although not directly mentioned in the snippet, this inheritance is inherent in its design and is essential for its extraction functionality.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between ExtractionStrategy and CosineStrategy, and the purpose of this relationship for enabling extraction functionality. While it's more concise than the ground truth, it captures the essential architectural relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet uses 'async with AsyncWebCrawler() as crawler:' which clearly demonstrates the instantiation and use of the AsyncWebCrawler class for initiating a crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler's usage with async context management, but misses crucial aspects about its role in content filtering, customizable extraction strategies, and clustering support that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the 'arun' method of AsyncWebCrawler (i.e., crawler.arun(...)) to perform crawling and extraction based on the provided strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling and extraction but misses crucial aspects about how it handles custom clustering configurations and content filtering parameters, as well as its role in managing the full extraction pipeline including caching and error handling",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the snippet accesses 'result.extracted_content' to retrieve the processed clustering output, demonstrating the use of this public attribute from the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately describes the core relationship that extracted_content is accessed from the CrawlResult object to get processed clustering output. While it doesn't mention all the specific details about pricing features and scores, it captures the essential relationship correctly.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides a complex schema with keys like 'baseSelector' and 'fields' that mirrors the input expected by JsonCssExtractionStrategy. Although the class name is not mentioned explicitly in the snippet, the schema structure demonstrated is designed to be used by this strategy to extract nested objects and lists.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy uses a schema with CSS selectors to extract data, and the schema structure matches what's expected by the class. While it doesn't explicitly mention BeautifulSoup's select() method, this is a minor implementation detail that doesn't affect the high-level understanding.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "ExtractionStrategy defines the abstract interface for all extraction methods. The provided schema example is intended for use with extraction strategies that implement this interface, thereby indirectly linking the schema design to ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an interface for extraction methods, but misses the crucial connection to schema-based extraction patterns and structured data processing that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'JsonCssExtractionStrategy' in its title and discussion. It highlights advanced usage scenarios for this strategy to handle complex nested HTML structures, directly linking the documentation to this artifact.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy and its connection to handling complex HTML structures, but misses the crucial implementation details about BeautifulSoup and schema-based selectors that are central to the ground truth",
      "error_type": "missing_core_mechanism"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although the base class is not mentioned explicitly in the documentation snippet, its role is inherent in the advanced functionality of JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is a subclass of ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy providing the foundation for JsonCssExtractionStrategy. While the predicted is less detailed, it captures the core inheritance relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates CosineStrategy with parameters (e.g., semantic_filter, word_count_threshold, top_k, sim_threshold, max_dist) to handle different extraction use cases. This directly demonstrates the creation and configuration of the CosineStrategy object.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CosineStrategy parameter configuration but misses the core functionality of cosine similarity matching for text extraction that the ground truth emphasizes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the example, the result is obtained by calling 'crawler.arun(...)' with the extraction_strategy parameter set to the instantiated CosineStrategy. This implicitly refers to AsyncWebCrawler.arun(), which handles the crawling and extraction process where the extraction strategy is applied.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() accepts extraction_strategy and performs crawling/extraction, but misses the key point about how it implements the flexible use cases with different configurations shown in the documentation examples",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning that the abstraction defined by ExtractionStrategy underpins the extraction functionality. Although ExtractionStrategy is not directly instantiated, its role as the base class is essential for understanding the design and use of CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class for CosineStrategy but focuses narrowly on the inheritance relationship while missing the key functionality aspects (extract method, parallel processing) and variety of use cases that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet clearly shows a call to 'crawler.arun(...)' with parameters such as 'word_count_threshold', 'exclude_external_links', 'exclude_external_images', and 'excluded_tags'. This directly demonstrates usage of the 'AsyncWebCrawler.arun()' method, making the trace explicit.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() and some parameters, but focuses on showing an example usage rather than explaining its core content filtering functionality as described in the ground truth.",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly instantiates and uses AsyncWebCrawler in the context manager (using 'async with AsyncWebCrawler(verbose=True) as crawler:'). This shows its direct usage to manage the crawling workflow.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage in a context manager, but misses the core functionality of integrated JavaScript execution and extraction strategy implementation described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the provided code example, the 'arun()' method is called on the AsyncWebCrawler instance to perform the crawl with integrated JavaScript execution and waiting logic.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling with the AsyncWebCrawler, but misses the crucial aspect that it implements JavaScript execution and waiting functionality through js_code parameter handling",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code example explicitly creates an instance of JsonCssExtractionStrategy with a defined schema to extract commit elements from the page.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses a schema for extraction, but misses the crucial aspect that it implements the structured data extraction logic and processes HTML content according to the schema specifications",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "After processing the pages, the example explicitly calls 'kill_session()' on the crawler's underlying strategy to clean up the session.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main purpose of kill_session() being called to clean up the session after crawling, which aligns with the ground truth's explanation of cleaning up browser resources by closing page and context objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the code snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling engine. Its methods (like kill_session) are implicitly used in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes AsyncPlaywrightCrawlerStrategy's role but mischaracterizes it as just being a default engine for AsyncWebCrawler. The ground truth correctly identifies its core functionality of implementing JavaScript execution and waiting through the csp_compliant_wait method.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, making AsyncCrawlerStrategy the abstract base class that defines the contract for asynchronous crawling. This relationship is implicit in the inheritance hierarchy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class, but focuses only on inheritance hierarchy while missing the key point about its role in providing foundational methods for JavaScript execution and waiting functionality.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a concrete implementation that extends the abstract base class ExtractionStrategy. This inheritance link defines the standard interface for extraction strategies used in the crawling process.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class, but focuses on JsonCssExtractionStrategy which isn't mentioned in the ground truth. It misses the core purpose of processing HTML into structured data through the extract method as described in the ground truth.",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly names 'JsonCssExtractionStrategy' and shows its instantiation with a provided schema that uses CSS selectors. This directly demonstrates its purpose to extract data using CSS selectors.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses CSS selectors for data extraction, matching the ground truth's explanation of using BeautifulSoup's select() method with CSS selectors from the schema.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "'JsonCssExtractionStrategy' extends the abstract base class 'ExtractionStrategy'. Although not mentioned explicitly in the snippet, this inheritance relationship ensures that the CSS-based extraction strategy adheres to the required extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of parallel processing capabilities that the base class provides, which is a key part of the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example in the snippet calls 'crawler.arun(...)' with 'extraction_strategy' as a parameter. This implies that the asynchronous web crawler\u2019s 'arun()' method is used to execute the extraction strategy, although it is not directly named in the documentation text.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() accepts and uses extraction_strategy as a parameter, but misses the crucial aspect that it actively implements the core execution logic to enable CSS selector-based extraction as part of the crawling workflow",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet shows a direct invocation of kill_session on the crawler\u2019s strategy within a try-finally block for resource cleanup. This explicitly demonstrates that sessions should be terminated using the kill_session() method.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions focus on the core purpose of kill_session() as a resource cleanup mechanism for managing sessions, with the predicted description emphasizing its usage in a try-finally block and the ground truth emphasizing its implementation details - but both convey the same fundamental relationship to resource management.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation includes examples of using the arun() method to perform crawling actions with specific parameters such as session_id, js_code, and wait_for. This clearly demonstrates how state management (such as login and dashboard verification) is implemented via the arun() method.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the arun() method handles session management for crawling actions, allowing stateful operations like login flows. While it focuses more on examples than implementation details, it conveys the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions and demonstrates the usage of 'NlpSentenceChunking' by importing it and invoking its 'chunk' method. The provided example code shows how to instantiate NlpSentenceChunking and use it to split text into sentences.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of NlpSentenceChunking as a sentence splitter, matching the ground truth's explanation of using NLP (specifically NLTK) for sentence boundary detection",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation, NlpSentenceChunking extends ChunkingStrategy. This inheritance establishes an implicit relationship that defines the contract for chunking text, ensuring that NlpSentenceChunking conforms to the expected public interface for text chunking.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between NlpSentenceChunking and ChunkingStrategy, and captures the core concept that this defines a contract for text chunking functionality. While using slightly different wording, it conveys the same essential relationship as the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly imports JsonCssExtractionStrategy and uses it for pattern\u2010based extraction by passing a schema that defines the selectors for dynamic items. This shows that the extraction strategy implementation is directly used to extract structured content from the page.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy implements pattern-based extraction using a schema with selectors, which aligns with the ground truth's explanation of how it parses HTML and applies CSS selectors to extract structured data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly imports LLMExtractionStrategy and demonstrates its usage by passing parameters such as provider, schema (derived from a Pydantic model), and an instruction to analyze the content. This indicates that the strategy is intended for LLM-based extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that LLMExtractionStrategy is used for LLM-based content extraction and accurately describes its core functionality of processing content using configurable providers, schemas, and instructions, which aligns with the ground truth's description.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Although not imported explicitly in the snippet, the code sample calls 'crawler.arun(...)' indicating that the crawling functionality provided by AsyncWebCrawler.arun() is used to combine page interaction (JavaScript execution, waiting for elements) with the extraction strategies. This method orchestrates the overall crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship of how AsyncWebCrawler.arun() combines page interaction with extraction strategies, which aligns with the ground truth's explanation of implementing functionality for structured data extraction with JavaScript execution and wait conditions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it within an async context (using 'async with AsyncWebCrawler(verbose=True)') to create an instance. This shows that AsyncWebCrawler is the main class users interact with for asynchronous crawling in Crawl4AI.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is the main class used for async crawling and specifically mentions its usage with the async context manager ('async with') syntax, which aligns with the ground truth's description of the class implementing __aenter__ and __aexit__ methods enabling this functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the async context provided by AsyncWebCrawler, the 'arun()' method is explicitly called to crawl a URL (using 'await crawler.arun(url=\"https://www.nbcnews.com/business\")'). This demonstrates that the method is the designated function for performing the crawl operation.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling URLs, but misses crucial aspects of its asynchronous implementation including content extraction, caching, and error handling functionality that are core to its purpose as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After invoking the arun() method, the example accesses the 'markdown' attribute of the resulting CrawlResult object (via 'print(result.markdown)'). This indicates that the extracted content is stored in the 'markdown' attribute.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute/property of the CrawlResult stores the extracted content that gets printed in the example",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation code snippet directly calls 'crawler.arun(url=\"https://example.com\")', which corresponds to the AsyncWebCrawler.arun() method. This method is responsible for crawling the given URL and returning a CrawlResult object.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method and CrawlResult but misses the key focus on markdown extraction and content processing that is central to the fit_markdown functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The code in the snippet prints 'result.fit_markdown', explicitly accessing the 'fit_markdown' attribute of the CrawlResult class. This attribute holds the main content in markdown format, which is the focus of the documentation regarding 'Fit Markdown'.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown is an attribute that contains the main content in markdown format, which aligns with the ground truth's explanation of it holding cleaned and extracted main content as markdown.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with a proxy parameter. The code example 'async with AsyncWebCrawler(proxy=...) as crawler:' directly references this class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that AsyncWebCrawler supports proxy functionality, but it incorrectly suggests the proxy parameter is passed directly to AsyncWebCrawler when it's actually handled through the crawler_strategy parameter",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the method 'arun' is explicitly called on the AsyncWebCrawler instance to perform the page crawl (e.g., 'result = await crawler.arun(url=\"https://example.com\")').",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling, but misses the key aspect about proxy configuration handling mentioned in the ground truth. The prediction focuses only on method invocation rather than its proxy-related functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when not provided with an alternative, defaults to instantiating AsyncPlaywrightCrawlerStrategy with the provided keyword arguments (including the proxy parameter). Although not directly mentioned in the snippet, it is crucial for handling the proxy configuration under the hood.",
    "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy to handle proxy configuration. While it doesn't detail the specific HTTP/SOCKS implementation, it accurately describes the high-level relationship and proxy handling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly imports JsonCssExtractionStrategy and instantiates it with a defined extraction schema to extract cryptocurrency prices from Coinbase.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used with a schema to extract cryptocurrency data from Coinbase, but misses the crucial aspect of how it processes the HTML using CSS selectors to extract specific fields (name, symbol, price) from table rows",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly imports AsyncWebCrawler and uses it as an asynchronous context manager to initiate the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage as an async context manager, but misses the core functionality of implementing cryptocurrency price extraction using arun() method and JsonCssExtractionStrategy that the ground truth emphasizes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls the arun() method on the AsyncWebCrawler instance to perform the crawl, which triggers the extraction process.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used to perform crawling and extraction, but omits the crucial details about handling cryptocurrency price data structuring and returning CrawlResult that are central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The extracted_content attribute is accessed from the CrawlResult object to retrieve the structured JSON output of the extraction.",
    "ground_truth_relationship": "The extracted_content property stores the scraped cryptocurrency data in string format that gets parsed into JSON containing name, symbol, and price fields from Coinbase's webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that extracted_content contains the extracted data that gets parsed as JSON. While it doesn't mention the specific fields being extracted (name, symbol, price), it correctly describes the fundamental functionality of accessing structured data from the CrawlResult.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code asserts the success attribute of the CrawlResult object to confirm that the crawling operation was successful.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the example code to verify that the cryptocurrency price crawl operation completed successfully before proceeding with data extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the success boolean property is used to verify/confirm successful completion of the crawling operation",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with proxy and headers parameters. This shows the intended use of AsyncWebCrawler to configure network settings and enable anti-detection features.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler can be instantiated with proxy and headers parameters to enable anti-detection features. While it doesn't mention the **kwargs implementation detail from the ground truth, this omission doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example calls the 'arun' method on the AsyncWebCrawler instance with 'magic=True'. This explicitly triggers the crawling process with anti-detection features enabled.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() supports magic mode functionality, but misses the key aspect of proxy integration described in the ground truth. The prediction focuses only on anti-detection features while omitting the proxy configuration capability.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy when no custom crawler strategy is provided. Inside its 'crawl' method, it checks for flags like 'magic' (or 'simulate_user') to enable anti-detection features, making it a hidden but critical component for Magic Mode.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description discusses AsyncWebCrawler's magic mode and anti-detection features, but misses the core focus of the ground truth which is about proxy configuration implementation in AsyncPlaywrightCrawlerStrategy.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy inherits from AsyncCrawlerStrategy. This base class establishes the interface for crawling strategies, of which the magic mode anti-detection feature is an implementation detail provided by its subclass.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncCrawlerStrategy is a base class defining the interface for crawling strategies, which enables the magic mode and other features mentioned in the ground truth through its abstract methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly refers to 'advanced features of JsonCssExtractionStrategy' as a means to demonstrate how a complex HTML structure (such as one for an e-commerce website with categories, products, reviews, and related items) can be processed. This indicates that the JsonCssExtractionStrategy is intended for parsing such structured data using CSS selectors and JSON extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy is designed to parse complex HTML structures using CSS selectors and JSON extraction, which aligns with the ground truth's explanation of using BeautifulSoup to extract structured data based on CSS selectors and schema mapping.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While ExtractionStrategy is not directly mentioned in the documentation snippet, it is the foundational abstract base class for extraction strategies. Since JsonCssExtractionStrategy extends ExtractionStrategy, it is implicitly part of the demonstrated extraction approach.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the abstract base class that underlies the extraction functionality, which aligns with the ground truth's description of it serving as the foundation for extracting structured data from HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using an 'async with' statement. This shows that the example relies on AsyncWebCrawler as the primary entry point for session-based crawling.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler and its use with async/with, but misses the crucial aspect of session handling and user interactions that are central to the ground truth's description of the relationship. The ground truth emphasizes specific methods and session management capabilities that are key to understanding the relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code explicitly calls the arun() method on the AsyncWebCrawler instance to perform the crawling task for each page, including dynamic content loading and content extraction.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() functionality for crawling but misses the key session maintenance aspect that's central to the ground truth relationship",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet calls kill_session() via the crawler_strategy attribute of AsyncWebCrawler to properly close the session. This direct call ensures that session management is properly handled after crawling.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While both describe session cleanup, the predicted description incorrectly states it's called via crawler_strategy attribute, when the code shows it's a direct method. However, it does capture the core purpose of closing the session.",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly uses AsyncWebCrawler within an async context (async with AsyncWebCrawler(verbose=True)) to initiate a crawl session. This shows that AsyncWebCrawler is the entry point for the crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the entry point for crawling operations, which is true, but misses the key relationship regarding the wait_for parameter functionality described in the ground truth. The wait_for parameter support through arun method is a crucial aspect that's completely omitted.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the arun() method (e.g., result = await crawler.arun(...)) to execute the crawling operation with parameters such as wait_for, js_code, and extraction_strategy.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called with various parameters, but misses the key point that arun() specifically implements the wait_for functionality by passing it through to crawler_strategy.crawl() to control page loading conditions.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the example, a new instance of JsonCssExtractionStrategy is explicitly created with a schema for commit extraction. This shows its role as the extraction strategy in the crawling process.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in the extraction process but omits the crucial implementation detail about using BeautifulSoup to parse HTML according to the schema's selectors",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet explicitly calls kill_session() (via crawler.crawler_strategy.kill_session(session_id)) to terminate the session after crawling. This confirms that session management is integrated into the process.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is called to terminate the session, but misses the key detail about cleaning up browser resources (closing page and context objects) which is a crucial part of the functionality described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although the snippet does not directly name AsyncPlaywrightCrawlerStrategy, AsyncWebCrawler defaults to this strategy for handling crawling operations. This indirectly provides the wait_for functionality and other dynamic content-handling capabilities.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the wait_for functionality but incorrectly suggests it's provided indirectly through AsyncWebCrawler rather than being directly implemented in AsyncPlaywrightCrawlerStrategy through the smart_wait method.",
      "error_type": "incorrect_implementation_source"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a call to 'crawler.arun(url=\"https://example.com\")'. This directly maps to the AsyncWebCrawler.arun() method, which is responsible for initiating the crawl process and returning a CrawlResult instance that holds metadata among other information.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that arun() method processes a URL and returns a CrawlResult containing metadata. While it doesn't explicitly list all the metadata fields, it accurately describes the main functionality and connection between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "After obtaining the result from the crawler.arun() call, the snippet accesses 'result.metadata' to print metadata such as title, description, keywords, and others. This attribute is implemented as the 'metadata' field in the CrawlResult class, making it the point of extraction for page metadata.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that result.metadata is used to access page metadata fields from the crawler result, which aligns with the ground truth's explanation of metadata being stored as an optional dictionary in CrawlResult",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation explicitly names 'FixedLengthWordChunking' and shows its usage in the example code, indicating that this class splits text into fixed-length word chunks. The usage example demonstrates creating an instance with a 'chunk_size' parameter and calling the 'chunk' method.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main functionality of splitting text into fixed-length word chunks using the chunk_size parameter, which aligns with the ground truth's description of the implementation",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "FixedLengthWordChunking is implemented as a subclass of ChunkingStrategy. Although the documentation does not mention ChunkingStrategy explicitly, its inclusion is implicit because FixedLengthWordChunking inherits the defined interface and behavior from ChunkingStrategy.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core relationship - that FixedLengthWordChunking implements/inherits from the abstract ChunkingStrategy class. The predicted description accurately captures this inheritance relationship, even if phrased slightly differently.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation text explicitly recommends a 'CSS Strategy' for well-structured HTML and highest structure accuracy. JsonCssExtractionStrategy implements a CSS\u2010based extraction approach and is thus aligned with this recommendation.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that JsonCssExtractionStrategy implements the CSS Strategy and aligns with the documentation's recommendation for handling well-structured HTML content using CSS selectors.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly advises using an 'LLM Strategy' for processing natural language text and for best semantic understanding. LLMExtractionStrategy is designed to use language models (LLM) for extraction, matching this guidance.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship between LLMExtractionStrategy and the documentation's guidance, identifying its purpose for natural language processing and semantic understanding.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation clearly indicates that for mixed or complex content and for achieving best content relevance, a 'Cosine Strategy' should be used. CosineStrategy computes similarity based on cosine measures, fitting this requirement.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is designed for mixed/complex content and uses cosine similarity as its core mechanism, which aligns with the ground truth's description of how the class implements the document's recommendation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates how to instantiate CosineStrategy with specific parameters (sim_threshold, word_count_threshold, top_k). The provided code examples (e.g., 'strategy = CosineStrategy(sim_threshold=0.8)') directly reference the CosineStrategy class and illustrate configuring its constructor parameters.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture how the documentation's parameters map to CosineStrategy's implementation, showing the relationship between the documented parameters and their usage in the class. While the predicted description focuses more on instantiation examples and the ground truth provides more implementation details, they both correctly convey the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning that while ExtractionStrategy is not directly mentioned in the snippet, it is implicitly part of the trace. This base class defines the extraction interface that CosineStrategy implements.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but focuses too narrowly on CosineStrategy's inheritance relationship while missing the core purpose of being a template for various extraction configurations",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet shows a direct invocation of CosineStrategy with parameters such as semantic_filter, word_count_threshold, sim_threshold, max_dist, linkage_method, top_k, model_name, and verbose. These match the __init__ signature of the CosineStrategy class, providing explicit evidence of its usage in configuration.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that the documentation parameters match the CosineStrategy class initialization parameters, with the same configuration options and default values. While it's slightly less detailed than the ground truth, it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy. Although the base class is not directly mentioned in the snippet, the configuration options imply it is part of the extraction framework provided by the ExtractionStrategy. This inheritance relationship is fundamental to the overall design of the extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy extends ExtractionStrategy and is part of the extraction framework. While it's more tentative in stating the relationship, it captures the same core inheritance relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")', demonstrating the use of the 'AsyncWebCrawler.arun()' method as the entry point for link analysis.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the entry point for link analysis, but misses the crucial aspects of how it implements the core crawling functionality, processes pages asynchronously, and returns a CrawlResult with categorized links",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The variable 'result' assigned from the call to arun() is an instance of CrawlResult. Although not named explicitly in the documentation snippet, it is implied by the usage as it holds the output data including links.",
    "ground_truth_relationship": "The CrawlResult class implements the documented link analysis functionality by storing categorized links in its 'links' dictionary field, where each link category (internal, external, social, etc.) contains a list of dictionaries with link metadata like href, text, context, and type.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship that CrawlResult holds output data, but misses the crucial aspect of how links are categorized and stored in the links dictionary with specific metadata structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet directly references 'result.links' to iterate over categorized link data (internal and external). This confirms that the CrawlResult.links attribute is used to store the link analysis details.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that CrawlResult.links stores categorized link data, even though it doesn't list all categories and link detail fields. The core relationship and usage pattern is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler using an async context manager (evidenced by 'async with AsyncWebCrawler(verbose=True) as crawler:'), indicating its crucial role as the starting point for the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage with async context manager, but misses the crucial relationship with JavaScript execution and extraction strategy coordination mentioned in the ground truth. It focuses only on instantiation rather than the full dynamic content extraction functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The 'arun()' method is explicitly called on the AsyncWebCrawler instance to perform the crawling operation with parameters such as js_code, wait_for conditions, css_selector and an extraction strategy.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core functionality of the arun() method being used with AsyncWebCrawler and mentions the key parameters (js_code, wait_for, css_selector, extraction strategy) that align with the ground truth's explanation of handling dynamic web content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is explicitly instantiated as the value for the 'extraction_strategy' parameter in the 'arun()' method call, providing LLM-based extraction capabilities by specifying provider, api_token, and instruction.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's use in the documentation example, but presents it only as a parameter value rather than acknowledging it's the main class being implemented with extraction functionality",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the abstract base class ExtractionStrategy. Although not directly mentioned in the snippet, this inheritance relationship is fundamental to ensuring that LLMExtractionStrategy adheres to the common extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is a base class that LLMExtractionStrategy inherits from/extends. While they emphasize different aspects (predicted focuses on inheritance, ground truth emphasizes practical usage), they convey the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code snippet accesses the 'extracted_content' attribute of the crawl result (i.e., result.extracted_content) to retrieve the summarized articles, demonstrating its role in conveying the extraction output.",
    "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that extracted_content contains LLM-processed data that is handled as JSON output. The predicted description correctly identifies its role in conveying extraction output, which aligns with the ground truth's explanation of it serving as a bridge between crawled data and structured output.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation code example directly calls the 'arun' method on a crawler instance (e.g., await crawler.arun(...)), demonstrating how to initiate a crawl with a specific extraction strategy.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly indicates the arun() method usage, but fails to capture the key error handling and caching patterns mentioned in the ground truth that are central to the code-documentation relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "Within the example, the error_message attribute is accessed (print(f\"Extraction failed: {result.error_message}\")) to display error details when extraction does not succeed.",
    "ground_truth_relationship": "The error_message field in CrawlResult directly enables the error handling best practice shown in the documentation by providing a string description of what went wrong when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that error_message is used to display error details when extraction fails, which aligns with the ground truth's explanation of enabling error handling when success is False",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The example explicitly checks the 'success' property (if not result.success) to determine if the crawl and data extraction were successful.",
    "ground_truth_relationship": "The CrawlResult.success boolean property directly supports the error handling best practices shown in the documentation by enabling conditional verification of successful extraction operations.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of the success property for error checking, but misses the broader context that this property specifically supports the documented error handling best practices, which is a key part of the ground truth relationship.",
      "error_type": "incomplete_context"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After confirming success, the code snippet proceeds to parse the extracted data by accessing the 'extracted_content' attribute (data = json.loads(result.extracted_content)).",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the successfully crawled data as demonstrated in the documentation's error handling code sample where it's parsed as JSON after a successful extraction.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that extracted_content stores the crawled data which is then parsed as JSON after successful extraction, matching the ground truth's explanation",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation recommends starting with CSS for structured data extraction. Although not directly mentioned by name, this guidance implicitly suggests using the JsonCssExtractionStrategy, which leverages CSS selectors to extract data.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that JsonCssExtractionStrategy implements the CSS-based extraction approach recommended in the documentation for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet advises using LLM for complex interpretation. This recommendation implicitly refers to the LLMExtractionStrategy, which is designed to extract content using LLM-based methods.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM usage for complex interpretation, while the ground truth describes error handling and retry logic implementation. These are completely different aspects of the code.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "By suggesting the use of Cosine for content relevance, the documentation implicitly points to the CosineStrategy, which utilizes cosine similarity to evaluate and filter content relevance.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy implements content relevance evaluation using cosine similarity, which aligns with the ground truth's explanation of using cosine similarity metrics for semantic filtering.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'JsonCssExtractionStrategy' as a feature to extract structured data using CSS selectors with a defined schema. This directly maps to the JsonCssExtractionStrategy class found in the code, which implements extraction using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that JsonCssExtractionStrategy uses CSS selectors with a schema to extract structured data. While it's less detailed than the ground truth, it doesn't contradict or misunderstand the fundamental functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet indicates how to use the JSON CSS Extraction Strategy in conjunction with the AsyncWebCrawler. This implies that AsyncWebCrawler is designed to integrate with various extraction strategies including JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that AsyncWebCrawler works with JsonCssExtractionStrategy as one of its extraction strategies. While it doesn't detail the implementation specifics mentioned in the ground truth, it correctly identifies the integration capability.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although ExtractionStrategy is not explicitly mentioned in the text, JsonCssExtractionStrategy extends it. This makes ExtractionStrategy an implicit part of the extraction functionality described in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and JsonCssExtractionStrategy, but misses the key role of ExtractionStrategy in providing the core extraction and parallel processing functionality mentioned in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates a call to the 'arun()' method by using 'crawler.arun(..., js_code=...)'. This clearly shows that users can pass custom JavaScript commands via the 'js_code' parameter, making this method an explicit interface for JS execution.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles JavaScript execution, but incorrectly states that js_code is passed directly to arun() when it's actually handled through **kwargs and crawler_strategy.crawl()",
      "error_type": "implementation_detail_mismatch"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly referenced in the snippet, AsyncPlaywrightCrawlerStrategy is the default crawler strategy used by AsyncWebCrawler. Its 'crawl' method retrieves the 'js_code' parameter and executes it via page.evaluate(), thereby implementing the custom JavaScript execution functionality.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JavaScript execution is handled through page.evaluate(), but incorrectly states AsyncPlaywrightCrawlerStrategy is the default strategy for AsyncWebCrawler (which isn't mentioned in the ground truth and adds an unsupported claim).",
      "error_type": "unsupported_claim"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates JsonCssExtractionStrategy with an advanced schema for data extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that JsonCssExtractionStrategy uses a schema for data extraction, but misses the crucial aspect of how it processes HTML using BeautifulSoup and extracts structured data according to field definitions",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends the abstract base class ExtractionStrategy, linking the advanced extraction functionality to the overall extraction framework.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy, but misses the crucial aspect that ExtractionStrategy is an abstract base class providing foundational extraction methods that enable structured data extraction through extract() and run() methods.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly imports AsyncWebCrawler and uses it as an asynchronous context manager to perform crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's use as an asynchronous context manager, but misses crucial aspects like its caching functionality and support for JsonCssExtractionStrategy mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code directly calls the arun() method of an AsyncWebCrawler instance to execute the crawl and retrieve results.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function call correctly but misses crucial aspects of the method's implementation like handling extraction strategies, caching, and returning structured results that enable JSON data extraction as described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet asserts on the 'success' attribute of the crawl result to ensure the crawl was successful.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the success boolean is checked via assertion to verify the crawl completed successfully, which aligns with the ground truth's description of validating the crawl operation.",
      "error_type": null
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code accesses the 'extracted_content' attribute of the crawl result to obtain the structured JSON output of the product data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that extracted_content contains the product data, but misses the crucial point that it needs to be JSON-parsed first since it's stored as a raw string",
      "error_type": "omitted_crucial_step"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with headless=True in an async context manager, indicating it is the primary class handling the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is the main class handling crawling functionality and is used with an async context manager, which aligns with the ground truth describing its role in handling protected site crawling via the arun method. While the predicted description omits some implementation details about specific parameters, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance with parameters such as magic=True, remove_overlay_elements=True, and page_timeout=60000, which are essential for handling protected sites.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on parameter usage shown in the example documentation, but misses key aspects of the actual implementation like error handling, caching, and customizable extraction/chunking strategies that are core to enabling protected site crawling.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet returns 'result.markdown' if the crawl is successful, explicitly accessing the 'markdown' attribute from the CrawlResult object returned by arun().",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that result.markdown contains the crawl output text and is returned when the crawl succeeds",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation snippet uses 'result.success' in a conditional expression to determine whether the crawl was successful.",
    "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that result.success is used in a conditional but misses the key relationship between success and the return of markdown vs None",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler internally defaults to using AsyncPlaywrightCrawlerStrategy for its crawling operations. Parameters like magic and remove_overlay_elements influence its behavior.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the protected site crawling functionality by using headless browser automation with features like popup removal and extended timeouts as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes AsyncPlaywrightCrawlerStrategy's role in crawling but inaccurately states it's an internal default of AsyncWebCrawler rather than being the actual implementation class shown in the code/documentation.",
      "error_type": "incorrect_relationship_assumption"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the 'RegexChunking' class and shows its instantiation and usage in the provided example code. The example demonstrates initializing the class with a 'patterns' parameter and calling its 'chunk' method to split text. This direct reference establishes an explicit trace to the RegexChunking artifact.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on API usage and class instantiation rather than explaining the core functionality of using re.split() to iteratively split text with patterns as described in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the documentation snippet, RegexChunking extends ChunkingStrategy. This implicit relationship is critical because RegexChunking implements the abstract 'chunk' method defined in ChunkingStrategy, linking it to the overall chunking framework.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that RegexChunking implements ChunkingStrategy's abstract chunk() method for text splitting functionality. The predicted description accurately conveys this inheritance and implementation relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an invocation 'crawler.arun(...)', which implies that 'crawler' is an instance of the AsyncWebCrawler class. This class is responsible for managing the overall crawling process, including handling content customization options such as HTML to text conversion via passed keyword arguments.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class handles crawling and content customization through its arun() method, including HTML-to-text conversion options via parameters. While it doesn't explicitly mention kwargs, it captures the core relationship between the class and its HTML conversion functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example explicitly calls the 'arun' method on a crawler instance with a configuration dictionary provided to the 'html2text' parameter. This indicates that AsyncWebCrawler.arun() is the method responsible for processing these conversion options and returning a CrawlResult containing the markdown conversion of HTML.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles HTML to text conversion options passed as parameters, which aligns with the ground truth explanation of how html2text options are passed as kwargs to customize webpage content conversion.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions the use of `LLMExtractionStrategy` for LLM extraction. This artifact provides the logic for extracting structured data using language models and directly implements the extraction functionality as described in the snippet.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy as an implementation for extracting structured data using language models, which aligns with the ground truth's description of using LLMs for web content extraction and structuring.",
      "error_type": null
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation states that AsyncWebCrawler is used to perform asynchronous web crawling with integrated LLM extraction. This makes AsyncWebCrawler directly relevant as the main component coordinating the extraction tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler's main purpose is asynchronous web crawling with LLM extraction capabilities, which aligns with the ground truth's explanation of the class implementing async crawling with LLM extraction support through its arun method.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While not mentioned explicitly in the text snippet, ExtractionStrategy is the base class for LLMExtractionStrategy. Its inclusion in the chain is necessary to understand the inheritance and design of the extraction functionality that enables LLM-based extraction.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class, but focuses mainly on inheritance structure while missing the key functionality of parallel processing and structured data extraction described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit call to 'crawler.arun()' with parameters such as simulate_user, override_navigator, and magic set to True. This directly maps to the 'AsyncWebCrawler.arun()' method, which is responsible for initiating the crawl and passing along these anti-detection flags.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that arun() accepts anti-detection parameters which are used for crawling. While it describes it more directly through the method call example, it conveys the same fundamental relationship as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Inside the crawl method of 'AsyncPlaywrightCrawlerStrategy', the code checks for the flags simulate_user, override_navigator, and magic. When any of these flags are enabled, it injects stealth scripts to override navigator properties. This is how the strategy implements the anti-detection features described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the class implements anti-detection features through script injection when certain flags are enabled. Both descriptions reference the same key flags (simulate_user, override_navigator, magic) and the main mechanism (injecting scripts to override navigator properties).",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class extends the abstract base class 'AsyncCrawlerStrategy'. This inheritance forms the backbone of the crawler's design, ensuring a consistent interface and supporting the implementation of anti-detection features.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class but focuses on inheritance structure rather than its core purpose of defining anti-detection features interface",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', demonstrating its use for taking screenshots.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's screenshot capability and async usage, but focuses only on instantiation rather than explaining the core screenshot functionality through the arun method and base64 handling described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example explicitly calls the arun() method on the AsyncWebCrawler instance to perform the crawl, with parameters including 'screenshot=True' and 'bypass_cache=True'.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is called with screenshot functionality, but misses the key aspects of how screenshots are handled internally (caching, screenshot_data population) and the end-to-end flow described in the ground truth",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "In the snippet, after receiving the result from arun(), the code checks 'result.screenshot' to verify if a screenshot was captured. This implies the use of the CrawlResult.screenshot attribute in the public interface.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the result.screenshot attribute is checked and used in the workflow, which aligns with the ground truth's explanation of how the screenshot field stores base64 data that gets decoded and saved",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet also checks 'result.success' to ascertain if the crawl succeeded, thus implicitly utilizing the CrawlResult.success attribute from the result object.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the success boolean property is used to verify/check if the operation succeeded before proceeding with saving the screenshot",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of the 'arun()' method by invoking 'crawler.arun(url=\"https://example.com\")'. This clearly shows that this method is responsible for initiating the crawl and returning output formats.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is responsible for crawling and returning output formats, which aligns with the ground truth's explanation that it implements core crawling functionality and populates the different output formats.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "While the documentation snippet does not mention 'CrawlResult' by name, the returned object from 'arun()' is an instance of CrawlResult. This object encapsulates all the output formats (raw HTML, cleaned HTML, markdown, and fit markdown) that are accessed later in the snippet.",
    "ground_truth_relationship": "The CrawlResult class directly implements the documented output formats through its properties html, cleaned_html, markdown, and fit_markdown, which store the different content representations described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the CrawlResult class handles the output formats described in the documentation, with properties that match the documented access patterns. While it doesn't explicitly state the direct property implementation, it correctly captures the core relationship between the class and the documented functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The code snippet directly accesses 'result.html' to retrieve the original (raw) HTML. This clearly maps to the 'html' attribute defined in the CrawlResult model.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that html is a property containing raw HTML content, which aligns with the ground truth's description of it being a string type for storing raw HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The example shows the use of 'result.cleaned_html' to obtain sanitized HTML, directly linking the documentation to the public attribute 'cleaned_html' in the CrawlResult class.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that cleaned_html provides access to sanitized HTML content through the result object, which aligns with the ground truth's explanation of it being a property storing sanitized HTML as an optional string.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet explicitly retrieves 'result.markdown' for the standard markdown output, directly mapping to the 'markdown' attribute in CrawlResult.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that 'markdown' is an attribute accessed via result.markdown that provides standard markdown output, which aligns with the ground truth's explanation of the markdown field storing HTML converted to standard markdown format.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet shows accessing 'result.fit_markdown' to obtain the most relevant or concise markdown output. This directly corresponds to the 'fit_markdown' attribute defined in the CrawlResult model.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown contains the most relevant content in markdown format, matching the ground truth's core meaning.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (using 'async with AsyncWebCrawler(verbose=True) as crawler'), indicating that this class provides the main crawling interface and session management functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class and mentions context manager usage, but misses key functionality like dynamic content crawling, JavaScript execution, and custom extraction strategies that are central to the class's purpose according to the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the 'arun' method is explicitly called (e.g. 'result = await crawler.arun(...)') to perform the actual crawling and extraction of dynamic content from multiple pages.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted describes arun's usage for crawling but misses key aspects about its role in session management and bypass_cache functionality mentioned in ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly creates an instance of JsonCssExtractionStrategy by passing a schema to extract commit titles from the crawled page. This extraction strategy processes HTML content using defined CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main functionality of JsonCssExtractionStrategy - using a schema with CSS selectors to extract content from HTML, which aligns with the ground truth's explanation of how it processes GitHub commit data using BeautifulSoup and defined selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "At the end of the example, the 'kill_session' method is explicitly called (via 'await crawler.crawler_strategy.kill_session(session_id)') to clean up the active session after crawling dynamic content.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship - that kill_session is called at the end to clean up the session resources. While it doesn't detail the specific cleanup actions (closing page/context), it gets the core functionality and usage correct.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although the documentation does not directly mention AsyncPlaywrightCrawlerStrategy by name, AsyncWebCrawler internally defaults to instantiating AsyncPlaywrightCrawlerStrategy when no custom strategy is provided. This strategy underlies the session management and dynamic content handling, including the kill_session functionality.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the session management and page navigation logic required to support the dynamic content crawling example, including handling session_id persistence and executing JavaScript for next page navigation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncPlaywrightCrawlerStrategy implements the core session management and dynamic content handling functionality needed for the documented example. The predicted description correctly identifies the strategy's role in session management and dynamic content handling, matching the ground truth's key points.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun()' with parameters such as 'js_code' and 'wait_for'. This direct invocation maps to the AsyncWebCrawler.arun() method, which is responsible for handling dynamic content such as infinite scroll and form interactions.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method handles dynamic content processing through js_code and wait_for parameters, matching the core functionality described in the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not directly mentioned in the snippet, the 'crawler' variable is understood to be an instance of AsyncWebCrawler. This class encapsulates the 'arun()' method, thereby providing the broader crawling framework used in the examples.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its arun method connection, but misses the crucial dynamic content handling functionality through js_code and wait_for parameters that is central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "By default, AsyncWebCrawler instantiates AsyncPlaywrightCrawlerStrategy as its crawler strategy. This strategy implements the detailed crawling behavior\u2014including handling JavaScript actions\u2014needed for features like scrolling and form submission as illustrated in the snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncPlaywrightCrawlerStrategy as implementing crawling behavior but oversimplifies by focusing on instantiation rather than its core dynamic content handling functionality described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. This relationship ensures that the strategy conforms to a defined interface for crawling operations, including those needed for processing dynamic content.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that AsyncCrawlerStrategy is an abstract base class providing essential interface methods for handling dynamic content and crawling operations. The predicted description captures the core relationship even though it mentions a specific implementation (AsyncPlaywrightCrawlerStrategy) not in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an AsyncWebCrawler instance using the async context manager (async with AsyncWebCrawler() as crawler:).",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only mentions the async context manager functionality of AsyncWebCrawler, while missing the core purpose described in the ground truth - its ability to process HTML content with different extraction strategies and return structured data, markdown, and media elements.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the AsyncWebCrawler instance calls the 'arun' method to perform the crawling tasks. This method is explicitly invoked to retrieve the crawl results.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling, but misses the key relationship shown in the ground truth about how arun() supports different extraction strategies as demonstrated in the documentation examples",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates LLMExtractionStrategy with parameters (provider, schema, instruction) and passes it as the extraction_strategy to the 'arun' method for structured data extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship shown in the documentation - that LLMExtractionStrategy is instantiated with provider, schema, and instruction parameters and used with crawler.arun for structured data extraction. While it doesn't mention all implementation details, it captures the main functionality relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet creates an instance of JsonCssExtractionStrategy (with a provided schema) and passes it to the 'arun' method to extract repeated patterns.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of JsonCssExtractionStrategy for pattern extraction, but misses the crucial aspect of how it systematically uses CSS selectors to extract structured data from repeated HTML elements",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet accesses the 'fit_markdown' attribute (result.fit_markdown) from the CrawlResult returned by the arun() method, using it to extract the main content in a formatted markdown output.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is used to access the main extracted content from the crawl result and is stored in the returned dictionary. The predicted description focuses on the same core functionality without any significant contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet retrieves structured and pattern data by accessing 'extracted_content' on the results (llm_result.extracted_content and pattern_result.extracted_content) obtained from the arun() method.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship that extracted_content contains crawled data that is parsed as JSON, with the predicted description correctly noting how it's accessed from the results of arun()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet explicitly uses the 'media' attribute (result.media) of the CrawlResult to retrieve media information that was extracted during the crawl.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that media content is extracted during crawling and returned as part of the function's output via result.media",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy from crawl4ai.extraction_strategy and instantiates it with a schema. This directly demonstrates its usage for pattern\u2010based extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for pattern-based extraction with a schema, but misses crucial aspects about how it recursively extracts nested data from repeating HTML elements using CSS selectors",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is defined as a subclass of ExtractionStrategy. Although not directly mentioned in the snippet, this base class is part of the underlying design, providing the common interface for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of parallel processing and the core purpose of pattern-based content extraction that the base class enables.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun' to perform the crawl operation with the given extraction strategy. This call maps to the AsyncWebCrawler.arun() method, which processes the URL and extraction strategy.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles crawling with an extraction strategy, but misses the key aspect that it specifically processes the JSON schema to extract structured data according to defined selectors and patterns",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler instance with parameters (e.g., verbose=True, headless=True). This direct usage in the example confirms its explicit traceability.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler instantiation but misses the core relationship about how user simulation is implemented through simulate_user and override_navigator parameters. However, it correctly recognizes AsyncWebCrawler's role in the example.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code calls the arun() method on the AsyncWebCrawler instance. This method initiates the crawling process and is directly invoked in the provided documentation example.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method being called but misses the core focus on user simulation features that is central to the documented relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy by default. This strategy\u2019s crawl() implementation checks for flags like 'simulate_user' and 'override_navigator' to simulate real user behavior, fulfilling the documented functionality.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements user simulation functionality through simulate_user and override_navigator flags, matching the ground truth's core explanation of how user behavior is simulated via mouse movements, clicks, and navigator property overrides.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. Although the abstract base class is not directly used in the snippet, it underpins the crawling interface used in the simulation of user behavior.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class for crawling functionality, but misses the crucial aspect that its abstract methods specifically enable user simulation features mentioned in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit usage example by calling 'crawler.arun(...)'. This directly maps to the AsyncWebCrawler.arun() method (artifact id 5), which accepts parameters such as url, word_count_threshold, remove_overlay_elements, and process_iframes, thereby enabling basic crawl options.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation and code - that the arun() method accepts various crawling options and parameters as shown in the documentation example. Both descriptions emphasize the configurable nature of the crawler through parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within the implementation of AsyncWebCrawler.arun(), the crawler internally uses a crawler strategy. In this case, the AsyncPlaywrightCrawlerStrategy (artifact id 1) is used to execute the actual crawling. This strategy processes additional options like remove_overlay_elements (to remove popups/modals) and process_iframes (to handle iframe content) as passed in the documentation snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core relationship - that AsyncPlaywrightCrawlerStrategy implements the documented crawling options (like remove_overlay_elements and process_iframes) through its crawl method. The predicted description correctly identifies how the strategy processes these options, even if it incorrectly assumes it's used by AsyncWebCrawler.arun().",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun()' with parameters (screenshot=True and screenshot_wait_for=2.0). This explicitly demonstrates the usage of the arun() method to perform a crawl that includes screenshot capture.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately conveys that the arun() method handles screenshot functionality through its parameters, which aligns with the ground truth's explanation of how the method implements screenshot capture functionality and includes it in the results.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncPlaywrightCrawlerStrategy is the underlying strategy used by AsyncWebCrawler. Its implementation of the crawl() method contains the logic for handling the 'screenshot' and 'screenshot_wait_for' parameters, including invoking its 'take_screenshot' method for capturing page screenshots with enhanced error handling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of screenshot capabilities in AsyncPlaywrightCrawlerStrategy, including the take_screenshot method and error handling. While it mentions some additional implementation context about crawl(), this doesn't contradict the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The snippet explicitly checks and uses the 'screenshot' attribute of the result object (i.e., 'result.screenshot'). This attribute is part of the public interface of CrawlResult and holds the Base64 encoded image obtained from the screenshot capture.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the screenshot attribute stores Base64 encoded image data from captured screenshots, matching the core functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and instantiates it with a schema object. This directly demonstrates its role for pattern-based extraction (e.g., extracting repeated product listings).",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in pattern-based extraction but omits the crucial implementation detail of how it iterates through base elements and extracts fields according to the schema structure",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although not mentioned by name in the snippet, this inheritance relationship is essential, as ExtractionStrategy defines the contract for extraction methods used in pattern-based extraction.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the core inheritance relationship between ExtractionStrategy and JsonCssExtractionStrategy, and explain how the base class provides the framework for pattern-based extraction through derived classes.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the example code uses 'result.extracted_content' to retrieve the extracted data. This attribute is a public interface of CrawlResult that holds the output of the extraction process.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content holds extraction output, but misses the crucial pattern-matching and JSON array structure explained in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'crawler.arun(...)' to execute the crawling process with the specified extraction strategy. This method (AsyncWebCrawler.arun()) orchestrates the crawl and extraction, even though its explicit class name is not mentioned in the snippet.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() executes crawling with extraction strategy, but misses the key aspect of JsonCssExtractionStrategy being used specifically for pattern-based extraction using CSS selectors to extract structured data",
      "error_type": "key_functionality_omission"
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates instantiating an AsyncWebCrawler object with 'async with AsyncWebCrawler() as crawler:'. This clearly maps to the AsyncWebCrawler class in the codebase.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the async context manager pattern usage of AsyncWebCrawler with 'async with' syntax, which aligns with the ground truth's explanation of the class supporting this pattern through __aenter__ and related methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the provided example, after updating the proxy, the crawl is executed using 'result = await crawler.arun(url=url)'. This call directly maps to the AsyncWebCrawler.arun() method implementation.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method implementation corresponds to the crawling execution shown in the documentation example, and both descriptions acknowledge the URL parameter and crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows the invocation of 'crawler.arun(url=\"https://example.com\")'. This directly maps to the 'AsyncWebCrawler.arun()' method which is responsible for crawling the provided URL and returning a result.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is called to crawl URLs, but misses the key media selection functionality described in the ground truth - specifically the ability to access different media types (images, videos, audios) through the result's media dictionary.",
      "error_type": "omitted_core_functionality"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Even though 'CrawlResult' is not explicitly named in the snippet, the variable 'result' is the output of 'AsyncWebCrawler.arun()' and is subsequently used to access its 'media' attribute. The 'CrawlResult' class defines the 'media' attribute where different media types (images, videos, audios) are stored.",
    "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship that CrawlResult's media dictionary field stores different media types (images, videos, audios) and enables media selection. The predicted description adds context about result variable usage but maintains the same fundamental understanding.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler with browser_type arguments (e.g., 'firefox', 'webkit', and default Chromium). This demonstrates how the user can select different browser engines.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports different browser engines and shows valid examples, but misses that this is implemented through the crawler_strategy parameter using PlaywrightCrawlerStrategy rather than direct browser_type configuration.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage examples, the arun() method of the AsyncWebCrawler instance is explicitly called to perform the crawl. This demonstrates how crawling is executed after setting up the crawler instance.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the method for executing crawls, but misses the key relationship that arun() enables browser selection through crawler_strategy based on the browser_type parameter set during initialization.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler internally uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy when no custom strategy is provided. This strategy supports browser selection based on the 'browser_type' parameter.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that AsyncPlaywrightCrawlerStrategy handles browser selection based on the browser_type parameter, supporting Firefox, WebKit, and Chromium as options. The main functionality alignment is accurate.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. This inheritance ensures that the crawling strategies adhere to a common interface, which is fundamental to the design of the crawler framework.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that AsyncCrawlerStrategy is an abstract base class that defines the interface methods that browser implementations must follow. While the predicted description is more general and doesn't mention specific browser types, it correctly identifies the fundamental relationship and purpose of the abstract class.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates CosineStrategy, demonstrating its role in similarity\u2010based clustering for content extraction.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic purpose of CosineStrategy for similarity-based clustering, but misses the crucial aspect that the code implements configurable parameters that control the clustering behavior, which is a key part of the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, which defines the abstract interface for all extraction strategies. This relationship is implicit in the inheritance hierarchy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the inheritance relationship but misses the crucial aspect of parallel processing capabilities and the purpose of the abstract extract() method that the base class provides to derived strategies.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows the extraction strategy being passed to the crawler's arun() method. Although not explicitly named as AsyncWebCrawler, the usage implies that the arun() method (from AsyncWebCrawler.arun()) integrates the extraction functionality into the crawling process.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() accepts and integrates extraction strategies into the crawling process, which aligns with the ground truth's explanation of arun() implementing crawling logic that can apply extraction strategies like CosineStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet directly demonstrates accessing the 'fit_markdown' attribute (content = result.fit_markdown) which is meant for representing formatted article content such as blog posts and news articles.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that fit_markdown is used for formatted article content like blog posts, with the predicted description accurately reflecting the documented best practice.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet shows a usage example filtering images from 'result.media[\"images\"]' by relevance score. This clearly indicates that the 'media' attribute of the CrawlResult is used to store and process media assets.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship that the media dictionary property allows filtering of media elements (particularly images) based on relevance scores, as demonstrated in the Best Practices section.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation includes a code example that filters internal links via 'result.links[\"internal\"]'. This explicitly shows the usage of the 'links' attribute from the CrawlResult model.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the same core relationship between CrawlResult.links and its use for filtering internal content links as shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The provided example calls 'crawler.arun(...)' with several parameters such as 'word_count_threshold', 'keep_data_attributes', and 'process_iframes'. This usage clearly corresponds to the 'AsyncWebCrawler.arun()' method responsible for cleaning and processing content.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method and some parameters correctly, but misses crucial aspects of the ground truth like media processing and link extraction functionality mentioned in the best practices.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and instantiates it within an async context manager. This demonstrates its role as the main crawler object that initiates the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main crawler object and mentions its usage with an async context manager, but it misses the crucial aspects of its functionality around HTML processing and structured data extraction that are core to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the provided example, the 'arun()' method is explicitly called on an AsyncWebCrawler instance. This method is responsible for performing the crawling and subsequently processing the extracted content.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of arun() for crawling and processing, but misses crucial aspects like the specific extraction strategy (LLMExtractionStrategy), caching functionality, and the structured data extraction purpose shown in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly instantiates LLMExtractionStrategy with detailed parameters (provider, api_token, schema, extraction_type, and instruction) to extract structured data from the pricing page. This demonstrates its role as the extraction strategy for processing the crawled content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for structured data extraction with provider-based LLM models and specified parameters, which aligns with the ground truth's explanation of how it implements extraction functionality using LLM models to parse HTML according to schemas.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows the usage of the 'arun' method on a crawler object (i.e. 'result = await crawler.arun(url=\"https://example.com\")'). This demonstrates that the method is responsible for initiating the crawl and returning a CrawlResult containing the raw HTML.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the arun() method is responsible for crawling a webpage and returning raw HTML content via CrawlResult. The predicted description correctly identifies the core functionality without any significant misunderstandings.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The snippet prints 'result.html', explicitly accessing the 'html' attribute of the CrawlResult. This attribute holds the complete raw HTML output (including headers and scripts), which is the documented functionality.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality - that html is an attribute containing the complete raw HTML content from the crawled page. It mentions the same key aspects as the ground truth (complete HTML, headers/scripts included).",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates the use of the 'arun' method by calling 'crawler.arun' with the 'js_code' parameter. This explicitly maps to the 'AsyncWebCrawler.arun()' method which is part of the public interface for initiating crawling operations with JavaScript execution.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() method provides JavaScript execution functionality via crawler.arun and js_code parameter, which aligns with the ground truth's description of how the method handles JavaScript execution by accepting js_code through kwargs and passing it to crawler_strategy.crawl()",
      "error_type": "none"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While the snippet does not mention this class by name, the 'js_code' parameter passed to 'AsyncWebCrawler.arun()' is handled internally by the crawler strategy. The 'AsyncPlaywrightCrawlerStrategy' class processes the 'js_code' parameter within its 'crawl' method to execute JavaScript commands on the page, making it an essential but implicit part of the execution chain.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that js_code execution is handled through page.evaluate() in the crawl method, even though it frames it in terms of the parameter handling flow rather than the implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit example of calling the 'arun' method on a crawler instance with parameters like 'word_count_threshold' and various exclusion flags. This directly correlates to the 'AsyncWebCrawler.arun()' method as implemented in the code artifact.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its parameter handling, but misses the key point about content filtering being processed during HTML extraction stages, instead focusing only on the method signature.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet does not explicitly mention the class name, the 'crawler' object used in the example is implicitly an instance of the 'AsyncWebCrawler' class. This class provides the framework within which the arun() method operates.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class but focuses only on its framework nature and arun() method existence, missing the core functionality of content filtering which is the main focus of the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a call 'crawler.arun(...)' with domain filtering parameters. This directly maps to the 'AsyncWebCrawler.arun()' method which is responsible for handling the crawl process and accepts additional keyword arguments (such as exclude_domains) that control content filtration.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the arun() method accepts domain filtering parameters through **kwargs which are used for content control. While the ground truth provides more implementation detail about how these parameters flow to crawler_strategy.crawl(), the predicted description captures the essential relationship correctly.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet does not mention the class name explicitly, the 'crawler' variable used in the example is an instance of the 'AsyncWebCrawler' class. Its inclusion is inferred based on the method call and overall crawler instantiation pattern.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only identifies that the 'crawler' variable is an instance of AsyncWebCrawler, but fails to explain the core functionality of domain-based filtering that the class implements. The ground truth correctly explains how the class handles domain filtering through its arun method parameters.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet for E-commerce Scraping shows a CSS strategy by defining a 'schema' with a 'baseSelector' and 'fields'. This configuration matches the design of JsonCssExtractionStrategy, which uses CSS selectors to extract information. Although the class name is not explicitly mentioned in the snippet, the schema structure implies its use.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements CSS-based scraping using a schema with baseSelector and fields, matching the e-commerce example in the documentation. While it's more tentative in tone, it captures the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "In the News Article Extraction use case, the snippet directly instantiates LLMExtractionStrategy with parameters like provider and schema (using Article.schema()). This is a clear and explicit reference to the implementation that uses LLM-based extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain how LLMExtractionStrategy implements schema-based extraction by accepting provider and schema parameters, with specific reference to the News Article Extraction use case. The predicted description captures the essential relationship between the implementation and usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "For Content Analysis, the snippet explicitly calls CosineStrategy with parameters such as 'semantic_filter' and 'top_k'. This clear instantiation directly corresponds to the available artifact that implements topic analysis using cosine similarity.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that CosineStrategy implements content analysis using cosine similarity and reflects the documented use case. Both descriptions align on the key functionality of semantic filtering and topic analysis.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the import and usage of AsyncWebCrawler via 'async with AsyncWebCrawler(verbose=True) as crawler:', demonstrating direct instantiation and context management.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core async context manager functionality shown in the code through __aenter__ and __aexit__ methods, and correctly describes the documented usage pattern with 'async with'",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls the 'arun' method on the crawler object (result = await crawler.arun(url=...)), which initiates the crawl process.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted describes the basic crawl invocation but misses the key error handling relationship shown in the documentation and implemented in the code via CrawlResult status fields",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "After invoking the crawl method, the snippet checks 'result.success' to determine if the crawl was successful. This attribute is part of the CrawlResult object.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that success is a boolean attribute used to check if a crawl operation was successful, which aligns with the ground truth's description of error handling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet prints 'result.error_message' when the crawl fails, indicating that the error details are provided by this attribute of the CrawlResult object.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that error_message stores and displays the failure reason when a crawl is unsuccessful",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The snippet prints out 'result.status_code' to display the HTTP status code returned by the crawl operation, thereby using this attribute from the CrawlResult.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that status_code represents the HTTP status code from the crawl operation, but misses the key purpose of error handling and diagnostics emphasized in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")'. This directly maps to the AsyncWebCrawler.arun() method which initiates the crawling process and returns a result.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is the core crawling method, but fails to acknowledge its significant media handling capabilities (lazy-loading, screenshots) and customization options that are central to the documented functionality",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result from 'crawler.arun()' is a CrawlResult object which encapsulates various crawl outputs including media. The documentation's use of 'result.media' implies that the returned object conforms to the CrawlResult model.",
    "ground_truth_relationship": "The CrawlResult class implements the media processing functionality described in the documentation through its 'media' dictionary field, which stores lists of processed media elements including images with their metadata like source, alt text, description, context, and relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies CrawlResult as containing media outputs, but misses the crucial detail about the media field's role in storing processed media elements with their detailed metadata (source, alt text, context, etc.)",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The sample code iterates over 'result.media[\"images\"]', explicitly accessing the media attribute on the CrawlResult object to retrieve image metadata.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the code accesses image metadata through result.media['images'], but it focuses only on the iteration aspect while missing the core purpose of the media dictionary as a storage mechanism for all media-related data and metadata",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The AsyncWebCrawler class uses a crawler strategy which, by default, is an instance of AsyncPlaywrightCrawlerStrategy. This strategy implements the crawl() method handling parameters such as 'wait_for' and 'delay_before_return_html' to manage lazy-loaded content, as described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements lazy-loaded image handling through its smart_wait() method, which detects and waits for lazy-loaded elements using CSS selectors or JavaScript functions as specified in the documentation's wait_for parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the AsyncPlaywrightCrawlerStrategy handles lazy-loaded content through wait mechanisms specified by the wait_for parameter. While the predicted description mentions this at a higher level and the ground truth provides more implementation detail about smart_wait(), they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract interface for crawler strategies. This hierarchical relationship underpins the implementation of crawl functionalities, including media extraction and lazy loading.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the core interface for implementing media crawling capabilities described in the documentation, with the crawl() method being responsible for extracting images and handling lazy-loaded content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy is an abstract interface defining crawling functionality including media handling. The predicted description adds detail about AsyncPlaywrightCrawlerStrategy but doesn't contradict the main relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly creates an AsyncWebCrawler instance with advanced configuration parameters (e.g., browser_type, headless, user_agent, headers, and proxy) to set up the crawling environment.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles configuration parameters, but it only mentions a subset of the configuration options (browser setup, identity, proxy) while missing crucial aspects like content handling, timing controls, and anti-detection features that are part of the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the asynchronous context, the snippet calls the arun() method on the AsyncWebCrawler instance to perform the crawl. This call passes parameters for content handling, timing, anti-detection, and dynamic JavaScript actions.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for web crawling with various parameters, but misses the crucial aspects of error handling, caching, and content extraction that are central to the implementation as noted in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy when no custom crawler_strategy is provided. Although not mentioned explicitly in the snippet, it is implicitly engaged to perform the underlying crawl operation.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that AsyncPlaywrightCrawlerStrategy is used by AsyncWebCrawler, but misses explaining that this class implements core crawling functionality shown in the example. The prediction focuses only on the default usage rather than the class's actual features.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The snippet accesses result.screenshot to obtain a base64 encoded screenshot of the crawled page. This attribute is defined in the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that result.screenshot contains a base64 encoded screenshot from the crawled page, which aligns with the ground truth's explanation that the screenshot data is stored in CrawlResult.screenshot when screenshot=True is set.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The example returns result.success to indicate if the crawling process completed successfully. This boolean attribute is part of the CrawlResult object interface.",
    "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the success boolean field indicates whether the crawling operation completed successfully, and both note it's part of the return/result structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls the 'arun' method on the crawler (as seen in 'result = await crawler.arun(...)'). This shows that AsyncWebCrawler.arun() is explicitly used to initiate crawling of dynamic content with parameters such as 'wait_for', 'js_code', 'process_iframes', and 'delay_before_return_html'.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that arun() method is used for handling dynamic content with various parameters. While it focuses more on listing the parameters rather than explaining the implementation details, it captures the essential functionality described in the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, the dynamic content handling parameters ('wait_for', 'process_iframes', 'js_code', 'delay_before_return_html') are processed inside the crawl method of AsyncPlaywrightCrawlerStrategy. This strategy is used by AsyncWebCrawler and therefore implicitly supports the features demonstrated.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncPlaywrightCrawlerStrategy implements the dynamic content handling features shown in the documentation through its crawl and related methods. Both descriptions emphasize the support for JavaScript execution, iframe processing, and configurable delays for content loading.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows the use of 'async with AsyncWebCrawler() as crawler:' which explicitly instantiates the AsyncWebCrawler class to maintain state between requests using a session_id.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core concept of using AsyncWebCrawler with session management through session_id, matching the ground truth's explanation of how sessions are maintained between requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls the arun() method on the AsyncWebCrawler instance with a session_id parameter. This shows that the method is used to perform requests while maintaining session state.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that the arun() method handles session management via session_id parameter. While it provides less detail than the ground truth about implementation specifics, it correctly identifies the main functionality of maintaining session state.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation demonstrates cleaning up the session by calling kill_session() on the crawler's strategy. This call directly maps to the kill_session() method in the AsyncPlaywrightCrawlerStrategy class.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is used for cleanup and maps to the documentation example, but misses the key implementation details about closing page/context objects and removing from sessions dictionary that are central to the ground truth's description.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows creating an AsyncWebCrawler instance via 'async with AsyncWebCrawler(headless=False) as crawler:'. This indicates direct use of the AsyncWebCrawler class as the primary interface for crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the main interface for crawling which is true, but misses the core relationship described in the ground truth about how it specifically implements overlay removal functionality through the arun() method parameter.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet directly calls the 'arun' method on the AsyncWebCrawler instance (i.e., 'result = await crawler.arun(...)'). This demonstrates the use of the crawling method for retrieving and processing content.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method usage but misses the key functionality around overlay removal which is the main focus of the ground truth relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler by default instantiates AsyncPlaywrightCrawlerStrategy. This class implements key functionalities such as removing overlay elements (via its 'remove_overlay_elements' method) and taking screenshots (via 'take_screenshot'), which are triggered by parameters like 'remove_overlay_elements=True' and 'screenshot=True'.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements overlay removal and screenshot functionality, which aligns with the ground truth's explanation of how the code removes overlays using JavaScript to handle elements with high z-index, fixed positions, and common overlay selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates a JsonCssExtractionStrategy with a provided schema and verbose=True flag. This clearly shows its intended use for extracting structured data from dynamic pages using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of JsonCssExtractionStrategy for structured data extraction using CSS selectors, which aligns with the ground truth's explanation of its role in processing schema-defined selectors for dynamic content scraping",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet uses an async context manager to create an instance of AsyncWebCrawler (via 'async with AsyncWebCrawler(verbose=True) as crawler:'), which sets up the crawling environment for subsequent operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as using an async context manager pattern, but misses the core functionality described in the ground truth about JavaScript execution, waiting conditions, and extraction strategies support.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code snippet, the arun() method of AsyncWebCrawler is invoked with parameters including URL, extraction_strategy, js_code, wait_for, and bypass_cache. This call initiates the actual crawl and extraction process.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of the arun() method being used to initiate crawling and extraction processes with various parameters. While it doesn't explicitly mention js_code execution and screenshot capabilities as in the ground truth, it correctly identifies the main purpose and parameter handling of the method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the code accesses result.extracted_content to retrieve the extracted data (which is then parsed using json.loads). This attribute is a public part of the CrawlResult interface.",
    "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately states that extracted_content is accessed from the result, but misses the crucial context that it specifically stores dynamic crypto pricing data after JavaScript-based scraping",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy inherits from ExtractionStrategy. Although ExtractionStrategy is not mentioned directly in the snippet, its role is implicit since JsonCssExtractionStrategy implements its extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the core relationship between ExtractionStrategy as a base class and JsonCssExtractionStrategy as an implementation, with the predicted description accurately capturing the inheritance relationship and the ground truth elaborating on its practical application.",
      "error_type": ""
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions and demonstrates the usage of the TopicSegmentationChunking class. The example shows the instantiation of TopicSegmentationChunking with a parameter (num_keywords=3) and its subsequent use to chunk text.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the main class and its basic purpose, but misses the crucial keyword extraction functionality and the underlying TextTiling algorithm implementation details that are central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "TopicSegmentationChunking is implemented as a subclass of ChunkingStrategy. Although the base class is not explicitly mentioned in the documentation snippet, its relationship is implicit due to inheritance, defining the interface for chunking strategies.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between TopicSegmentationChunking and ChunkingStrategy, and acknowledges the interface implementation requirement. While it's less detailed than the ground truth, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet is titled 'Cosine Strategy' and explicitly describes a method that uses cosine similarity for clustering and content extraction. This directly corresponds to the CosineStrategy class (Artifact 7), which implements similarity-based clustering by converting text into vector representations and grouping similar text chunks.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the core purpose of CosineStrategy and its use of similarity-based clustering, but misses describing the specific 5-step workflow that is central to the ground truth's explanation. While there's no contradiction, omitting this key architectural aspect results in an incomplete understanding.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning that all extraction functionalities and interfaces defined in ExtractionStrategy form the foundation for CosineStrategy. Although the documentation does not explicitly mention ExtractionStrategy, it is implicitly involved as the parent class that provides the framework for extraction strategies like CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that ExtractionStrategy is the parent class providing the foundational framework and interface for implementing extraction strategies like CosineStrategy. The predicted description correctly identifies the inheritance relationship and framework provision.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet directly invokes the 'arun' method (as seen in 'crawler.arun(...)') and passes timing parameters such as 'page_timeout' and 'delay_before_return_html'. This clearly demonstrates its use for controlling delays and timeouts.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun method's role in timing control, but incorrectly suggests direct parameter usage when the code actually implements these controls through **kwargs parameter passing",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within AsyncWebCrawler.arun(), the crawler delegates the actual processing to its internal crawler strategy. By default, this is an instance of AsyncPlaywrightCrawlerStrategy whose 'crawl' method (implemented in this class) uses 'page_timeout' and 'delay_before_return_html' to control page load timing and delay before returning the HTML. This implementation detail supports the documented timing control behavior.",
    "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core timing control functionality through page_timeout and delay_before_return_html parameters, matching the ground truth's explanation of how these parameters control page load timing and delay before HTML capture.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a usage example that explicitly calls 'crawler.arun(...)'. This directly corresponds to the method 'AsyncWebCrawler.arun()' from the available artifacts.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct arun() method but fails to mention its key functionality of HTML-to-markdown conversion and processing, which is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the usage example, after the call to 'crawler.arun()', the snippet prints 'result.markdown'. This directly accesses the 'markdown' attribute of the CrawlResult object returned by the method.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that markdown is an accessible attribute of the result object, but misses the key point that it stores HTML-to-Markdown converted text and is an optional string type",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it as an async context manager to initiate a crawl, which is central to the example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used as an async context manager for crawling, but misses the crucial aspect of customizable extraction strategies and the specific functionality for extracting targeted content that is central to the ground truth relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code example, the 'arun()' method is called on an AsyncWebCrawler instance to perform the crawling and extraction operation.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling, but misses the key aspect that it also handles content extraction and filtering using different strategies as shown in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The example explicitly creates an instance of LLMExtractionStrategy, passing parameters such as provider, api_token, and an instruction to extract technology-related content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that LLMExtractionStrategy is used for extracting content with specific parameters and instructions, which aligns with the ground truth's explanation of how it processes HTML content through LLM models with instructions for targeted extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the abstract base class ExtractionStrategy, thereby inheriting its extraction interface which underpins the extraction functionality demonstrated in the snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy serves as the base class framework that enables custom extraction functionality through its subclasses (like LLMExtractionStrategy). The predicted description correctly identifies the inheritance relationship and extraction interface, which aligns with the ground truth's explanation of how this enables the documented example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses the 'extracted_content' attribute from the result of the crawl (an instance of CrawlResult) to retrieve the JSON data extracted by the strategy.",
    "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that extracted_content stores content filtered by the extraction strategy and is accessed as JSON data. The predicted description accurately captures the core relationship even if it's less detailed.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates the invocation of the 'arun' method on a crawler instance by passing the 'css_selector' parameter. This explicitly shows that users can pass CSS selectors (e.g. '.main-article' or 'article h1, article .content') to extract specific content.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately explains that the arun method accepts CSS selectors as a parameter to extract specific content from webpages, which aligns with the ground truth's explanation of CSS selector-based content extraction functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet does not explicitly name the class, the variable 'crawler' used in the example is an instance of AsyncWebCrawler, which provides the 'arun' method. This implicitly connects the documentation to the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements CSS selector functionality in its arun() method through the css_selector parameter, which is passed to the WebScrappingStrategy's scrap() function to target specific HTML elements as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the connection between the documentation and the AsyncWebCrawler class, but fails to explain the key CSS selector functionality relationship that allows targeting specific HTML elements. It focuses only on the class connection rather than the functional relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the use of 'AsyncWebCrawler' in a context manager ('async with AsyncWebCrawler() as crawler:'), making it the entry point for enabling Magic Mode.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class enabling Magic Mode functionality, just through a simpler example. While it omits implementation details about crawler_strategy and kwargs handling, it captures the core relationship that AsyncWebCrawler is the entry point for Magic Mode capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example calls the 'arun' method on an AsyncWebCrawler instance with the parameter 'magic=True', directly demonstrating how Magic Mode is activated.",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that Magic Mode is enabled through the arun() method using a parameter, which aligns with the ground truth's explanation that the method accepts a magic parameter via kwargs to enable anti-bot protection features.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy as its crawler strategy. Its 'crawl' method checks for the 'magic' parameter (alongside 'override_navigator' and 'simulate_user') and injects anti-detection scripts, thus implementing the Magic Mode behavior.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the code implements Magic Mode through the magic parameter which enables anti-detection features and browser automation masking. While it's less detailed than the ground truth, it captures the essential functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class for AsyncPlaywrightCrawlerStrategy. It defines the interface for asynchronous crawling, upon which the Magic Mode enhancements are built.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class, but oversimplifies its purpose by only mentioning AsyncPlaywrightCrawlerStrategy rather than its broader role in implementing Magic Mode's anti-bot features.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy from crawl4ai.extraction_strategy and shows its use in supporting multiple LLM providers for data extraction. This direct reference makes it an explicit trace.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy supports multiple LLM providers, but misses the key implementation detail about how this is achieved through the provider parameter and api_token handling in the constructor. It focuses only on the import and existence of the feature rather than the mechanism.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, it is implicitly part of the extraction framework as the abstract base class for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is a subclass of ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy serving as the base class for different LLM provider implementations.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly names the JsonCssExtractionStrategy and explains its advantages (speed, precision, structured JSON output, and no external dependencies), which directly corresponds to the implementation provided by this class.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on advantages of the strategy but misses the core relationship between code and documentation - how the schema structure guides the data extraction process using BeautifulSoup selectors",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is built as a concrete implementation of the ExtractionStrategy abstract base class. Although not directly mentioned in the documentation, the structured schema detailed in the text implies the use of an extraction interface, which is provided by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and JsonCssExtractionStrategy, and recognizes that ExtractionStrategy serves as an abstract base class that defines the interface. While it doesn't explicitly mention the shared parallel processing capabilities, this omission of detail doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly instructs adding a chunking strategy using RegexChunking. It is directly imported and instantiated with a regex pattern, demonstrating its use as the chunking strategy.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of RegexChunking but omits the crucial functionality of how it actually splits text into chunks using the patterns and its default behavior",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "RegexChunking extends ChunkingStrategy. Although ChunkingStrategy is not mentioned directly in the snippet, it underpins RegexChunking and provides the abstract interface that RegexChunking implements.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that RegexChunking extends ChunkingStrategy and implements its interface, which aligns with the ground truth's description of the inheritance relationship and implementation requirement.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code example creates an instance of AsyncWebCrawler with verbose=True, demonstrating its role as the main crawling component.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as the main crawling component but misses the crucial relationship with RegexChunking strategy and its role in text splitting functionality that's central to the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls the arun() method on the AsyncWebCrawler instance to perform the crawl and pass the RegexChunking strategy. This method processes the crawl and applies the provided chunking strategy.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship of using arun() with a RegexChunking strategy, which aligns with the ground truth's implementation of accepting and validating a chunking_strategy parameter. Minor details about default values and validation are omitted but don't change the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the result returned is accessed for its 'extracted_content' attribute, which holds the processed content. This attribute is explicitly printed in the snippet.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that extracted_content holds processed content, but misses the crucial detail about RegexChunking's role in splitting the content using regex patterns",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls the arun() method on the crawler object (e.g., await crawler.arun(...)), which directly corresponds to the AsyncWebCrawler.arun() method in the code.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method but focuses only on the method name/call syntax rather than its key functionality of supporting different extraction strategies as described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the snippet a variable 'css_strategy' is provided to the arun() method. Although not named explicitly, this implies a CSS-based extraction strategy. JsonCssExtractionStrategy is an implementation that uses CSS selectors to extract initial HTML structure.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that JsonCssExtractionStrategy implements CSS-selector based extraction as part of a combinable strategy. While the predicted version is less detailed about the combination aspect, it correctly identifies the main functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet shows a separate call to crawler.arun() with an 'llm_strategy' variable for semantic analysis. This implies the use of an LLM-based extraction strategy. LLMExtractionStrategy is the corresponding artifact that handles extraction using LLMs.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy enables LLM-based extraction and maps to the documented ability to use LLM strategy for semantic analysis. While it doesn't mention all implementation details, it captures the core relationship between the class and its documented usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both JsonCssExtractionStrategy and LLMExtractionStrategy extend the abstract base class ExtractionStrategy. This base class defines the common interface for extraction strategies used in the crawler.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different extraction strategies that can be combined sequentially as shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class for different extraction strategies, but misses the key aspect about combining strategies sequentially and the parallel processing functionality mentioned in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler by passing custom headers, as shown in 'async with AsyncWebCrawler(headers=headers) as crawler:'. This demonstrates direct usage of the class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that headers can be passed to AsyncWebCrawler, but misses the key implementation detail that this happens through kwargs being passed to AsyncPlaywrightCrawlerStrategy. The predicted focuses only on usage while ground truth explains the mechanism.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage example, the 'arun()' method is explicitly invoked on the AsyncWebCrawler instance to perform the crawling action on the given URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling URLs, but misses the key relationship about custom headers being passed through the crawler_strategy, which is the main focus of the documentation example.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy as its crawling strategy. The custom headers provided in the snippet are passed through to this strategy via its 'headers' attribute.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes that custom headers are supported but incorrectly states this happens through AsyncWebCrawler defaulting to AsyncPlaywrightCrawlerStrategy, rather than explaining the direct header-setting functionality implemented in the strategy class itself.",
      "error_type": "mechanism_misunderstanding"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract class AsyncCrawlerStrategy. This base class defines the contract for crawler strategies, linking the header functionality indirectly to the overall crawling interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy is an abstract base class defining crawler functionality including header-related operations. The predicted description correctly identifies it as a base class defining contracts for crawler strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the use of 'crawler.arun()' with parameters such as 'page_timeout', 'delay_before_return_html', and 'wait_for'. This call directly corresponds to the AsyncWebCrawler.arun() method, which serves as the entry point for configuring timeouts and waiting behavior.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the entry point for timeout/waiting configuration, but misses that these parameters are passed through **kwargs to crawler_strategy.crawl() rather than being directly handled by arun()",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler.arun() internally calls the crawl method of its crawler strategy. By default, AsyncWebCrawler instantiates an AsyncPlaywrightCrawlerStrategy, which processes parameters like 'page_timeout', 'delay_before_return_html', and 'wait_for'. This indicates that the waiting and timeout behavior documented is implemented here.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the documented timeout/waiting behavior and its implementation in the AsyncPlaywrightCrawlerStrategy class, noting how parameters like page_timeout, delay_before_return_html, and wait_for are handled. While it's less detailed than the ground truth, it captures the essential relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract interface for crawling methods including 'crawl'. This inheritance ensures that the contract for handling timeouts and waiting behavior is maintained across implementations.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies AsyncCrawlerStrategy as defining an interface for crawling, but incorrectly specifies 'AsyncPlaywrightCrawlerStrategy' which isn't shown in the code and assumes specific implementation details not present in the ground truth",
      "error_type": "over_specification_and_assumption"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows code examples which instantiate and configure CosineStrategy with parameters like word_count_threshold, top_k, verbose, semantic_filter, sim_threshold, and max_dist. This demonstrates direct usage of the CosineStrategy class in the context of adjusting clustering thresholds and performance for content extraction.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture how the CosineStrategy class implements configurable parameters for content-based clustering and filtering, with the predicted description correctly identifying the key parameters that align with the documentation's recommended usage patterns.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, which serves as the foundational base class for extraction strategies. Although ExtractionStrategy isn\u2019t directly mentioned in the snippet, its inclusion is implicit through the inheritance hierarchy that CosineStrategy relies on.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text focuses on inheritance hierarchy with CosineStrategy, while the ground truth describes ExtractionStrategy's configurable framework supporting best practices. While both discuss ExtractionStrategy's role, they emphasize different aspects of the relationship.",
      "error_type": "incorrect_focus"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with a 'proxy_config' parameter for authenticated proxy usage.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts a proxy_config parameter for authenticated proxy usage, which aligns with the ground truth's explanation of how the proxy configuration is handled for enabling authenticated proxy connections.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform a crawl operation.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method but misses the key relationship about proxy support and configuration that is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned, AsyncWebCrawler, by default, instantiates AsyncPlaywrightCrawlerStrategy with the kwargs (including 'proxy_config'). This strategy handles proxy authentication internally.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that proxy authentication is handled internally by the AsyncPlaywrightCrawlerStrategy, even though it doesn't detail the specific implementation using ProxySettings. The core relationship and functionality is accurately conveyed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, thus inheriting the standard crawl interface that underpins proxy-based crawling operations.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining a crawling interface, but incorrectly specifies 'AsyncPlaywrightCrawlerStrategy' which isn't mentioned in the code or documentation, and misses the key aspect of proxy authentication shown in the ground truth.",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions the SlidingWindowChunking class by name. It describes that the class uses a sliding window approach to create overlapping chunks, details its parameters (window_size and step), and shows a usage example that imports and instantiates SlidingWindowChunking.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between code and documentation - both describe a sliding window chunking implementation that creates overlapping chunks using window_size and step parameters to preserve context.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "SlidingWindowChunking is implemented as a subclass of ChunkingStrategy. Although the documentation does not explicitly mention ChunkingStrategy, its presence is inferred since SlidingWindowChunking extends it, indicating that the base functionality for chunking is defined in ChunkingStrategy.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that SlidingWindowChunking is a subclass of ChunkingStrategy and captures the inheritance relationship. While it's more tentative in its language, it conveys the same core relationship as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy to perform LLM-based extraction of structured data.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's basic purpose for LLM-based extraction but misses crucial aspects about its implementation features like chunking, rate limiting, and support for different LLM providers that are central to its functionality",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the abstract class ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role is implicit as the base class providing the extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses crucial functionality details about the base class's role in providing extraction interface and parallel processing capabilities that are central to the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a call to crawler.arun(), which clearly demonstrates the usage of AsyncWebCrawler's arun() method to initiate the crawling process with the configured extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between arun() and the crawling process, but misses the crucial aspect of structured data extraction through LLM providers that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl, the snippet accesses the 'extracted_content' attribute of the result, which holds the structured extraction output as a JSON string.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that extracted_content holds structured data as a JSON string obtained after crawling. While it omits mention of Pydantic models, this is a minor detail that doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides examples for defining extraction schemas with keys such as 'type', 'nested', 'list', and 'nested_list'. This schema definition pattern aligns with the functionality provided by the JsonCssExtractionStrategy class, which accepts a schema (including 'fields' definitions) to extract content from HTML documents using CSS selectors. Although the class is not explicitly named in the documentation example, the demonstrated schema structure is implicitly used by JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the JsonCssExtractionStrategy class and the schema-based extraction functionality described in the documentation. Both descriptions emphasize the use of nested JSON schemas with CSS selectors for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation snippet shows code accessing 'result.media', implying that the result object is an instance of the CrawlResult class, which defines a public attribute for storing media elements such as videos and audios.",
    "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that CrawlResult has a media dictionary field that stores video and audio content metadata which can be accessed as shown in the documentation",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet explicitly iterates over result.media['videos'] and result.media['audios'], which corresponds to the 'media' attribute defined in the CrawlResult class. This attribute is responsible for holding video and audio metadata.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the media dictionary stores video and audio metadata that can be accessed and processed. The predicted description captures the key relationship of the media attribute storing video/audio metadata for processing.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions LLMExtractionStrategy by using backticks and showing its import and instantiation. The code sample passes parameters such as provider, schema, and instruction to LLMExtractionStrategy, indicating it is the class responsible for LLM-based extraction.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the class and its basic initialization parameters but misses the core functionality of processing HTML content through language models to extract structured data. It focuses mainly on the surface-level API usage rather than explaining the actual extraction mechanism.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example shows 'crawler.arun()' being called with an extraction_strategy argument. Although the snippet does not explicitly define or import the crawler, this method call implies that the crawling process leverages the extraction strategy. Hence, the call to AsyncWebCrawler.arun() is indirectly linked to the LLM-based extraction functionality.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that the arun() method works with extraction strategies to process web content, even though it doesn't detail all the implementation specifics mentioned in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates the AsyncWebCrawler class using 'async with AsyncWebCrawler() as crawler:', clearly demonstrating its usage as the primary interface for crawling a webpage.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used as the main interface for web crawling and accurately describes the async context manager pattern shown in the documentation, which aligns with the ground truth's description of the implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun(url=\"https://example.com\")', which clearly demonstrates the use of the 'arun()' method of the AsyncWebCrawler class to perform the crawl operation.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling but misses crucial aspects like content extraction, caching, and return of CrawlResult that are core to the implementation's functionality as described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After crawling, the code prints 'result.markdown', which accesses the public 'markdown' field of the CrawlResult object that is returned by the 'arun()' method.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that result.markdown is a field accessible after crawling that contains the webpage content, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet is titled 'Tips for Using JsonCssExtractionStrategy' and directly names the JsonCssExtractionStrategy class. This class uses CSS selectors to extract content based on a defined JSON schema and therefore directly implements the functionality described in the tips (such as inspecting and testing CSS selectors for dynamic content).",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship between the code and documentation - that the JsonCssExtractionStrategy class implements CSS-based content extraction that aligns with the documentation's tips about using CSS selectors and BeautifulSoup for HTML parsing",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not mentioned by name in the snippet, JsonCssExtractionStrategy inherits from ExtractionStrategy. This abstract base class establishes the interface for extraction strategies, which underpins the functionality of JsonCssExtractionStrategy. The documentation\u2019s context on selector-based extraction implicitly relies on this base functionality.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the fundamental relationship where ExtractionStrategy serves as a base class providing core functionality that enables JSON/CSS extraction features discussed in the documentation. The predicted description correctly identifies the inheritance relationship and base functionality aspect.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation advises that users 'Always check the result.success flag' to handle potential failures. This flag is a public attribute of the result object, defined as CrawlResult.success, and indicates whether a crawl operation succeeded.",
    "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the success flag is used for error handling and checking operation success, which aligns with the ground truth's explanation of how the flag implements the documentation's error handling guidance.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet demonstrates the instantiation of an AsyncWebCrawler using a context manager (i.e., 'async with AsyncWebCrawler() as crawler:'), which is an explicit call to create a crawler object.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the context manager implementation of AsyncWebCrawler, while the ground truth describes the fit_markdown feature and its content extraction functionality through the aprocess_html method. These are completely different aspects of the class.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the code examples, the 'arun()' method is directly called on the crawler instance (result = await crawler.arun(url=...)), making it an explicitly referenced method that performs the crawl action.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a crawler method but misses the core purpose of processing webpage content and implementing fit_markdown functionality with extraction/chunking strategies",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet accesses the 'fit_markdown' attribute of the result object (main_content = result.fit_markdown) to retrieve the optimized, filtered main content extracted by Crawl4AI, making it an explicit feature described in the documentation.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that fit_markdown is a property that contains filtered/extracted main content from webpages after removing boilerplate elements. The predicted description accurately captures the core functionality and purpose, even if it doesn't specify it's an optional string property.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The 'markdown' attribute, accessed via 'result.markdown', is used in the examples to compare the regular content extraction with the enhanced 'fit_markdown' output, making its use explicitly evident in the documentation.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the key relationship that markdown is the basic text extraction method that can be compared against fit_markdown, with the predicted description focusing on the comparison aspect shown in examples while the ground truth emphasizes the functional difference.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct invocation of 'crawler.arun()' with keyword arguments (process_iframes=True and remove_overlay_elements=True). This call maps explicitly to the 'AsyncWebCrawler.arun()' method implementation.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method invocation but misses a key aspect about how iframe processing is actually implemented through kwargs being passed to crawler_strategy.crawl()",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The options 'process_iframes=True' and 'remove_overlay_elements=True' passed to 'crawler.arun()' are handled internally by the crawl method of the distributed crawler strategy. 'AsyncPlaywrightCrawlerStrategy' (used by AsyncWebCrawler by default) implements logic to process iframe content and remove intrusive overlay elements, though it is not directly mentioned in the snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of processing iframes in AsyncPlaywrightCrawlerStrategy. Both describe that the strategy handles iframe content processing when process_iframes=True is passed, though the predicted is less detailed about the specific implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet begins with 'async with AsyncWebCrawler() as crawler:', explicitly instantiating the AsyncWebCrawler class to manage crawling sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's instantiation pattern but misses the crucial functionality aspects described in the ground truth regarding complex page interactions, JavaScript execution, and dynamic content loading capabilities.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls 'await crawler.arun(...)' for both initial page load and subsequent dynamic interactions. The method is invoked with parameters like 'js_code', 'wait_for', 'session_id', and 'js_only' to handle complex page behaviors.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that arun() implements dynamic page crawling functionality with parameters for JavaScript execution, session management and wait conditions, which aligns with the ground truth's description of how it enables complex page interactions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet accesses 'result.cleaned_html' (via 'len(result.cleaned_html)') to process and print the number of items loaded, indicating the use of the cleaned_html property from the crawl result.",
    "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that cleaned_html stores processed HTML content and is used to track loaded items through dynamic page loads. The predicted description correctly identifies its usage in the code example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "At the end of the example, 'await crawler.crawler_strategy.kill_session(session_id)' is called to clean up the session. This shows an explicit invocation of the kill_session() method to terminate an ongoing session in dynamic interactions.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main purpose of kill_session being used to clean up and terminate a session at the end of dynamic interactions, which aligns with the ground truth's explanation of resource cleanup.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions JsonCssExtractionStrategy as the recommended tool to 'extract highly structured data' from complex web pages. It also advises users to start with a basic schema and use keys like 'default' and 'transform', which are features supported by the JsonCssExtractionStrategy implementation.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that JsonCssExtractionStrategy implements schema-based data extraction that aligns with the documented tips, particularly around schema definition and incremental development.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not mentioned directly in the text snippet, its presence is implicit because any use of JsonCssExtractionStrategy automatically involves its base class, which defines the extraction interface and underlying schema handling.",
    "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, while the ground truth specifically describes ExtractionStrategy's parallel processing implementation using ThreadPoolExecutor. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates the use of the 'arun' method by invoking 'crawler.arun()' with the anti-detection parameters 'simulate_user' and 'override_navigator'. This explicit usage maps to the AsyncWebCrawler.arun() method.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the arun() method accepts parameters for anti-bot features, with the predicted description demonstrating this through a concrete example and the ground truth explaining it more generally. The core relationship is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler.arun() internally calls the crawl method of its default strategy, AsyncPlaywrightCrawlerStrategy. This class processes the parameters 'simulate_user' and 'override_navigator', implementing the manual anti-bot options described in the documentation.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncPlaywrightCrawlerStrategy class implements manual anti-bot options through the simulate_user and override_navigator parameters, which matches the ground truth's explanation of how these parameters enable anti-detection features.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which serves as the abstract base defining the crawling interface, including how anti-bot parameters might be handled. This establishes the underlying architectural design for crawler strategies.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy class defines the abstract interface for implementing the documented anti-bot features through abstract methods like crawl() that accept kwargs parameters where options like simulate_user and override_navigator can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncCrawlerStrategy serves as an abstract base class defining the crawler interface, which aligns with the ground truth's explanation of it defining the abstract interface for implementing anti-bot features through its methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows explicit usage of the method with the sample call 'await crawler.arun(...)'. The wait conditions (both CSS and JS based) are passed as the 'wait_for' parameter to this method, directly mapping to its intended interface.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the arun() method accepts wait conditions through parameters (whether directly named or via **kwargs) to control crawling behavior using CSS and JavaScript conditions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Though not mentioned by name in the snippet, the underlying implementation of wait conditions is handled inside AsyncPlaywrightCrawlerStrategy. Its 'crawl' method (which is invoked by arun()) calls the internal 'smart_wait' function to process the 'wait_for' parameter, interpreting the 'css:' and 'js:' prefixes.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles wait conditions through its smart_wait method, processing both CSS and JavaScript wait conditions as documented. The core functionality and relationship matches the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy serves as the abstract base class for crawler strategies. AsyncPlaywrightCrawlerStrategy extends it to provide concrete implementations, including the wait condition logic. This inheritance chain underpins the overall functionality described in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that AsyncCrawlerStrategy is an abstract base class that enables waiting functionality through its interface. While the predicted description mentions PlaywrightCrawlerStrategy which isn't in the ground truth, this doesn't contradict the core relationship described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of the AsyncWebCrawler class using a context manager (async with AsyncWebCrawler(...)). The provided parameters such as headless, verbose, and sleep_on_close directly reflect its configurable public interface.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncWebCrawler can be instantiated with configuration parameters via a context manager, which aligns with the ground truth's explanation of how these parameters are handled through __init__ and passed to AsyncPlaywrightCrawlerStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to the 'arun' method (await crawler.arun(url=\"https://example.com\")), demonstrating its use to execute a crawling operation. This method forms part of the public interface of AsyncWebCrawler.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a key public interface method, but misses the crucial aspect of how it interacts with configuration parameters passed through the constructor. The ground truth emphasizes this configuration relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet directly shows an instance being created with LLMExtractionStrategy using parameters (provider, api_token, instruction). This clearly demonstrates that the class is used to customize the LLM provider within Crawl4AI.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that LLMExtractionStrategy class enables customization of LLM providers through constructor parameters (provider, api_token, instruction). The predicted description captures the core functionality and purpose, matching the ground truth's explanation of how the class implements LLM provider customization.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy inherits from ExtractionStrategy. Although the base class is not explicitly mentioned in the snippet, it is a vital part of the extraction framework that establishes a contract for extraction strategies used in Crawl4AI.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions emphasize ExtractionStrategy's role as a base class that enables extensibility for different extraction implementations, particularly for LLM providers. The predicted description captures this core relationship accurately.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', demonstrating a direct use of this class as the starting point for crawling operations.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler usage but misses the core caching functionality that is central to the ground truth. The prediction focuses on class instantiation rather than the key caching behavior.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet the 'arun' method is called on the AsyncWebCrawler instance (e.g., result1 = await crawler.arun(url=...)), which shows that crawling functionality is implemented via this method.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method's role in crawling but misses the core caching functionality that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet accesses the 'markdown' attribute from the object returned by 'arun()' (result1.markdown[:100]), illustrating that the crawl output is encapsulated in a CrawlResult instance with a public 'markdown' field.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the markdown attribute/property of CrawlResult stores the crawled webpage content in markdown format and show it being accessed in the example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with 'async with AsyncWebCrawler(verbose=True) as crawler', indicating that this class is used to manage the crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role in managing crawling sessions, but misses the crucial aspect of supporting custom execution hooks through the crawler_strategy attribute, which is the main focus of the ground truth.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls 'await crawler.arun(...)' to perform the crawling operation, demonstrating explicit usage of the 'arun' method on the AsyncWebCrawler object.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method is used for crawling but misses the crucial aspect of how it enables custom hooks through crawler_strategy, which is a key part of the ground truth relationship",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "Within the snippet, the custom execution hook 'on_execution_started' is registered using the call 'crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)', directly invoking the set_hook() method.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the registration of the hook using set_hook(), but misses the crucial aspect of storing hooks in a dictionary for execution at specific lifecycle points mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet explicitly calls 'await crawler.crawler_strategy.kill_session(session_id)' to terminate a crawling session, thereby utilizing the kill_session() method of the crawler strategy.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session() is used to terminate a session, but misses the crucial aspect of cleaning up browser resources (closing page and context objects) which is the main purpose described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun(...)', which is an explicit invocation of the AsyncWebCrawler.arun() method. This method is responsible for initiating the crawl, processing the input URL, and returning a CrawlResult object that contains the cleaned HTML output.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately describe the core functionality of the arun() method - crawling a URL and cleaning/processing the HTML with configurable options. The predicted description captures the main relationship even though it's less detailed about the cleaning specifics.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet prints 'result.cleaned_html' immediately after the crawl operation. This indicates that the cleaned_html attribute of the CrawlResult object holds the sanitized HTML. This attribute is an integral part of the output format produced by the crawling process.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that cleaned_html is an attribute containing sanitized HTML output from the crawling process, which aligns with the ground truth's explanation of it storing sanitized HTML after removing unwanted elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using an async context manager (async with AsyncWebCrawler(verbose=True) as crawler:), which makes it the main entry point for crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main entry point for crawling but misses the core functionality of dynamic content crawling with pagination and JavaScript support shown in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example code, the arun() method of AsyncWebCrawler is explicitly called to crawl a URL using provided parameters such as session_id, css_selector, and extraction_strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling URLs with parameters, but misses the crucial dynamic content and session-based crawling aspects emphasized in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates JsonCssExtractionStrategy with a provided schema to extract commit information from the crawled HTML, making it the extraction strategy for the crawl.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that JsonCssExtractionStrategy uses a schema to extract structured data from HTML based on selectors and fields. The predicted description accurately describes the class's main functionality, even if it's slightly less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "After each crawl iteration, the example calls kill_session() on crawler.crawler_strategy to terminate the session by passing a session_id, ensuring proper cleanup of the browser context.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that kill_session is used to terminate/cleanup browser sessions after crawling by passing a session_id, which aligns with the ground truth's explanation of cleaning up browser resources by closing page and context objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly named in the snippet, AsyncWebCrawler by default instantiates AsyncPlaywrightCrawlerStrategy when no custom crawler_strategy is provided. This strategy handles the low-level browser interactions.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling with dynamic content handling through its crawl() method, which supports pagination through JavaScript execution (js_code parameter) and dynamic content loading (wait_for parameter) as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy as handling browser interactions, but misses the core functionality of session-based crawling and dynamic content handling through pagination and wait conditions that is central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, which is used by AsyncWebCrawler, extends AsyncCrawlerStrategy. This base class defines the abstract interface for crawler strategies, forming part of the underlying design.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods needed to implement the advanced session-based crawling functionality shown in the documentation, particularly the crawl method that handles the dynamic content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as the abstract base class that defines the interface for crawler strategies, which aligns with the ground truth's description of it defining core interface methods for session-based crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports CosineStrategy from crawl4ai.extraction_strategy and creates an instance with parameters 'semantic_filter', 'word_count_threshold', and 'sim_threshold'. This directly demonstrates its usage.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic instantiation of CosineStrategy but fails to mention the key functionality of content extraction and clustering that is central to the class's purpose as described in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is defined as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, it is inherently used as the base class for CosineStrategy, thus forming an implicit relationship.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy as base/subclass. While it's more concise than the ground truth, it captures the core relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet uses AsyncWebCrawler in an asynchronous context manager (using 'async with'), demonstrating that the crawler is directly instantiated and operated for web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the async context manager usage correctly but misses crucial aspects about the extraction strategy functionality and parameter handling that are central to the documented usage pattern",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "After instantiating AsyncWebCrawler, the snippet calls the 'arun()' method to perform the crawl, passing in the URL and the extraction strategy. This is an explicit usage of the arun() method.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of arun() with URL and extraction strategy, but misses significant aspects like asynchronous operation, configurable thresholds, and the CrawlResult return type that are crucial to understanding the full relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The return object from crawler.arun() is expected to be a CrawlResult, and the snippet accesses its 'extracted_content' attribute to obtain the extracted data. This is an implicit reference to the public interface provided by CrawlResult.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the crawler result, but misses the key aspect that it specifically holds filtered and clustered text content.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with verbose mode enabled. The usage of 'AsyncWebCrawler(verbose=True)' shows that the class is being directly used to enable detailed logging and debugging.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the verbose parameter enables detailed logging functionality in the AsyncWebCrawler class. The predicted description shows the usage pattern while the ground truth explains the implementation details, but they convey the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls the 'arun' method (result = await crawler.arun(url=\"https://example.com\")) on the AsyncWebCrawler instance to perform the crawling operation. This demonstrates how the public interface of AsyncWebCrawler is used.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the method call syntax and public interface usage, while the ground truth describes the internal verbose logging functionality implementation. These are different aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows usage of the 'arun()' method via the call 'result = await crawler.arun(...)'. This method is the entry point for crawling and cleaning content, as the parameters (e.g. word_count_threshold, remove_overlay_elements) are directly passed to it.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that the arun() method is the key interface for content cleaning functionality, accepting parameters that control the cleaning behavior. The predicted description focuses more on usage while ground truth focuses on implementation, but they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet prints 'result.cleaned_html' to display the cleaned HTML. This attribute represents the output after the cleaning process has been applied, showing the noise\u2010free HTML content.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that cleaned_html contains the cleaned HTML output after processing, which aligns with the ground truth's core meaning of storing sanitized HTML after cleaning steps",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example includes printing 'result.markdown', which provides the markdown version of the cleaned content. This shows how the processed content is made available in a different format.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that result.markdown provides content in markdown format, but misses the crucial aspect that this is a cleaned version with noise elements removed, which is a key part of the ground truth relationship",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the usage snippet, 'AsyncPlaywrightCrawlerStrategy' is the underlying crawler strategy used by 'AsyncWebCrawler.arun()'. The parameter 'remove_overlay_elements=True' in the call triggers internal cleaning functionality (i.e. removal of overlays/popups) that is implemented within this strategy.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy implements the cleaning functionality through remove_overlay_elements when triggered by the parameter, matching the core relationship described in the ground truth",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows direct instantiation of the AsyncWebCrawler class using the browser_type parameter (e.g., 'chromium', 'firefox', 'webkit'), which clearly demonstrates its role in supporting multiple browser engines.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports multiple browser engines, but incorrectly states that browser_type is passed directly to AsyncWebCrawler's constructor. The ground truth indicates this is actually handled through the crawler_strategy parameter.",
      "error_type": "implementation_detail_error"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls the arun() method on the AsyncWebCrawler instance to commence crawling, thus explicitly demonstrating its use as part of the browser configuration workflow.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun() method's role in crawling but misses the key relationship with browser engine selection via browser_type parameter that is central to the documented configuration functionality",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly mentioned in the documentation snippet, AsyncWebCrawler internally defaults to using AsyncPlaywrightCrawlerStrategy to handle browser automation and configuration based on the browser_type parameter. This underlying strategy is essential for enabling the behavior described.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncPlaywrightCrawlerStrategy handles browser type selection (Chromium, Firefox, WebKit) as specified in the documentation, with Chromium being the default option.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This inheritance relationship underpins the design of the crawling strategies, ensuring that browser configurations comply with a defined abstract interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the base class role in defining strategy behavior, but incorrectly specifies AsyncPlaywrightCrawlerStrategy as the implementation when the ground truth shows it's about multiple browser type implementations (Chromium, Firefox, WebKit).",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an AsyncWebCrawler instance using 'async with AsyncWebCrawler(verbose=True) as crawler', indicating its direct role in initiating the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role but focuses only on instance creation, missing the core error handling functionality described in the ground truth. The ground truth specifically emphasizes the error handling implementation in arun() method.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates LLMExtractionStrategy with parameters (provider, api_token, instruction), which is used to define how the content extraction via an LLM should be performed.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy and some basic parameters but misses the core functionality around error handling and parallel processing described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code calls the 'arun()' method on the AsyncWebCrawler instance (via 'crawler.arun(...)'), which is implicitly responsible for executing the crawl and extraction operation with the provided strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality of arun() but misses the crucial error handling aspect that is central to the ground truth relationship. The documentation specifically focuses on error handling, which is a key feature implemented in the code.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses 'result.extracted_content' to retrieve the processed extraction output from the crawl result, demonstrating usage of this public attribute as the endpoint for extraction data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the result object to get extraction output, but misses the key context about error handling and retries which is central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using 'from crawl4ai import AsyncWebCrawler' and 'async with AsyncWebCrawler(verbose=True) as crawler:'.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the import/instantiation syntax shown in the documentation, while missing the core functionality alignment between the AsyncWebCrawler implementation and its documented features. However, it is accurate about the basic connection between the code and documentation.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the method 'arun' is explicitly called on the AsyncWebCrawler instance to perform the crawling process.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of arun() but misses the crucial relationship between the documented parameters and their implementation in the actual method definition that the ground truth emphasizes",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code snippet checks 'if result.success:' to determine if the crawl was successful, indicating direct use of the 'success' attribute from the CrawlResult object.",
    "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core usage of success as a boolean check but omits the important context that this check guards the processing of extracted content, media and links",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example prints 'result.markdown[:500]' to output the clean content, directly accessing the markdown attribute of the CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that result.markdown contains the cleaned content and is accessed to display the first 500 characters. The predicted description conveys the same core relationship even though it's more concise.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The sample code iterates over 'result.media[\"images\"]' to process and print information about found images, indicating usage of the CrawlResult.media attribute.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of accessing and iterating over images stored in result.media['images'], which aligns with the ground truth's explanation of the CrawlResult.media dictionary property and its usage for storing and accessing media elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "In the example, the 'result.links[\"internal\"]' attribute is used to process internal links, directly referencing the links attribute of CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core idea that links are accessed through result.links['internal'], but misses the important aspect that links are categorized into both internal and external links in the dictionary",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "If crawling fails, the example prints out 'result.error_message', directly accessing the error_message attribute of the CrawlResult.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that error_message is used to communicate failure reasons when crawling fails. The predicted description focuses on the specific print statement while the ground truth gives a more general explanation, but the fundamental relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy to configure an LLM\u2010based extraction strategy for smart content selection.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions LLMExtractionStrategy's role in content extraction but misses crucial aspects about structured formatting via schema and instruction parameters that define how the content should be parsed",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is an extension of ExtractionStrategy. Although ExtractionStrategy is not explicitly mentioned in the snippet, it underpins the extraction functionality, making it a necessary base class.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy as a base class, which aligns with the ground truth's explanation of ExtractionStrategy providing the foundation for specialized extractors like LLMExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls 'crawler.arun()' to perform the crawling operation while supplying the extraction strategy. This indicates that the 'arun()' method of the crawler is used to integrate and execute the extraction process.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() uses the extraction strategy, but misses the key aspect that it specifically implements LLM-based extraction with schema and instruction processing",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet shows the use of 'result.extracted_content' to retrieve the structured content extracted by the LLM. This attribute is central to obtaining the output of the extraction process.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is used to retrieve the extraction output, but misses the crucial aspect that it's specifically stored as a JSON string matching a Pydantic schema for typed parsing",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler with proxy parameters (e.g., proxy and proxy_config) used in the 'async with AsyncWebCrawler(...)' block.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly shows that AsyncWebCrawler works with proxy settings, but implies these are direct constructor parameters rather than being passed through kwargs to the crawler strategy.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'crawler.arun(url=\"https://example.com\")', explicitly demonstrating the use of the arun() method of AsyncWebCrawler to perform the crawling operation.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies correct usage of arun() but misses the core relationship about proxy configuration support that is central to the ground truth. It describes basic method usage rather than the proxy functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the snippet, AsyncWebCrawler internally creates an instance of AsyncPlaywrightCrawlerStrategy (its default crawler_strategy) which accepts and processes the proxy and proxy_config parameters.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly states that AsyncPlaywrightCrawlerStrategy handles proxy configuration, but incorrectly suggests this happens through AsyncWebCrawler creating an instance, rather than directly in AsyncPlaywrightCrawlerStrategy's own implementation through browser_args and ProxySettings as described in the ground truth.",
      "error_type": "incorrect_implementation_detail"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This base class underpins the strategy\u2019s implementation, indirectly supporting proxy configuration even though it is not directly referenced in the snippet.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but incorrectly specifies AsyncPlaywrightCrawlerStrategy and focuses on inheritance rather than the interface's role in enabling proxy configuration",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates and uses the AsyncWebCrawler class with 'async with AsyncWebCrawler(verbose=True) as crawler:', explicitly demonstrating its usage as the central crawler class in Crawl4AI.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the central crawler class and demonstrates its usage through async context manager syntax, which aligns with the ground truth's description of the class's implementation of __aenter__/__aexit__ and its core crawling functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the 'arun' method of AsyncWebCrawler is invoked using 'await crawler.arun(url=\"https://www.nbcnews.com/business\")', which clearly shows that this method is responsible for performing the crawling function.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is the method for crawling and shows correct usage, but it misses significant functionality mentioned in the ground truth like caching, HTML extraction, and processing capabilities that are core to the relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After calling the 'arun' method, the returned result (a CrawlResult object) is accessed via its 'markdown' attribute (result.markdown[:500]). This shows that the output includes markdown content, which is a key component of the public interface.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the markdown attribute stores/contains the extracted text content from the crawl result that gets accessed and printed",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly names 'Cosine Strategy' and explains its effectiveness when dealing with inconsistent content structure and the need for semantic understanding. This directly points to the CosineStrategy class as an extraction option.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies the core purpose of CosineStrategy as handling inconsistent content structure and semantic understanding, which aligns with the ground truth's explanation of its semantic-based extraction using cosine similarity and clustering methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is built on top of ExtractionStrategy, its abstract base class. Although not directly named in the snippet, its role is implicit since CosineStrategy extends this foundational class.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that CosineStrategy inherits from ExtractionStrategy as its base class. While the ground truth provides more implementation details about the extract() method and parallel processing, these are minor details that don't affect the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example in the documentation shows a call to 'crawler.arun()' for crawling and extraction. This usage explicitly maps to the AsyncWebCrawler.arun() method which performs the operation and returns the result used in error handling.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its connection to crawler operations, but misses the key focus on error handling implementation which is central to both the code and documentation example.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation example inspects attributes such as 'success', 'extracted_content', and 'error_message' from the result of the crawl, which corresponds directly to the CrawlResult data model.",
    "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the CrawlResult class and its usage in error handling shown in the documentation, noting how the key fields (success, extracted_content, error_message) are used for status checking and error reporting.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet clearly shows a direct call to 'crawler.arun()' with keyword arguments for smart link filtering. This usage explicitly maps to the 'AsyncWebCrawler.arun()' method, which is designed to accept additional parameters (via **kwargs) that can control behaviors such as excluding external and social media links.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that arun() accepts keyword arguments for link filtering parameters which get passed through to control crawling behavior. Both descriptions align on this key functionality without major contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet directly calls 'arun()', the 'crawler' object is implicitly an instance of the 'AsyncWebCrawler' class. This class encapsulates the 'arun()' method and underpins the overall crawling functionality, including smart link filtering.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler class handles the crawling functionality through arun() method, while the ground truth provides more implementation details about link filtering through kwargs. The core relationship is accurately represented even though specific filtering details are omitted.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation snippet explicitly references CrawlResult (via the link [CrawlResult]) and shows it as the return type from the arun() method. This signals that CrawlResult is the primary data model encapsulating the crawl response.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies CrawlResult as the primary data model for crawl responses, which aligns with the ground truth showing CrawlResult as a class containing all the demonstrated properties. While it doesn't list all fields explicitly, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The sample code uses 'print(result.html)' to output raw HTML. This explicitly maps to the CrawlResult.html attribute.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that there is an 'html' property containing raw HTML content as part of the CrawlResult object's properties.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet shows 'print(result.cleaned_html)' to display the cleaned HTML, directly corresponding to the CrawlResult.cleaned_html attribute.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies cleaned_html as an attribute used to display cleaned HTML, but misses the crucial fact that it's an Optional[str] property",
      "error_type": "missing_key_property"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "Usage example 'print(result.markdown)' clearly shows that the markdown version of the content is provided by this attribute.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the markdown attribute provides the Markdown version of the content, which aligns with the ground truth's explanation of the markdown property storing the converted Markdown representation of the crawled webpage content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The example 'print(result.fit_markdown)' shows the use of this attribute to output the most relevant markdown content.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown outputs/contains the most relevant markdown content, which aligns with the ground truth's explanation that it stores the most relevant content in markdown format. While the ground truth mentions it's optional and provides more detail, the core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation uses 'print(result.success)' to check if crawling was successful, directly referring to the success attribute.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that success is used to check if crawling was successful, which aligns with the ground truth's explanation of success as a boolean field indicating successful crawl completion.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The sample 'print(result.status_code)' reflects the retrieval of the HTTP status code (e.g., 200, 404) from the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that status_code represents the HTTP status code (like 200, 404) returned from the web request, which aligns with the ground truth's explanation of the status_code property and its purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code example 'print(result.media)' is used to access media extracted during crawling, mapping directly to the CrawlResult.media attribute.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that result.media contains extracted media, but misses that the code is actually defining an empty dictionary structure to store this media, rather than accessing/printing it",
      "error_type": "incorrect_operation_described"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The example 'print(result.links)' demonstrates that the links (both internal and external) are accessible via the CrawlResult.links attribute.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that links (both internal and external) are accessible through the CrawlResult.links attribute, which aligns with the ground truth's description of a dictionary property storing internal and external links that can be accessed from the CrawlResult object.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly states that 'Crawl4AI's AsyncWebCrawler class supports session-based crawling', directly naming this class as providing persistent browser session functionality through the session_id parameter.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship by stating that AsyncWebCrawler supports session-based crawling through the session_id parameter, which matches the ground truth's description of the class's functionality for maintaining persistent browser sessions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler strategy. Within its crawl() method, it checks for a session_id to reuse browser sessions, thus indirectly enabling session-based crawling as described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the class implements session-based crawling functionality through maintaining browser sessions. The predicted description correctly identifies the key mechanism (session_id and browser session reuse) even if it doesn't mention the specific implementation details about the sessions dictionary.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. Although not directly handling sessions, it defines the abstract interface that supports methods such as crawl() which are implemented in its subclass to manage sessions.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface for session-based crawling through abstract methods like crawl() and crawl_many() that implement the documented session maintenance and page interaction capabilities.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify AsyncCrawlerStrategy as an abstract base class that defines core crawling interfaces through abstract methods. While the predicted description mentions AsyncPlaywrightCrawlerStrategy, it correctly describes the abstract nature and method definitions that support the documented functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation instructs users to keep using the same session_id across multiple arun calls to maintain state. The arun() method of AsyncWebCrawler accepts a session_id parameter and incorporates it into its logic, fulfilling this requirement.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that the arun() method handles session-based crawling by accepting and managing a session_id parameter. The predicted description accurately reflects the ground truth's explanation of how session state is maintained across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the async context manager. The code 'async with AsyncWebCrawler(verbose=True) as crawler:' directly references this class.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the use of AsyncWebCrawler with async context manager, but misses discussing the core arun() functionality that processes web crawling requests, which is a crucial aspect mentioned in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the arun() method is explicitly invoked on the crawler instance to perform the crawl operation. The example 'result = await crawler.arun(url=\"https://www.nbcnews.com/business\")' clearly references this method.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method usage but focuses only on the method invocation aspect, missing crucial functionality like HTML extraction, caching, and processing that are core aspects of the relationship described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet accesses and prints a slice of the markdown attribute via 'result.markdown[:500]'. This makes it clear that the returned CrawlResult object contains a 'markdown' field that is used to display the result.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the markdown attribute stores content that can be accessed and printed from the result object, which aligns with the ground truth's explanation of markdown storing extracted text content from the crawled website.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This clearly demonstrates that the AsyncWebCrawler class is the entry point for managing dynamic sessions.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as handling dynamic sessions, but focuses only on the initialization syntax while missing the crucial functionality aspects like JavaScript execution and custom extraction strategies mentioned in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the arun() method of the AsyncWebCrawler instance is directly invoked to perform the crawling of a GitHub commits page and extract commit data.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling GitHub commits, but misses crucial functionality around session management and caching that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code explicitly creates an instance of JsonCssExtractionStrategy by passing a schema. This strategy object is used as a parameter in the call to arun(), guiding the extraction of commit information from the crawled HTML.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is instantiated with a schema and used for extraction, but misses the core implementation detail about using BeautifulSoup to parse HTML and apply selectors for data extraction",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "At the end of the example, the kill_session() method is explicitly called on the crawler's strategy object to clean up and terminate the session, ensuring proper resource management.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that kill_session is used to clean up and terminate browser sessions/resources at the end of the crawling process",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates JsonCssExtractionStrategy with a schema and verbose flag to extract structured data from dynamic pages. This direct instantiation demonstrates its intended use for handling dynamic content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses a schema for structured data extraction and works with dynamic content, which aligns with the ground truth's explanation of the class's purpose and functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly mentioned in the snippet, this base class defines the extraction contract that JsonCssExtractionStrategy implements.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that ExtractionStrategy is the base class that provides the foundation for specific extraction strategies like JsonCssExtractionStrategy. The predicted description captures this core inheritance relationship, even though it's more concise than the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly uses AsyncWebCrawler in an asynchronous context ('async with AsyncWebCrawler(verbose=True)') to perform web crawling for dynamic content.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's asynchronous nature but misses key functionality around dynamic data extraction with JavaScript execution and custom extraction strategies that are central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the AsyncWebCrawler context, the arun() method is called with parameters such as extraction_strategy, js_code, wait_for, and bypass_cache. This method invocation demonstrates how the crawler is run with dynamic JavaScript execution and extraction.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of the arun() method supporting dynamic content extraction through JavaScript execution and extraction strategy parameters, which aligns with the ground truth's description of the same capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the extracted_content attribute of the CrawlResult is accessed to retrieve the structured data output. This demonstrates how extraction results are made available to the user.",
    "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that extracted_content stores output data, but misses the key context that this is specifically for dynamic crypto pricing data processed through JavaScript execution",
      "error_type": "missing_key_context"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy if none is provided. This strategy supports JavaScript execution required for dynamic content loading.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its crawl method by executing JavaScript code and waiting for specified selectors as documented in the example's advanced usage section for combining with JavaScript execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions the JavaScript execution capability but incorrectly states AsyncPlaywrightCrawlerStrategy is the default strategy of AsyncWebCrawler, which is not mentioned in the ground truth or code",
      "error_type": "unsubstantiated_claim"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows a call to 'crawler.arun()' with parameters such as 'word_count_threshold', 'excluded_tags', 'exclude_external_links', 'exclude_social_media_links', and 'exclude_external_images'. These parameters are used to control content filtering and directly map to the AsyncWebCrawler.arun() method's interface.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts content filtering parameters that are passed as kwargs. Both descriptions acknowledge that these parameters control content filtering and are used during the crawler's execution.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet does not name the class explicitly, the variable 'crawler' is used to invoke the 'arun()' method. This implies that 'crawler' is an instance of the AsyncWebCrawler class, making the class an implicit part of the content filtering functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and arun method connection, but misses explaining the core content filtering functionality and control parameters that are central to the ground truth's description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates JsonCssExtractionStrategy with a defined schema. This clearly demonstrates its intended use for pattern\u2010based extraction of repetitive elements like product cards.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of JsonCssExtractionStrategy as a pattern-based extraction tool that uses CSS selectors to scrape repetitive elements according to a defined schema",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, which defines the contract for all extraction strategies. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the abstract base class is implicit in the functionality demonstrated.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables pattern-based extraction through derived classes like JsonCssExtractionStrategy, which implements the schema-based extraction shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the core relationship where ExtractionStrategy is the base class that provides the framework for specialized extraction strategies like JsonCssExtractionStrategy. The predicted description correctly identifies the inheritance relationship and contract-based nature of the design.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls 'await crawler.arun(...)' to perform the crawling operation with the custom extraction strategy. This demonstrates an explicit use of the arun() method to integrate the extraction process into the web crawling workflow.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling operations, but misses the key relationship with extraction_strategy and JsonCssExtractionStrategy for structured data extraction using CSS selectors",
      "error_type": "key_aspect_omission"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the documentation accesses 'result.extracted_content' to retrieve the extracted data. This attribute is a key part of the public interface that delivers the processed content.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies extracted_content as delivering processed content but misses the crucial pattern-based/schema-driven nature of the extraction and the structured JSON array output format described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports CosineStrategy from crawl4ai.extraction_strategy and instantiates it with parameters (semantic_filter, word_count_threshold, sim_threshold) to target product reviews.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship shown in the code and documentation - importing and instantiating CosineStrategy with configurable parameters for semantic filtering and thresholds.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, ExtractionStrategy is the abstract base class from which CosineStrategy derives, underpinning its extraction behavior.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between ExtractionStrategy as the abstract base class and CosineStrategy as a derived class, which aligns with the ground truth's description of the relationship structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example explicitly creates an AsyncWebCrawler instance using an async context manager, which initiates the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core async context manager usage but omits the crucial aspect of extraction strategy parameter handling and extracted content functionality shown in the documentation example",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet directly calls the arun() method of AsyncWebCrawler to crawl a URL and process content with the provided extraction strategy.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of using arun() to crawl URLs, but omits key aspects like asynchronous behavior and the return of CrawlResult with extracted content that are crucial to understanding the full functionality.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After invoking the crawl method, the example accesses the extracted_content attribute of the resulting CrawlResult to retrieve the processed content.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of accessing extracted content, but misses the important aspect of semantic filtering and clustering strategy being applied to the content",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The documentation snippet defines a complex JSON schema with keys like 'baseSelector' and 'fields', which corresponds to the expected input for extraction routines. This aligns with the blueprint provided by the ExtractionStrategy abstract base class, even though the text does not explicitly mention it.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes the schema-extraction relationship but doesn't fully capture how ExtractionStrategy implements and processes the schema pattern through its abstract methods",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The provided schema example demonstrates nested objects, simple lists, and nested lists using CSS selectors. JsonCssExtractionStrategy is designed to accept such a schema (by using the 'baseSelector' and 'fields' properties) to extract complex HTML structures, making it the concrete implementation that embodies this advanced extraction approach.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship - that JsonCssExtractionStrategy uses a schema with CSS selectors to extract complex nested HTML structures. While it doesn't explicitly mention BeautifulSoup's select() method, this is a minor implementation detail that doesn't change the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows how to create an instance of LLMExtractionStrategy by passing in parameters such as provider, api_token, and instruction. This code example directly demonstrates how to customize the LLM provider.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core concept that LLMExtractionStrategy allows customization of LLM providers through parameters passed to its constructor, which matches the ground truth's explanation of how the code implements provider customization.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the base class ExtractionStrategy. Although not directly mentioned in the snippet, this inheritance relationship underpins the implementation of the extraction process shown. It implies that LLMExtractionStrategy inherits the interface and functionality from ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy serving as a foundation for implementing custom LLM providers.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy from crawl4ai.extraction_strategy and instantiates it with a provider, schema, and instruction. This shows that the strategy is used for smart extraction of structured content. Additionally, LLMExtractionStrategy extends the abstract base class ExtractionStrategy, tying it into the extraction strategy hierarchy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that LLMExtractionStrategy is used for structured content extraction and correctly identifies its key components (provider, schema, instruction) and inheritance relationship. While it focuses more on the import/instantiation syntax than the ground truth, it conveys the same core functionality.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on the crawler instance (i.e. await crawler.arun(...)), indicating that this method is invoked to execute the crawling process using the defined extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling, but misses the key relationship to LLM-based extraction functionality and schema-based content processing described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl, the result's 'extracted_content' attribute is accessed (via result.extracted_content), showing that the extraction outcome is stored in this public interface of CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed as an attribute of the result, but misses the crucial aspect that it contains LLM-processed structured JSON content matching a Pydantic schema",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(...)' in its usage example. This directly maps to the 'AsyncWebCrawler.arun()' method as shown in the example.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only notes the superficial mapping of the method name in example usage, while missing the core functionality of handling dynamic content and form interactions that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Since 'arun()' is a method of the AsyncWebCrawler class, the crawler instance used in the snippet is implicitly an instance of AsyncWebCrawler. This class encapsulates the public crawling interface including 'arun()'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts js_code and wait_for parameters to execute JavaScript for scrolling, form interactions, and waiting for content loads as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as a method of AsyncWebCrawler but misses the core dynamic content handling functionality described in ground truth, including js_code execution and wait_for parameters",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses a crawler_strategy which, by default, is an instance of AsyncPlaywrightCrawlerStrategy. This strategy handles parameters like 'js_code' and 'wait_for' for dynamic content, as demonstrated in the documentation snippet.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles dynamic content and uses parameters like 'js_code' and 'wait_for', but misses explaining the key smart_wait method's role in implementing these features and how it handles both scroll-to-bottom and form interaction patterns specifically.",
      "error_type": "key_component_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the crawling interface. AsyncPlaywrightCrawlerStrategy extends this base class, meaning that dynamic content handling (via js_code and wait conditions) is built on top of the contract established by AsyncCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that AsyncCrawlerStrategy is an abstract base class that provides the foundation/interface for implementing dynamic content handling and interactions through its abstract methods. The predicted description captures this core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(...)' to initiate the crawl, which is directly associated with the 'AsyncWebCrawler.arun()' method. This method is responsible for fetching and processing the webpage content, including media extraction.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() performs crawling and webpage content processing, but misses crucial functionality around media handling, lazy-loading support, and customizable parameters that are central to the documented media processing capabilities.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet iterates over 'result.media[\"images\"]', explicitly accessing the 'media' attribute of the CrawlResult data model. This attribute stores detailed media information (such as image source, alt text, description, context, and a relevance score) extracted during the crawl.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the media dictionary stores image metadata and details (source, alt text, description, context, score) from web crawling, which aligns with the ground truth's core meaning.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'JsonCssExtractionStrategy' in its header and explanation, highlighting its advanced usage for handling complex nested HTML structures.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies JsonCssExtractionStrategy and its purpose for complex HTML structures, but omits the crucial implementation detail about using BeautifulSoup and schema-based selectors for recursive processing",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly mentioned in the text, ExtractionStrategy underpins the functionality of JsonCssExtractionStrategy by defining the extraction interface, thereby implicitly affecting advanced usage scenarios.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy and acknowledges the base class's role in defining extraction functionality. While it's more concise than the ground truth, it captures the essential inheritance relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly invokes the 'arun()' method (e.g., 'result = await crawler.arun(url=\"https://example.com\")'). This shows that the method is used to initiate a crawl and return the crawl results.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately captures that arun() is used for crawling and returns results, but misses explaining that it processes the content and returns a CrawlResult with specific properties (html, markdown, status, etc.) through extraction/chunking strategies.",
      "error_type": "crucial_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'arun()' method returns a 'CrawlResult' object, as explicitly mentioned in the documentation. This object encapsulates various response properties that are subsequently accessed.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() returns a CrawlResult object which contains various response properties. While it's less detailed than the ground truth, it captures the core relationship between the method and the class without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The documentation example explicitly prints 'result.html' to display the raw HTML content. This attribute is a public interface of the CrawlResult object.",
    "ground_truth_relationship": "The code defines a string property 'html' that stores the raw HTML content mentioned in the documentation's overview of CrawlResult properties.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that html is a property/attribute of the CrawlResult object that contains raw HTML content. The predicted description adds context about it being shown in the documentation example, but the core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet prints 'result.cleaned_html' to show the processed (cleaned) HTML. This explicitly demonstrates the use of a public attribute within the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult class defines cleaned_html as an optional string property that stores the sanitized version of the webpage's HTML content after processing.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies cleaned_html as an attribute/property of CrawlResult but misses that it's an optional string and focuses on printing rather than its fundamental definition",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example shows 'result.markdown' being printed, which outputs the Markdown version of the content. This demonstrates a direct usage of a CrawlResult property.",
    "ground_truth_relationship": "The CrawlResult class includes a markdown property that stores the converted Markdown representation of the crawled webpage content, which can be accessed through result.markdown as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that result.markdown outputs the Markdown version of the content, which aligns with the ground truth's explanation of the markdown property storing and providing access to the Markdown representation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation prints 'result.fit_markdown' to show the most relevant content in Markdown. This attribute is part of the CrawlResult\u2019s public interface.",
    "ground_truth_relationship": "The fit_markdown property in CrawlResult stores an optional string containing only the most relevant content of a crawled webpage in markdown format.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown is a property of CrawlResult that contains content in markdown format. While it doesn't explicitly mention it's optional or that it contains 'most relevant' content, these are minor details that don't change the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation uses 'result.success' to verify if the crawl operation succeeded. This property clearly represents the operation's status.",
    "ground_truth_relationship": "The CrawlResult.success property is implemented as a boolean field that indicates whether a web crawl operation completed successfully, as shown in the documentation's example where it can be checked via result.success.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that result.success indicates whether a crawl operation succeeded, which aligns with the ground truth's explanation of it being a boolean field indicating successful completion of a web crawl operation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "By printing 'result.status_code', the documentation shows how to access the HTTP status code of the crawl (e.g., 200, 404). This attribute is part of the CrawlResult interface.",
    "ground_truth_relationship": "The CrawlResult's status_code property, implemented as an Optional[int], stores the HTTP status code from the web request which the documentation shows being used to verify successful crawls through print statements.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that status_code represents the HTTP status code of the crawl and is accessible through the CrawlResult interface, which aligns with the ground truth's core meaning. The minor implementation detail of Optional[int] being omitted doesn't affect the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation example accesses 'result.media' to obtain a dictionary of media items (images, videos, audio). This is a public attribute of the CrawlResult.",
    "ground_truth_relationship": "The code defines an empty dictionary property 'media' that will store lists of media elements (images, videos, audio) extracted during crawling, which directly implements the media property shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies media as containing media items (images, videos, audio), but describes it as a result attribute being accessed rather than the empty dictionary being defined/initialized in the code",
      "error_type": "implementation_context_mismatch"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The code snippet prints 'result.links' to obtain a dictionary of internal and external links found during crawling. This attribute is integral to the CrawlResult class.",
    "ground_truth_relationship": "The code implements a dictionary property that stores both internal and external links found during crawling, which is documented as an accessible property of the CrawlResult object.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the links property is a dictionary containing internal and external links found during crawling, and both indicate it's an accessible property of the CrawlResult object.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun()' with the 'js_code' parameter, demonstrating how to execute JavaScript commands during crawling. This method is explicitly invoked in the usage examples.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles JavaScript execution, but misses the crucial implementation detail that it forwards js_code to crawler_strategy.crawl() rather than executing it directly",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not directly invoked in the snippet, the 'AsyncPlaywrightCrawlerStrategy' class implements the 'crawl' method that processes the 'js_code' parameter. The 'AsyncWebCrawler.arun()' method delegates to this strategy to execute the JavaScript commands, making its functionality implicitly used for JavaScript execution.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality that the code executes JavaScript commands through the js_code parameter in the crawl method using Playwright's evaluate function. While it provides less detail than the ground truth, it does not contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "'AsyncCrawlerStrategy' is the abstract base class that defines the interface for asynchronous crawling strategies. 'AsyncPlaywrightCrawlerStrategy' extends this class to implement functionality such as the processing of the 'js_code' parameter for executing JavaScript commands.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy defines the interface for crawling functionality that includes JavaScript execution capabilities. While the predicted description focuses more on the class structure and the ground truth emphasizes the JavaScript execution examples, they don't contradict each other and describe complementary aspects of the same relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with verbose=True to enable detailed logging, indicating the use of this class.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that verbose=True enables logging, but misses the key details about what specifically gets logged (crawler initialization, warmup status, crawling progress) that the ground truth specifies.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls the 'arun' method on an AsyncWebCrawler instance to perform crawling. This explicit usage of the method supports its inclusion in the trace.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the code as involving the arun method for crawling, but misses the core relationship about verbose logging functionality that the ground truth correctly describes",
      "error_type": "missing_core_concept"
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows the use of 'crawler.arun(url=\"https://example.com\")' to perform the crawl. This directly maps to the AsyncWebCrawler.arun() method which produces the output containing various formats.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method is responsible for crawling and producing the output formats shown in the documentation. While it doesn't detail all the processing steps, it accurately identifies the core relationship between the method and its outputs.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The snippet accesses 'result.html' to retrieve the original raw HTML. This attribute is defined as part of the CrawlResult class and represents the raw HTML output.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the html attribute contains raw HTML content, which aligns with the ground truth's definition of html as a string type for storing raw HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation demonstrates retrieving sanitized HTML via 'result.cleaned_html', which directly relates to the cleaned_html attribute in the CrawlResult class.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that cleaned_html provides access to sanitized HTML content, which aligns with the ground truth's explanation of it being a sanitized HTML version of the crawled webpage",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The example shows 'result.markdown' to obtain a standard markdown version of the page, which maps directly to the markdown attribute defined in the CrawlResult class.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the markdown attribute stores/provides standard markdown format of the content and can be accessed via result.markdown",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet accesses 'result.fit_markdown' to retrieve the most relevant content in markdown format. This attribute corresponds exactly to the fit_markdown field in the CrawlResult class.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown contains the most relevant content in markdown format, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet\u2019s second example shows using 'async with AsyncWebCrawler() as crawler:' to start a crawling session. This indicates that AsyncWebCrawler is the entry point for initiating the crawl that eventually produces the smart content extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the entry point, but misses the key aspect that it's specifically the aprocess_html method that implements the fit_markdown functionality through its scraping strategy and content processing",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The first code example explicitly calls 'await crawler.arun(url=\"https://example.com\")'. This method performs the crawling process and returns a CrawlResult that contains extraction attributes. It is directly responsible for obtaining the content that includes both the smart (fit_markdown) and regular markdown outputs.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() performs crawling and returns a CrawlResult, but misses the crucial aspect of extraction/chunking strategies used for identifying relevant content blocks and filtering boilerplate elements.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation highlights 'fit_markdown' as Crawl4AI's powerful feature for smart content extraction, explicitly showing its usage via 'main_content = result.fit_markdown'. This attribute is designed to hold the main, relevant content of the webpage while filtering out boilerplate material.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is a feature for extracting main content while filtering out boilerplate elements, which aligns with the ground truth's definition of it being an optional string property that stores extracted main content after removing boilerplate elements.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the provided code snippet the regular markdown is extracted via 'all_content = result.markdown'. This attribute serves as a baseline for content extraction, allowing users to compare it with the more refined 'fit_markdown' output.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship that markdown is a base text extraction method that can be compared against fit_markdown, which is consistent with the ground truth. Both descriptions emphasize the comparative nature between regular markdown and fit_markdown.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses AsyncWebCrawler as seen in the line 'from crawl4ai import AsyncWebCrawler', and it is instantiated in the context manager 'async with AsyncWebCrawler() as crawler:'.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of AsyncWebCrawler but misses the crucial aspect of the implementation details including the arun method and async context manager pattern that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method on the AsyncWebCrawler instance as observed in the line 'result = await crawler.arun(url=\"https://example.com\")'.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but only focuses on the method call syntax, missing the broader functionality around web crawling, content extraction, and caching described in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The usage 'print(result.markdown)' in the snippet clearly demonstrates that the markdown attribute of the CrawlResult object is accessed to display the clean markdown content extracted by the crawler.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the markdown attribute contains the cleaned webpage content that can be accessed from the CrawlResult object returned by the crawler",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'await crawler.arun(...)' with options such as word_count_threshold, remove_overlay_elements, and process_iframes. This clearly maps to the AsyncWebCrawler.arun() method where these parameters are accepted and utilized.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documented crawler.arun() method corresponds to the implementation, accepting the same configuration options shown in the documentation through its parameters and kwargs system.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet uses 'crawler.arun(...)', indicating that the 'crawler' object is an instance of AsyncWebCrawler. Although the class name isn\u2019t mentioned in the snippet, it is implicitly used as the container for the 'arun' method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that AsyncWebCrawler is used and contains the arun method, but misses the key relationship that the class implements the documented configuration options through its parameters. It focuses only on the structural relationship rather than the functional relationship described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Options like remove_overlay_elements and process_iframes shown in the snippet are handled internally. AsyncWebCrawler uses a crawling strategy (defaulting to AsyncPlaywrightCrawlerStrategy) to process these options. Hence, this class is indirectly responsible for implementing the functionality triggered by the provided parameters.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the AsyncPlaywrightCrawlerStrategy class implements the documented crawling options (like process_iframes and remove_overlay_elements) and executes them during crawling. The predicted description correctly identifies that this class handles these options internally, even if it adds unnecessary context about AsyncWebCrawler.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows an example calling the 'arun' method on the crawler instance with parameters 'process_iframes=True' and 'remove_overlay_elements=True'. This directly corresponds to the 'AsyncWebCrawler.arun()' method usage.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles iframe processing through parameters, even though it doesn't explicitly mention the kwargs implementation detail from the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, the 'AsyncWebCrawler.arun()' method internally delegates crawling functionality to an instance of 'AsyncPlaywrightCrawlerStrategy'. This class implements the handling of the 'process_iframes' and 'remove_overlay_elements' flags, thereby fulfilling the documented functionality of processing iframe content and removing overlays.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the high-level relationship - that AsyncPlaywrightCrawlerStrategy handles iframe processing functionality when process_iframes=True is passed. While it's less detailed than the ground truth, it correctly identifies the core relationship and functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet includes an example call 'result = await crawler.arun(url=\"https://example.com\")', which directly invokes the AsyncWebCrawler.arun() method. This method is responsible for retrieving the raw HTML from a webpage, as described in the snippet.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the arun() method and raw HTML retrieval functionality shown in the documentation example. Both descriptions explain that the method fetches webpage HTML content, and the example usage matches.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "In the example, the expression 'print(result.html)' accesses the 'html' attribute of the CrawlResult object. This attribute holds the raw, unmodified HTML content as returned by the crawler, mapping directly to the documented functionality.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the html attribute contains the raw, unmodified HTML content from the crawled webpage, which matches the ground truth's explanation",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler (via 'from crawl4ai import AsyncWebCrawler') and uses it within an async context manager. This clearly demonstrates the instantiation and usage of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports async context manager usage through the import and 'async with' syntax shown in the documentation, which aligns with the ground truth's explanation of the __aenter__ and __aexit__ implementation enabling this functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the async context, the snippet calls the 'arun' method on the AsyncWebCrawler instance (using 'await crawler.arun(url=...)'). This invocation directly maps to the AsyncWebCrawler.arun() artifact.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic async call relationship but misses crucial aspects of what arun() actually implements (content extraction, caching, error handling) that are core to enabling the quickstart functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After obtaining the result from the 'arun' method, the snippet accesses the 'markdown' attribute (via 'print(result.markdown)'). This attribute is part of the CrawlResult class's public interface, corresponding to the CrawlResult.markdown artifact.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the markdown attribute/property of CrawlResult stores and provides access to the extracted content, which can be printed out",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler', which demonstrates its role in managing advanced session-based crawling with dynamic content.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in session-based crawling, but misses the crucial aspect of how it handles dynamic content through custom JavaScript injection and wait conditions for pagination",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls 'await crawler.arun(...)' to perform the crawl operation on dynamic pages. This demonstrates the use of the 'arun' method that belongs to the AsyncWebCrawler class.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun as an async method used for crawling, but misses the crucial aspect of session-based dynamic crawling with JavaScript execution and content update capabilities that is central to the ground truth description.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The extraction strategy is explicitly instantiated as 'extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)' to extract commit details using a defined schema. This indicates its role in processing the crawled HTML content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic role of JsonCssExtractionStrategy in extracting data using a schema, but misses the crucial implementation detail about how it processes the baseSelector and fields to extract commit information from GitHub's dynamic pages",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet implicitly uses the kill_session() method by calling 'await crawler.crawler_strategy.kill_session(session_id)'. Since the 'crawler_strategy' attribute of AsyncWebCrawler defaults to an instance of AsyncPlaywrightCrawlerStrategy, this call maps to the kill_session() method of that class.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session() is called through the crawler strategy, but misses the core purpose described in the ground truth about cleaning up browser resources by closing page and context objects",
      "error_type": "incomplete_main_functionality"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet explicitly shows the usage of 'result.fit_markdown' to obtain article content, which directly maps to the 'fit_markdown' attribute of the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown is used for article content, which aligns with the ground truth's explanation of it being a property for blog posts and articles formatting",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example code filters images using 'result.media[\"images\"]', directly referencing the 'media' attribute of the CrawlResult class, which holds media details such as images.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the media dictionary property allows filtering of media elements (specifically images) based on properties like relevance scores, with the predicted focusing on the core access pattern and the ground truth adding context about the purpose",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation explicitly shows the extraction of content links using 'result.links[\"internal\"]'. This correspondingly maps to the 'links' attribute in the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture the relationship between the links dictionary structure and filtering internal content links from it, with the predicted description correctly identifying the core functionality shown in the documentation example.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet shows a call to 'crawler.arun(...)' with various parameters. This is a usage example of the 'arun' method from the AsyncWebCrawler class, mapping directly to the AsyncWebCrawler.arun() artifact.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method but only describes it as a method call, missing its key implementation of the documented best practices and functionality for content cleaning, media processing, and link extraction mentioned in the ground truth",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler using the 'async with AsyncWebCrawler(...) as crawler:' statement. This direct reference indicates that the AsyncWebCrawler class is used to set up the crawling operation while simulating user interactions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for web crawling, but misses the key point about how user simulation is implemented through specific parameters. It focuses on instantiation rather than the functionality described in the ground truth.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the 'arun()' method is directly invoked on the AsyncWebCrawler instance. This method call, accompanied by parameters like simulate_user=True and override_navigator=True, underscores its role in initiating a crawling process that simulates real user behavior.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the arun() method's role in initiating crawling, it treats simulate_user and override_navigator as direct parameters, whereas the ground truth specifies these are passed through **kwargs. The core user simulation functionality is captured but the implementation mechanism is misunderstood.",
      "error_type": "parameter_implementation_misunderstanding"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation clearly demonstrates resource management by calling 'await crawler.crawler_strategy.kill_session(session_id)'. This example explicitly instructs users to clean up sessions by invoking the kill_session() method, which is implemented in the AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions focus on how kill_session() implements resource cleanup by closing and removing sessions. The predicted description accurately captures the core relationship between the documentation's resource management best practice and the implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet demonstrates state management practices by calling 'result = await crawler.arun(...)' with a session_id parameter and additional JavaScript instructions. This directly maps to the AsyncWebCrawler.arun() method, which handles crawling with session context.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the arun() method handles session state management through session_id parameter to enable stateful crawling across requests. The predicted description accurately reflects this functionality even if it provides slightly different examples.",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The documentation snippet defines a JSON-based schema for extracting nested objects and lists (using keys such as 'type', 'fields', 'selector'). This extraction schema aligns with the contract specified by the ExtractionStrategy abstract base class, even though the snippet does not explicitly mention the class name. It underpins the design of extraction strategies across the system.",
    "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the relationship between the JSON schema format and the ExtractionStrategy class's purpose of converting HTML into structured data, even though it approaches it from a slightly different angle than the ground truth",
      "error_type": ""
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The JSON examples in the documentation illustrate how to define nested objects, simple lists, and nested lists using keys like 'type' and 'fields'. The JsonCssExtractionStrategy class directly implements this extraction logic by processing a schema that leverages CSS selectors and field definitions. Although not explicitly named in the documentation snippet, its functionality is directly implied.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the JsonCssExtractionStrategy class and the JSON schema documentation - both describe a system for extracting structured data using CSS selectors and nested field definitions",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an asynchronous call 'await crawler.arun(...)' which clearly invokes the AsyncWebCrawler.arun() method to perform the crawl and extraction.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the async execution aspect but misses the core focus on error handling that the ground truth emphasizes. The ground truth specifically describes the comprehensive error handling and result states, which is a crucial aspect of the relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "Within the example, the code checks 'if result.success:' to determine whether extraction was successful, directly referencing the 'success' attribute of the crawl result.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the error handling code to determine if content extraction succeeded before attempting to process the results.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the success boolean property is used to check if extraction was successful before proceeding with processing results",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet prints an error message using 'result.error_message' when the extraction fails, showing direct use of this attribute.",
    "ground_truth_relationship": "The error_message field in CrawlResult is used to store failure details when content extraction fails, as shown in the error handling documentation where it's accessed via result.error_message to provide user feedback.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of error_message in error handling, but misses the broader context that it's a storage field for failure details in CrawlResult",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example processes the extraction output by calling 'json.loads(result.extracted_content)', directly using the extracted_content attribute of the crawl result.",
    "ground_truth_relationship": "The optional extracted_content field in CrawlResult stores the semantically-matched content found by the crawler using strategies like Cosine matching, which gets parsed as JSON if extraction succeeds.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the JSON parsing of extracted_content but misses the crucial context that it's an optional field used for storing semantically-matched content from strategies like Cosine matching",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'The Cosine Strategy' as effective for scenarios requiring semantic understanding and handling inconsistent content structure, making it a recommended extraction strategy.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key functionality of CosineStrategy for semantic-based extraction and handling inconsistent content, which aligns with the ground truth implementation of cosine similarity and hierarchical clustering methods",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the usage code, it forms the foundation upon which the CosineStrategy is built.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is a subclass of ExtractionStrategy and inherits its foundation. While it doesn't detail all the capabilities mentioned in the ground truth, it captures the core inheritance relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun' with the 'js_code' parameter, demonstrating the use of this method to execute custom JavaScript before crawling.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used to execute JavaScript, but misses that this happens via kwargs being passed through to crawler_strategy.crawl() rather than directly through a js_code parameter",
      "error_type": "parameter_confusion"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within the implementation of the crawl method in AsyncPlaywrightCrawlerStrategy, the 'js_code' parameter is processed (using page.evaluate) to execute custom JavaScript. This underlies the functionality demonstrated by the documentation snippet.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation - that JavaScript execution is handled through the crawl method using page.evaluate(), supporting both single and multiple commands as shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract crawl interface requiring custom JavaScript execution implementations. This base class is a necessary part of the inheritance chain that supports the js_code functionality.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify AsyncCrawlerStrategy as the abstract base class that provides core crawling functionality supporting JavaScript execution during web scraping, with the predicted description focusing on inheritance and the ground truth focusing on method implementation - but the core relationship is consistently described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions 'SlidingWindowChunking' and provides a usage example showing how to import and instantiate it with parameters 'window_size' and 'step'. This directly demonstrates its functionality.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on superficial aspects like importing and instantiating the class, while missing the core algorithmic relationship of how sliding windows create overlapping chunks that preserve context",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not mentioned directly in the documentation snippet, 'SlidingWindowChunking' extends 'ChunkingStrategy'. This implicit trace indicates that 'SlidingWindowChunking' inherits its core chunking contract from 'ChunkingStrategy'.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between SlidingWindowChunking and ChunkingStrategy, and captures the core concept that SlidingWindowChunking implements the chunking contract defined by ChunkingStrategy",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (async with AsyncWebCrawler(verbose=True) as crawler) to begin the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager functionality but misses the core relationship about integrated JavaScript execution and content extraction described in the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform the crawl with parameters such as URL, session_id, css_selector, extraction_strategy, js_code, and js_only.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the method and some parameters correctly, but misses the core functionality of integrated JavaScript execution and waiting that is central to the ground truth's description. It doesn't capture the key purpose of handling pagination and content loading.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of JsonCssExtractionStrategy with a defined schema to extract commit information from the crawled page.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship - that JsonCssExtractionStrategy uses a schema to extract structured data (commits) from HTML content. While more concise than the ground truth, it captures the essential functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, meaning that the extraction functionality used in the example is built upon the ExtractionStrategy interface.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing the commit extraction functionality shown in the documentation through its extract method, which processes HTML content into structured data as demonstrated in the documentation's schema-based extraction example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that ExtractionStrategy is involved in the extraction implementation, it only mentions inheritance without capturing the key role of the extract method and HTML processing functionality described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example explicitly calls the kill_session method on the crawler's strategy (crawler.crawler_strategy.kill_session(session_id)) to properly close the session after crawling.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that kill_session is used to clean up and close browser resources at the end of the crawling process. The predicted description focuses on the method call while the ground truth elaborates on the implementation details, but they convey the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawling strategy when no custom strategy is provided. Although not directly mentioned in the snippet, it underpins the kill_session method call.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the integrated JavaScript execution and waiting functionality through its csp_compliant_wait method, which wraps user-provided JavaScript in a polling loop that checks for conditions while respecting timeouts.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on AsyncWebCrawler using AsyncPlaywrightCrawlerStrategy as a default strategy, while the ground truth explains the specific implementation of JavaScript execution and waiting functionality through the csp_compliant_wait method. These are completely different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy, thereby inheriting its crawling interface which is essential for the overall crawling process configured by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like set_hook() that enable the integrated JavaScript execution and waiting functionality described in the documentation's advanced technique.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes AsyncCrawlerStrategy as a base class for crawler implementations, but misses the key point about its role in providing hook-setting and other methods that enable integrated JavaScript execution and waiting functionality",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly shows the asynchronous instantiation of AsyncWebCrawler using the 'async with' context manager and passing a 'proxy' parameter. This demonstrates its usage for basic proxy configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler can be instantiated with proxy configuration through the async with context manager, which aligns with the ground truth's explanation of proxy support through the crawler_strategy parameter. The core functionality is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on the AsyncWebCrawler instance (via 'result = await crawler.arun(url=\"https://example.com\")'), making this method an explicit part of the public interface used for crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is the key public interface method used for crawling functionality in AsyncWebCrawler, which aligns with the ground truth's description of it implementing the core crawling functionality. While the ground truth provides more detail about proxy configuration handling, the predicted captures the essential relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the snippet, AsyncWebCrawler internally defaults to using AsyncPlaywrightCrawlerStrategy when no crawler_strategy is provided. The provided 'proxy' parameter is passed through to this strategy to configure the crawling session with the appropriate proxy settings.",
    "ground_truth_relationship": "The code implements proxy support by accepting proxy URLs in both HTTP and SOCKS formats through the 'proxy' parameter in the AsyncPlaywrightCrawlerStrategy class, which is then configured in the browser launch arguments using Playwright's ProxySettings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies proxy support but incorrectly states AsyncWebCrawler defaults to AsyncPlaywrightCrawlerStrategy, which is not shown in the code. The core proxy functionality is correctly described though.",
      "error_type": "assumption_about_defaults"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates how to instantiate AsyncWebCrawler with various 'browser_type' values ('chromium', 'firefox', 'webkit'). This indicates that AsyncWebCrawler is the primary class used for browser configuration in Crawl4AI.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncWebCrawler is used for browser configuration and supports different browser types (chromium, firefox, webkit), which aligns with the ground truth's explanation of browser type selection through the crawler_strategy parameter.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example in the documentation explicitly calls the 'arun()' method on an instance of AsyncWebCrawler to perform crawling. This links the browser configuration to the actual web crawling functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses the key relationship about browser_type parameter determining the engine selection, which is central to the documented configuration functionality.",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly mentions and demonstrates the use of the NlpSentenceChunking class by showing its instantiation and the invocation of its 'chunk' method to split a sample text into sentences.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main relationship - that NlpSentenceChunking is used to split text into sentences. While it focuses more on demonstrating usage rather than implementation details, it conveys the same core functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, NlpSentenceChunking extends the abstract base class ChunkingStrategy, implying that it follows the standard interface for text chunking. This inheritance relationship is implicitly part of its design.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that NlpSentenceChunking extends ChunkingStrategy and follows its interface, which is the core relationship outlined in the ground truth. While the ground truth provides more implementation details, the fundamental inheritance relationship is accurately captured.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy with parameters such as provider, api_token, schema, and instruction. This clearly demonstrates its use for LLM\u2010based structured data extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies LLMExtractionStrategy's basic functionality and key parameters, but misses crucial aspects about its internal complexity including chunking, rate limiting, and parallel processing that are highlighted in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is derived from the base class ExtractionStrategy. This relationship is implicit in the usage example as the extraction strategy relies on the interface defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of the base class providing core infrastructure and parallel processing capabilities through its abstract extract() and run() methods.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'crawler.arun(...)' to perform the crawling process while passing the previously instantiated LLMExtractionStrategy. This method is responsible for executing the extraction strategy and returning the result.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that arun() executes the extraction strategy with crawling, which aligns with the ground truth's explanation of AsyncWebCrawler.arun() implementing structured data extraction through the extraction_strategy parameter.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the code accesses 'result.extracted_content' to retrieve the structured data. This attribute is defined in the CrawlResult model and holds the output from the extraction strategy.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that extracted_content stores structured data output from the extraction process as a result of crawling. The predicted description accurately captures the main relationship and purpose of the extracted_content field.",
      "error_type": ""
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly names RegexChunking and provides a usage example that imports and instantiates this class. This directly maps to the RegexChunking class implemented in the code base.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted describes the class name and import correctly, it misses the core functional relationship of how RegexChunking iteratively splits text using patterns, which is the main focus of the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the documentation snippet, RegexChunking extends ChunkingStrategy. This base class defines the abstract interface for chunking, making it an implicit part of the functionality described.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that ChunkingStrategy is an abstract base class that RegexChunking extends/implements, with the chunk() method being the key interface. The predicted description captures this core relationship accurately, even if less detailed.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation example shows a direct call to 'crawler.arun(...)' with parameters such as url and extraction_strategy, demonstrating how to initiate the crawling and extraction process.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the key relationship around error handling and best practices implementation that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The example code uses 'result.success', 'result.error_message', and 'result.extracted_content', indicating that the returned CrawlResult object is central to handling and verifying the extraction output.",
    "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how the CrawlResult class's fields (success, error_message, extracted_content) support the error handling pattern shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "One of the best practice recommendations is to 'Use LLM for complex interpretation', which directly corresponds to the LLMExtractionStrategy class that implements extraction using a language model.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted relationship focuses on LLM usage for complex interpretation, while the ground truth describes error handling implementation. These are completely different aspects of the code.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation advises to 'Try Cosine for content relevance'. The CosineStrategy class is designed to implement extraction based on cosine similarity and content relevance.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the CosineStrategy class implements content relevance filtering using cosine similarity, which aligns with the documented best practice. While the predicted description is more concise, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation suggests to 'Start with CSS for structured data'. This guidance maps directly onto the JsonCssExtractionStrategy class, which leverages CSS selectors for extracting content from HTML.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the JsonCssExtractionStrategy implements the CSS-based extraction approach recommended in the documentation for structured data extraction using selectors",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "All the extraction strategy classes mentioned (LLMExtractionStrategy, CosineStrategy, JsonCssExtractionStrategy) inherit from the ExtractionStrategy base class, forming the underlying framework for extraction.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class implements the documented 'Choose the Right Strategy' section by providing a flexible framework where different extraction methods (CSS, LLM, Cosine) can be implemented through the abstract extract() method while sharing common parallel processing functionality in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between extraction strategies and the base class, but misses the crucial point about how this implements the documented strategy selection and shared parallel processing functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation snippet explicitly names 'FixedLengthWordChunking' and demonstrates its instantiation and usage in the provided code example. It describes how the class splits text into fixed-length word chunks, which directly reflects the functionality of the 'FixedLengthWordChunking' class.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of splitting text into fixed-length word chunks, which aligns with the ground truth's explanation of the implementation using word tokens and chunk_size parameter",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although the documentation does not explicitly mention 'ChunkingStrategy', the 'FixedLengthWordChunking' class extends 'ChunkingStrategy'. This base class provides the abstract interface for chunking strategies, making it an essential part of the overall functionality documented.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the inheritance relationship between ChunkingStrategy as an abstract base class and FixedLengthWordChunking as its implementation, which matches the ground truth's description of their relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the documentation snippet, the variable 'crawler' is used to invoke the arun() method. Although the snippet does not explicitly name AsyncWebCrawler, it is implied that the crawler instance is of type AsyncWebCrawler which is responsible for initiating the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class processes webpage metadata through its aprocess_html method which extracts metadata into a dictionary stored in the CrawlResult object, exactly matching the documented metadata fields like title, description, and language that can be accessed as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role in crawling but misses its core metadata extraction functionality that is central to the ground truth. While it correctly connects crawler.arun() to AsyncWebCrawler, it fails to mention the metadata processing capabilities that are the focus of the documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet directly calls the 'arun' method (via 'crawler.arun(...)'). This method is responsible for initiating the crawl and returning the crawl results.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic functionality of arun() as initiating crawls and returning results, but misses the crucial metadata extraction and processing aspect emphasized in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'arun' method returns a crawl result object. Although the snippet only refers to it as 'result', this object is an instance of CrawlResult, which aggregates all crawl output including metadata.",
    "ground_truth_relationship": "The metadata extraction documented in the example is implemented through the metadata field in the CrawlResult class which stores the extracted page metadata as an optional dictionary.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the crawl result object (CrawlResult) contains the metadata, matching the ground truth's explanation that metadata is stored as an optional dictionary field in CrawlResult.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "The documentation snippet explicitly accesses 'result.metadata' to retrieve metadata such as title, description, keywords, author, and dates. This attribute is defined as part of the CrawlResult class.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that metadata is an attribute accessible via result.metadata that contains various page metadata fields, which aligns with the ground truth's description of it being a property storing extracted metadata as an optional dictionary",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation explicitly imports and instantiates RegexChunking as the chunking strategy. It shows the user how to pass a regex pattern (\"\\n\\n\") to split text, directly linking the docs to the RegexChunking class implementation.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship - RegexChunking class splits text using regex patterns, with the documentation showing proper usage by passing '\\n\\n' pattern. Minor implementation details aside, it conveys the same core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The example creates an instance of AsyncWebCrawler (with verbose=True) and uses it as the main crawler object. This shows that the crawler class is central to the crawling process described in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main crawler class, but misses the key relationship with RegexChunking strategy which is the main focus of the ground truth. The documentation specifically highlights RegexChunking functionality, not just general crawler usage.",
      "error_type": "missing_key_relationship"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet explicitly calls the arun() method on the AsyncWebCrawler instance to perform the crawl. This invocation uses the RegexChunking strategy passed as a parameter to process the webpage content.",
    "ground_truth_relationship": "The code implements the RegexChunking strategy mentioned in the documentation by accepting a chunking_strategy parameter in the arun() method, which defaults to RegexChunking() and validates it against the ChunkingStrategy type.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship of implementing RegexChunking strategy in the arun() method, with the predicted focusing on usage and ground truth on implementation details. The differences are minor and complementary rather than contradictory.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation prints a slice of the extracted_content attribute from the crawl result, indicating that this attribute\u2014part of the CrawlResult model\u2014is used to display processed content.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the text content after it has been processed by the RegexChunking strategy which splits the content using regex patterns.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies extracted_content as storing processed content from the crawl result, but misses the crucial aspect that RegexChunking specifically splits the content using regex patterns",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "RegexChunking is a concrete implementation that inherits from ChunkingStrategy, defining the contract for chunking text. Although not directly mentioned in the snippet, its inclusion as the base class is inherent in the design.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ChunkingStrategy is an abstract base class that RegexChunking inherits from/extends to implement text chunking functionality",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(...)' with parameters such as page_timeout, delay_before_return_html, and wait_for. This indicates that the AsyncWebCrawler.arun() method (artifact id 5) is explicitly invoked.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() being called with timing parameters, but incorrectly suggests direct parameter usage rather than recognizing they are passed through **kwargs to crawler_strategy.crawl()",
      "error_type": "implementation_misunderstanding"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, the parameters page_timeout, delay_before_return_html, and wait_for are processed within the 'crawl' method of AsyncPlaywrightCrawlerStrategy. This strategy (artifact id 1) is used by AsyncWebCrawler.arun() to perform the actual crawl, thereby handling the waiting and timeout behavior demonstrated in the documentation.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles the timeout and waiting behavior through its crawl method, which aligns with the ground truth's explanation of how the code implements these features. While the predicted version is less detailed, it captures the core relationship between the code and documented functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy (artifact id 0), inheriting its interface for crawling. Although the snippet does not reference this base class directly, it is inherent in the overall design and affects how crawling is configured.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawling, but it focuses on a hypothetical AsyncPlaywrightCrawlerStrategy implementation rather than the core timeout/waiting capabilities emphasized in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an explicit instantiation of AsyncWebCrawler with custom security headers via the 'headers' keyword argument. This directly demonstrates the use of the AsyncWebCrawler class to configure HTTP headers.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that AsyncWebCrawler supports custom header configuration through instantiation, even though it doesn't mention the specific implementation detail of passing through kwargs to AsyncPlaywrightCrawlerStrategy",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet directly calls the 'arun()' method on the AsyncWebCrawler instance to crawl the provided URL. This method is a primary interface for triggering the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is the main interface for crawling, but misses the crucial aspect about custom header support which is the main focus of the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within AsyncWebCrawler's constructor, if no custom crawler_strategy is provided, it defaults to creating an instance of AsyncPlaywrightCrawlerStrategy. The custom headers are passed along to this internal strategy, implicating its role in handling the header configuration.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom header functionality through its set_custom_headers method and context.set_extra_http_headers, allowing headers like X-Forwarded-For and Cache-Control to be set as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions the role of custom headers in AsyncPlaywrightCrawlerStrategy, but incorrectly suggests this happens through AsyncWebCrawler's constructor defaulting to creating a strategy instance. The ground truth shows that header handling is implemented directly in AsyncPlaywrightCrawlerStrategy through specific methods.",
      "error_type": "implementation_details_mismatch"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This abstract base class defines the core crawling operations that are eventually used, meaning the header handling indirectly relies on the contracts specified by AsyncCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines core crawler methods that enable custom header manipulation shown in the documentation through its crawl and update_user_agent methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the core relationship - that AsyncCrawlerStrategy is an abstract base class defining essential crawler operations that enable header/request customization functionality. The predicted text captures this fundamental architectural relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates using CosineStrategy by passing parameters such as sim_threshold, word_count_threshold, and top_k. The provided code examples (e.g., 'strategy = CosineStrategy(sim_threshold=0.8)') directly refer to this class.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the documentation demonstrates CosineStrategy usage with parameters, but misses the crucial point that these parameters are directly implemented in specific class methods that carry out their described functionalities.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is implemented as a subclass of ExtractionStrategy. Although the base class is not directly mentioned in the documentation snippet, it forms part of the chain since it defines the abstract interface for extraction strategies that CosineStrategy implements.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship of ExtractionStrategy being a base/abstract class that defines the interface for implementing extraction strategies with various configuration parameters. While the predicted description focuses more on CosineStrategy as an example implementation, it correctly identifies the inheritance relationship and purpose of the base class.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet provides a usage example that calls 'crawler.arun' with a parameter named 'html2text'. This explicitly demonstrates the invocation of the AsyncWebCrawler.arun() method, which is responsible for processing the crawl request and converting HTML content to markdown. The example directly correlates to this method, as it accepts additional keyword arguments (such as html2text) for content customization.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that the arun() method accepts html2text options via kwargs for customizing content processing. Both descriptions convey that these options are passed through the method for HTML-to-markdown conversion.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun(...)' with anti-detection parameters. This explicitly maps to the 'AsyncWebCrawler.arun()' method which accepts and forwards parameters like simulate_user, override_navigator, and magic.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() accepts and handles anti-detection parameters that are passed through to the crawler implementation, which aligns with the ground truth's explanation of how stealth-related arguments flow through kwargs to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly referenced in the snippet, the parameters simulate_user, override_navigator, and magic are processed in the 'crawl' method of AsyncPlaywrightCrawlerStrategy. This strategy is used by AsyncWebCrawler.arun(), thereby implicitly supporting the anti-detection features shown in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that anti-detection features exist and are handled in the crawl method, but misses the key implementation detail about using context.add_init_script() to inject JavaScript for overriding navigator properties. The mechanism of implementation is a crucial aspect of the relationship.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This base class establishes the abstract interface for crawling, making it an integral part of the chain where anti-detection features are implemented in the derived class.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing anti-detection features through methods like update_user_agent and set_hook, which enable the stealth capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class but focuses on a general inheritance relationship rather than its specific role in defining anti-detection features through its abstract methods",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the 'async with AsyncWebCrawler(verbose=True) as crawler' statement. This shows that the class is directly used to set up a crawler that caches crawl results.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage and caching capability, but fails to mention the key implementation details about async_db_manager and bypass_cache parameter that are central to how the caching functionality works",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the 'arun' method of the AsyncWebCrawler instance is called to perform the crawl ('result1 = await crawler.arun(url=...)'). This demonstrates explicit use of the arun() method to initiate and control a web crawl.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the arun() method being called for web crawling, while missing the core caching functionality described in the ground truth. However, it correctly identifies that arun() is used for web crawling.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet prints a portion of the crawl result by accessing 'result1.markdown', indicating that the CrawlResult object returned by the crawl includes a 'markdown' attribute which holds the extracted markdown content.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the markdown attribute/property of the CrawlResult object contains the crawled webpage content in markdown format and is used to display a portion of the crawl results.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet clearly shows a usage example calling 'crawler.arun()' with parameters such as 'exclude_external_links', 'exclude_social_media_links', etc. This explicitly demonstrates how the 'AsyncWebCrawler.arun()' method is used to implement smart link filtering.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for link filtering, but misses the key implementation detail that it works through **kwargs being passed to crawler_strategy.crawl(). The prediction implies direct parameter handling which isn't quite accurate.",
      "error_type": "implementation_mechanism_mismatch"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows the AsyncWebCrawler class being instantiated with the 'async with AsyncWebCrawler() as crawler:' statement to maintain session state. This directly demonstrates its intended use for session management.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler's usage with async context manager but focuses too much on that aspect while missing the key relationship about session_id parameter being used to maintain state between requests, which is the main point of the ground truth.",
      "error_type": "missing_key_concept"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method on the AsyncWebCrawler instance (e.g., result1 = await crawler.arun(...)) to perform crawling tasks while preserving the session state via session_id.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship of using arun() for crawling while maintaining session state through session_id, which aligns with the ground truth's explanation of session management implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet calls 'await crawler.crawler_strategy.kill_session(session_id)' to clean up the session, which directly maps to the kill_session() method implemented in AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that kill_session() implements the cleanup functionality shown in the documentation example, noting the direct mapping between the documented usage and implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an 'async with AsyncWebCrawler(headless=False) as crawler:' statement. This explicitly instantiates the AsyncWebCrawler class, demonstrating its intended use for initiating a crawl session.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's basic instantiation pattern but misses the core functionality about overlay removal that is the focus of the ground truth. While it's not wrong, it omits the key relationship regarding overlay handling.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, after instantiating AsyncWebCrawler, the 'arun()' method is called with parameters such as 'remove_overlay_elements=True' and 'screenshot=True'. This direct invocation indicates that AsyncWebCrawler.arun() is the critical method for performing the crawl and handling content fitting and overlay removal.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship that the arun() method is responsible for handling overlays and content processing with the specified parameters. While it omits some details about how the parameters are processed, it correctly identifies the key functionality and method responsible.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when no custom strategy is supplied, uses AsyncPlaywrightCrawlerStrategy by default. Inside its 'crawl' method, the flag 'remove_overlay_elements' triggers a call to its 'remove_overlay_elements' method. This behavior is implicit in the example as the flag is passed via arun() and handled by the underlying crawler strategy.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles overlay removal via the remove_overlay_elements flag, even though it doesn't detail the specific JavaScript implementation. The core relationship and functionality are accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a usage example that explicitly calls 'crawler.arun(url=\"https://example.com\")'. This indicates that the 'AsyncWebCrawler.arun()' method is being used to perform the crawl and return results for further analysis.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic role of arun() as performing crawls, but misses crucial aspects about its link analysis capabilities and processing of internal/external links that are core to the relationship described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The code example immediately accesses 'result.links' to analyze internal and external links. This shows that the 'links' attribute in the CrawlResult class is a key part of the library's link analysis functionality.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that links are stored in CrawlResult.links and used for analysis, but misses crucial details about the dictionary structure with categorized link types and the specific link details stored (href, text, context) that the ground truth describes",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions 'Crawl4AI's `AsyncWebCrawler` class' as supporting session-based crawling. This class is designed to maintain a persistent browser session via the 'session_id' parameter, directly aligning with the documentation's focus on session-based crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the same core relationship - that AsyncWebCrawler implements session-based crawling functionality through the session_id parameter, allowing persistent browser sessions across multiple requests. The predicted description captures this key functionality accurately, even if it doesn't elaborate on all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation advises using the same 'session_id' across multiple 'arun' calls to maintain session state. This directly relates to the implementation in AsyncWebCrawler.arun(), where the session_id is retrieved from kwargs and preserved across calls.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions describe the same core relationship - that the arun() method handles session-based crawling by working with a session_id parameter that gets passed through kwargs and stored in CrawlResult, allowing state to persist across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While the documentation does not explicitly name this class, AsyncPlaywrightCrawlerStrategy is the default crawling strategy used by AsyncWebCrawler. It manages browser sessions and handles the session_id parameter, which underpins session-based crawling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles session-based crawling functionality through session management, which aligns with the ground truth's explanation of how the class maintains browser contexts and pages across requests via its sessions dictionary.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and creates an instance of AsyncWebCrawler using an async context manager. This class is the entry point for the session-based crawling demonstrated in the example.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class and mentions its instantiation via async context manager, but misses the crucial aspect of session management and specific methods (arun, kill_session) that enable persistent browser sessions and user interactions described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the loop in the example, the 'arun()' method is explicitly invoked on the AsyncWebCrawler instance to perform the crawl. Parameters like session_id, js_code, css_selector, and bypass_cache are used to control crawling behavior.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the arun() method handles session-based crawling through passed parameters. While the predicted description focuses more on the parameter usage and the ground truth emphasizes the session state maintenance, they describe the same core functionality without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After retrieving the crawl result via 'arun()', the code snippet accesses the 'extracted_content' attribute to count occurrences of '.content-item'. This attribute is part of the CrawlResult model representing extracted content.",
    "ground_truth_relationship": "The extracted_content property stores the HTML content matching the '.content-item' CSS selector as shown in the example where it's used to count found items after each crawl.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that extracted_content stores HTML content matching the specified CSS selector from crawl results and is used for counting matched items. The predicted description covers the core functionality without any major misunderstandings.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "At the end of the crawling process, the example explicitly calls 'kill_session' on the crawler's strategy (which defaults to an instance of AsyncPlaywrightCrawlerStrategy) using the session_id to properly close the session.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core purpose of kill_session as a cleanup method that properly closes/terminates a crawler session, which aligns with step 4 in the documentation. The predicted description correctly identifies that it's called explicitly at the end of crawling using the session_id.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates JsonCssExtractionStrategy with a CSS-based schema. This shows that the class is used for data extraction via CSS selectors, matching the documented description.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy uses CSS selectors for data extraction via a schema, which aligns with the ground truth's explanation of using BeautifulSoup's select() method with the schema's baseSelector",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy as part of its design. Although not directly mentioned in the snippet, this base class establishes the extraction framework that JsonCssExtractionStrategy implements.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy and captures the core inheritance relationship, even though it doesn't detail all the implementation aspects mentioned in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The provided code snippet calls 'crawler.arun(...)' to perform the crawling and extraction process. This method (AsyncWebCrawler.arun()) is used to trigger the extraction strategy, thereby indirectly illustrating how the extraction is integrated into the crawling workflow.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core execution logic that enables the JsonCssExtractionStrategy to process web pages using CSS selectors by accepting an extraction_strategy parameter and integrating it into the crawling workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() is used to execute the extraction strategy as part of the crawling workflow, which aligns with the ground truth's explanation of how arun() implements the core logic for integrating extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates calling 'crawler.arun(url=\"https://example.com\")'. This shows that the AsyncWebCrawler.arun() method is used to initiate a crawl and obtain a CrawlResult object.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the arun() method is used for crawling, but misses the crucial aspect of markdown content extraction which is the main focus of the documentation. It focuses only on the method call rather than its core functionality.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet uses 'print(result.fit_markdown)', which directly accesses the 'fit_markdown' attribute. This attribute holds the main content converted to markdown, matching the documentation's purpose.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown is an attribute holding main content in markdown format, matching the ground truth's core meaning about extracted and cleaned content storage",
      "error_type": "none"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates how to extract specific content by calling the 'arun' method on a crawler object and passing a 'css_selector' parameter. This shows direct usage of AsyncWebCrawler.arun() as part of the public interface.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CSS selector-based content extraction through the arun method, matching the ground truth's explanation of how CSS selectors are used to filter and extract specific elements from crawled webpages.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates a call to 'crawler.arun()'. This invocation directly maps to the 'AsyncWebCrawler.arun()' method, which is responsible for performing the crawl and returning the crawl result.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its crawling purpose, but misses the crucial markdown conversion functionality that is central to the ground truth relationship",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After obtaining the crawl result from 'AsyncWebCrawler.arun()', the example prints 'result.markdown'. This directly maps to the 'markdown' attribute of the 'CrawlResult' object, which holds the clean markdown content of the crawled HTML.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that markdown is an attribute of the CrawlResult that contains the converted markdown content from HTML after crawling. Both descriptions convey the same core relationship and functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet directly instantiates CosineStrategy with parameters such as semantic_filter, word_count_threshold, top_k, sim_threshold, and max_dist for different use cases like Article Content Extraction, Product Review Analysis, and Technical Documentation. This explicit creation demonstrates its role as the extraction strategy.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that CosineStrategy is configured through parameters for different text extraction use cases. Both descriptions emphasize how semantic_filter, word_count_threshold, top_k and other parameters are adjusted for specific scenarios like article content, reviews, and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning that the extraction contract defined in ExtractionStrategy underpins the functioning of CosineStrategy. Although ExtractionStrategy is not directly instantiated in the snippet, its role is inherent to the design.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the various content extraction strategies shown in the documentation's use cases, including article content, product reviews, and technical documentation extraction through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy extends ExtractionStrategy, but misses the crucial aspect of ExtractionStrategy's role in providing foundational structure for different content extraction use cases and its parallel processing capability",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet includes a call to 'await crawler.arun(...)', explicitly invoking the AsyncWebCrawler.arun() method. This call integrates the chosen extraction strategy (the instantiated CosineStrategy) into the crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic arun() method usage with extraction strategy, but misses the key flexibility aspects described in the ground truth regarding how the parameters control content filtering, thresholds and similarity settings across different use cases.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun()', which maps directly to the 'AsyncWebCrawler.arun()' method implementation. This asynchronous method is used to crawl a URL and return a CrawlResult, making it the pivotal point for error handling as demonstrated in the snippet.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method, but focuses on the method implementation rather than the core error handling relationship shown in the documentation",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "After calling 'crawler.arun()', the result is stored in a variable and its attributes (success, error_message, status_code) are checked. The CrawlResult class (artifact_id 8) defines these attributes and encapsulates the response information from the crawl operation.",
    "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures how the CrawlResult class provides attributes for error handling (success, error_message, status_code) that are used in the documentation example. It accurately describes the relationship between the class definition and its usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet specifically tests 'result.success' to determine if the crawl was successful. This attribute is part of the CrawlResult public interface and indicates the overall outcome of the crawling process.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the success boolean is used to check if a crawl operation completed successfully and enables error handling, with the predicted description correctly identifying it as part of the CrawlResult interface used for determining crawl outcomes",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "When the crawl fails, the snippet prints 'result.error_message'. This attribute provides details about the error encountered and is a designated part of the CrawlResult data model.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that error_message is used to store/display error details when a crawl fails, with the predicted description capturing the core relationship in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The snippet prints 'result.status_code' to show the HTTP status code of the crawled page. This attribute is part of the CrawlResult public interface and is essential in determining the nature of any errors that occurred during crawling.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that status_code is used to store HTTP status codes from crawl responses and is used for error handling/diagnostics of failed crawls.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly refers to 'JsonCssExtractionStrategy' as the method for extracting structured data using CSS selectors. This directly corresponds to the class with artifact_id 23, which implements the extraction strategy using a schema that defines a base selector and field selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that JsonCssExtractionStrategy uses CSS selectors through a schema definition to extract structured data from HTML, with a base selector for repeating elements and field selectors for specific data points",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation instructs users on how to use the 'JsonCssExtractionStrategy' with the AsyncWebCrawler. This indicates that AsyncWebCrawler integrates or leverages the extraction strategy, making its inclusion explicitly relevant.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler integrates with JsonCssExtractionStrategy, which aligns with the ground truth showing the implementation checks for and executes this strategy in the aprocess_html method.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "While 'ExtractionStrategy' is not directly mentioned in the text, 'JsonCssExtractionStrategy' (artifact_id 23) is built on top of it by extending this abstract class. This inheritance relationship is critical to understanding how the extraction functionality is structured in Crawl4AI.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses describing the core functionality (abstract extract method and parallel processing) that the ground truth emphasizes",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy with parameters such as 'provider', 'schema', and 'instruction'. The code sample demonstrates how LLMExtractionStrategy is used to extract structured data from web content.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic setup and parameters of LLMExtractionStrategy, but misses the crucial functionality of processing HTML through language models and extracting structured data according to the schema/instructions",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the ExtractionStrategy abstract base class. Although the documentation does not mention ExtractionStrategy explicitly, it is an underlying dependency since LLMExtractionStrategy inherits its extraction interface and functionality.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, and acknowledges that ExtractionStrategy provides the base functionality. While it's less detailed than the ground truth, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet contains the line 'async with AsyncWebCrawler(verbose=True) as crawler:' which explicitly instantiates the AsyncWebCrawler class. This shows that the class is directly used to set up the asynchronous crawling context.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description only states that AsyncWebCrawler is used as a context manager, while the ground truth describes a specific functional capability regarding the wait_for parameter in the arun method. These are completely different aspects of the class's functionality.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the method 'arun' of the AsyncWebCrawler is invoked as seen in the call 'result = await crawler.arun(...)'. This call demonstrates how the wait_for parameter and other options are passed to initiate a crawl.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that 'arun' accepts parameters including wait_for, it fails to explain how the method actually implements and uses the wait_for functionality through crawler_strategy.crawl()",
      "error_type": "incomplete_implementation_details"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The sample code calls 'await crawler.crawler_strategy.kill_session(session_id)' to close the active session. This explicit call to the kill_session method is used to terminate the session after the crawling is complete.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core purpose of the kill_session method - terminating/closing an active session. While it doesn't mention all the implementation details about closing page and context objects, it accurately describes the high-level relationship and purpose shown in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly initializes the extraction strategy with 'extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)'. This demonstrates the use of a JSON/CSS-based extraction strategy to process the crawled HTML content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy is used for extraction, but misses the crucial aspect of how it implements schema-based parsing using BeautifulSoup to extract data according to the specified selectors. The prediction focuses only on initialization rather than the actual functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly invoked in the example, the 'wait_for' parameter supplied to the arun method is processed internally by the crawl method of AsyncPlaywrightCrawlerStrategy. This underlying class handles waiting for dynamic content via its smart_wait functionality, integrating seamlessly with the wait_for parameter provided in the sample.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how the wait_for parameter is processed through the smart_wait functionality in AsyncPlaywrightCrawlerStrategy to handle dynamic content loading, which aligns with the ground truth's explanation of the relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation header 'Tips for Using JsonCssExtractionStrategy' explicitly names this class. This artifact implements an extraction strategy using CSS selectors (via BeautifulSoup's select method) to parse HTML based on a provided schema. The document\u2019s tips about inspecting and testing selectors directly relate to how this class operates.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the class implements CSS-based extraction using BeautifulSoup selectors, and both emphasize how the documentation's tips about CSS selector inspection and testing align with the class's functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The tip 'Always check the result.success flag' explicitly mentions a flag that informs users about the success of an extraction/crawl. This flag corresponds to the 'success' attribute defined within the CrawlResult model, ensuring that error handling is based on the outcome of a crawl.",
    "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions describe the same core relationship - the success boolean flag is used for error handling and checking operation completion status",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly demonstrates how to create an instance of AsyncWebCrawler, passing parameters such as headless, verbose, and sleep_on_close. This shows its role in basic browser configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly explains that AsyncWebCrawler accepts basic browser configuration parameters like headless, verbose, and sleep_on_close, which aligns with the ground truth's explanation that these parameters are passed through kwargs to the AsyncPlaywrightCrawlerStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method on the AsyncWebCrawler object to perform the crawling operation. This confirms the usage of the arun() method as the interface for initiating a crawl.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies the arun() method as the interface for crawling, but misses the crucial configuration aspect described in the ground truth about how the crawler accepts and handles various configuration parameters through its constructor",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler with a 'proxy_config' parameter to use an authenticated proxy.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts proxy_config for authenticated proxy usage, but misses that this parameter is passed to the underlying AsyncPlaywrightCrawlerStrategy, which is a crucial part of how the functionality works.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform crawling on a specified URL.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic functionality of calling arun() for crawling, but misses the key relationship about proxy support implementation described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, AsyncWebCrawler internally instantiates AsyncPlaywrightCrawlerStrategy when no custom crawler_strategy is provided. The provided 'proxy_config' is passed as a keyword argument to configure proxy settings.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes proxy configuration exists but incorrectly states it's handled by AsyncWebCrawler class passing proxy_config to AsyncPlaywrightCrawlerStrategy. The ground truth correctly explains that proxy authentication is implemented directly in the start() method using ProxySettings.",
      "error_type": "incorrect_implementation_details"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This base class defines the common interface for asynchronous crawling, which is utilized indirectly when handling proxy configuration.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables proxy-authenticated web crawling through its abstract crawl methods, which the documentation demonstrates how to configure using proxy_config.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that AsyncCrawlerStrategy defines the interface for proxy-authenticated web crawling functionality, with the predicted description focusing on the class hierarchy and the ground truth emphasizing the proxy configuration aspect. The core relationship and purpose are aligned.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly advises using a 'CSS Strategy' when dealing with well-structured HTML. JsonCssExtractionStrategy implements content extraction using CSS selectors, making it the direct match for this recommendation.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that JsonCssExtractionStrategy implements the CSS Strategy recommended for well-structured HTML parsing using CSS selectors",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly recommends the 'LLM Strategy' for scenarios with natural language text. LLMExtractionStrategy uses language model capabilities for extraction, thereby aligning directly with this guidance.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy implements the LLM Strategy option and is used for natural language text processing, which aligns with the core relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation suggests using a 'Cosine Strategy' for mixed or complex content to achieve greater content relevance. CosineStrategy leverages cosine similarity for clustering and extraction, matching the documented requirement.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is designed for mixed/complex content and uses cosine similarity for clustering and extraction, which aligns with the ground truth's description of the implementation details and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler (via 'async with AsyncWebCrawler(verbose=True) as crawler') to start the crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic instantiation of AsyncWebCrawler but misses the core relationship about custom execution hooks being implemented through the crawler_strategy attribute. While it correctly notes the usage, it fails to capture the main purpose and mechanism described in the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the 'arun()' method is directly called on the AsyncWebCrawler instance to perform the crawling operation.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling but misses the crucial aspect of how it enables custom hooks through the crawler_strategy object, which is a key part of the ground truth relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The code snippet explicitly calls 'crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)' to attach a custom hook for monitoring new content.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality - using set_hook to attach custom hooks for monitoring content - which aligns with the ground truth's explanation of storing hook callbacks for execution during crawling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "As shown in the example, the session is terminated via 'await crawler.crawler_strategy.kill_session(session_id)', directly invoking this method.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the kill_session method being called but misses the crucial aspect of its purpose - cleaning up browser resources by closing page and context objects",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the snippet, AsyncWebCrawler defaults to using an instance of AsyncPlaywrightCrawlerStrategy as its crawler_strategy, which provides functionalities like set_hook() and kill_session().",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncPlaywrightCrawlerStrategy's role with hooks, but incorrectly suggests it's only used as a default strategy for AsyncWebCrawler rather than being the actual class implementing the hook functionality.",
      "error_type": "minor_misattribution"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, thereby inheriting the abstract interface (including hook management) used in the crawling process.",
    "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy is an abstract interface with hook functionality, but incorrectly focuses on inheritance by AsyncPlaywrightCrawlerStrategy rather than explaining the set_hook method's role in enabling custom hook functionality for crawling stages.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet includes a direct usage example of CosineStrategy, showing its configuration parameters such as 'semantic_filter', 'word_count_threshold', 'sim_threshold', 'max_dist', 'linkage_method', 'top_k', 'model_name', and 'verbose'. These parameters match the constructor signature in the CosineStrategy class.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that the CosineStrategy class implements all the documented configuration parameters with matching names and default values in its constructor.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, meaning that the extraction behavior defined in ExtractionStrategy underpins CosineStrategy. Although not explicitly mentioned in the snippet, this base class relationship is an implicit dependency that defines the extraction framework upon which CosineStrategy is built.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core inheritance relationship between ExtractionStrategy and CosineStrategy, and acknowledge the role of configuration parameters passed through kwargs. The predicted description correctly identifies the base class relationship and framework dependency.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation text explicitly mentions 'advanced features of JsonCssExtractionStrategy', demonstrating its application in extracting structured data (such as product details, reviews, and related items) from a complex HTML layout.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy is used for extracting structured data from complex HTML layouts using selectors, which aligns with the ground truth's explanation of using BeautifulSoup to parse and extract data based on CSS selectors and schema mapping.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the text, 'JsonCssExtractionStrategy' extends 'ExtractionStrategy'. This makes 'ExtractionStrategy' an implicit part of the extraction process, providing the abstract interface that the concrete strategy implements.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on JsonCssExtractionStrategy inheriting from ExtractionStrategy, while the ground truth correctly describes ExtractionStrategy's core purpose as an abstract base class for HTML data extraction. While not contradictory, the predicted text misses the main purpose and functionality.",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls the method 'arun()' on the crawler object (i.e. 'crawler.arun(...)'). This explicitly demonstrates usage of the AsyncWebCrawler.arun() method for handling dynamic content and lazy-loaded images.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() method usage but fails to capture its key implementation of dynamic content handling through kwargs parameters, focusing instead just on method invocation pattern",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the snippet, the parameters 'wait_for', 'process_iframes', 'js_code', and 'delay_before_return_html' passed to 'arun()' are handled internally by the crawl() method of AsyncPlaywrightCrawlerStrategy. This strategy is used by AsyncWebCrawler to implement dynamic content handling, iframe processing, and lazy-load support.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that AsyncPlaywrightCrawlerStrategy implements dynamic content handling through methods that support JavaScript execution, iframe processing, and configurable delays. While it mentions specific parameters being handled by crawl(), this aligns with the ground truth's description of the implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates calling 'crawler.arun()' with the parameters 'screenshot=True' and 'screenshot_wait_for=2.0'. This indicates that the arun() method of the AsyncWebCrawler is being used to initiate the crawl process with screenshot capture enabled.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of arun() method handling screenshot capabilities through a boolean parameter and storing/returning screenshot data. The predicted description focuses on usage while ground truth describes implementation, but they align on the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "In the code sample the result is checked by 'if result.screenshot:', directly accessing the 'screenshot' attribute of the CrawlResult model. This attribute holds the Base64 encoded image returned after crawling.",
    "ground_truth_relationship": "The screenshot property in CrawlResult stores the captured webpage screenshot as a base64-encoded string that can be decoded into a PNG image file.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the screenshot attribute stores a base64-encoded image from the crawl result, which matches the ground truth's explanation of the core relationship",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the snippet, when 'screenshot=True' is passed to arun(), the underlying implementation in AsyncPlaywrightCrawlerStrategy is invoked to perform the actual screenshot capture via its 'take_screenshot' method. This enhanced error-handling implementation is key to the screenshot capability described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that the AsyncPlaywrightCrawlerStrategy handles screenshot capture through its take_screenshot method when screenshot=True is passed, and includes error handling. While it's slightly less detailed than the ground truth, it doesn't contradict or misunderstand any major aspects.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy defines the abstract 'take_screenshot' method which is concretely implemented by AsyncPlaywrightCrawlerStrategy. This establishes the interface that underpins the screenshot capture functionality.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy defines the abstract take_screenshot method that enables screenshot functionality, which aligns with the ground truth's explanation of the screenshot capability implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows usage of CosineStrategy by instantiating it with parameters such as word_count_threshold, top_k, verbose, semantic_filter, sim_threshold, and max_dist. This clearly demonstrates how CosineStrategy is applied to optimize performance and handle mixed content types as per the best practices.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions emphasize how CosineStrategy implements configurable parameters to handle different content types and optimize performance. The predicted description accurately captures the relationship between the documentation's recommended practices and the class implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy, making ExtractionStrategy an essential base of CosineStrategy. Although ExtractionStrategy is not directly mentioned in the documentation snippet, it is implicitly involved since CosineStrategy implements its abstract extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on inheritance relationship with CosineStrategy, while ground truth describes ExtractionStrategy's role in implementing configurable parameters. Both describe architectural aspects but emphasize different relationships.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and instantiates it with a schema for pattern\u2010based extraction after user interaction. This demonstrates its direct use for structured extraction.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy is used for pattern-based extraction with a schema, which aligns with the ground truth's explanation of how it uses BeautifulSoup and CSS selectors for structured data extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet expressly creates an instance of LLMExtractionStrategy with parameters such as provider, schema, and instruction, demonstrating its role in analyzing dynamic content using an LLM.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as a tool for analyzing dynamic content using LLM models with configurable parameters, which aligns with the ground truth's explanation of its implementation and usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both JsonCssExtractionStrategy and LLMExtractionStrategy inherit from ExtractionStrategy. Although not explicitly mentioned in the snippet, this base class is fundamental to the extraction strategies used, thereby forming an implicit part of the trace.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core inheritance relationship between ExtractionStrategy and its derived classes (JsonCssExtractionStrategy and LLMExtractionStrategy). While the ground truth provides more detail about the specific methods, the predicted text correctly identifies the fundamental inheritance relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the code sample, the method 'arun' is invoked on a 'crawler' object to execute crawling and dynamic content extraction. This indicates direct usage of AsyncWebCrawler.arun() to combine page interactions with extraction strategies.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of using arun() for crawling and extraction, but misses the key functionality of combining page interactions (js_code, wait conditions) with structured extraction strategies that is central to the ground truth relationship",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with custom parameters (user_agent and headers). This demonstrates how the crawler\u2019s public interface is used to control its identity, as seen in the 'async with AsyncWebCrawler(...)' usage example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that AsyncWebCrawler allows customization of identity through user agent and headers parameters. The predicted description focuses on the instantiation interface while the ground truth mentions the implementation details, but they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the same snippet, the .arun() method is explicitly called on the AsyncWebCrawler instance to start the crawling process. This usage shows how the crawler's functionality is executed via its public method.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic existence of the arun() method but misses the core identity management functionality (user_agent and header customization) that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:', indicating explicit usage of this class as the primary crawling engine.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage pattern with async/await, but misses the crucial aspect that it supports multiple extraction strategies and output formats for processing HTML content, which is a key part of its functionality according to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the arun() method on the AsyncWebCrawler instance to perform crawling tasks, as seen in 'result = await crawler.arun(...)', which directly invokes this method.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the key aspect that it supports different extraction strategies, which is central to the relationship shown in the ground truth between the implementation and its usage",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "Within the snippet, LLMExtractionStrategy is instantiated with parameters like provider, schema, and instruction to extract structured data using a language model, thereby specifying the extraction strategy.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of LLMExtractionStrategy being used to extract structured data using language models with configurable parameters, which aligns with how it's demonstrated in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet creates an instance of JsonCssExtractionStrategy (i.e. 'extraction_strategy=JsonCssExtractionStrategy(your_schema)') to extract repeated pattern data, indicating explicit usage of this strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for pattern extraction, but misses the crucial implementation detail that it uses CSS selectors to systematically extract structured data from repeated HTML elements",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet accesses the 'fit_markdown' attribute (result.fit_markdown) on the crawl result, demonstrating its role in providing the extracted main content in a formatted markdown output.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is used to store and access the main extracted content from a webpage, and both show it being used in the return dictionary's main_content field",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The documentation snippet retrieves structured and pattern data via 'llm_result.extracted_content' and 'pattern_result.extracted_content', signaling the explicit access of the extracted content attribute from the CrawlResult object.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the key relationship that extracted_content stores crawled data as a string that gets parsed as JSON for both LLM and pattern-based extraction strategies. The predicted description accurately notes the usage of extracted_content attribute from crawl results.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet accesses 'result.media' to retrieve media information as part of the crawl output, which directly corresponds to the media attribute in the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that media content is retrieved from the CrawlResult object and returned as part of the crawl_content function's output in the 'media' field",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates the AsyncWebCrawler by using 'async with AsyncWebCrawler(verbose=True) as crawler:', thereby showing direct usage of this class for initiating the screenshot operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the usage of AsyncWebCrawler for screenshot functionality but focuses only on the instantiation pattern rather than explaining the core screenshot capability through the arun method and its return format as described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example directly calls the arun() method on the AsyncWebCrawler instance with parameters such as 'screenshot=True' and 'bypass_cache=True', demonstrating its role in performing the crawling task that includes screenshot functionality.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() handles screenshots but misses that it specifically handles caching the screenshot data and returning it in CrawlResult for base64 decoding/saving, which is a key part of the functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Although not mentioned by name in the snippet, the returned object from the arun() call is used by checking its 'success' and 'screenshot' properties. These properties are part of the CrawlResult model, implying its use in processing and storing the screenshot data.",
    "ground_truth_relationship": "The CrawlResult class includes a 'screenshot' field that stores base64-encoded image data, which enables the documented screenshot capture functionality to store and return webpage snapshots.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult handles screenshot data through its properties, which aligns with the ground truth's explanation of the screenshot field storing base64-encoded image data.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The documentation snippet demonstrates usage of a 'result' object from which the 'media' attribute is accessed. This implies that the 'result' is an instance of the CrawlResult class that encapsulates the media information along with other crawling results.",
    "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that CrawlResult class contains media information in its media dictionary field that can be accessed to get video and audio metadata. The predicted description is more general but accurately describes the relationship between the class and its media attribute.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The provided code example explicitly accesses 'result.media' with keys 'videos' and 'audios'. This indicates that the media attribute of the CrawlResult class stores metadata for video and audio elements.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that media is a dictionary that stores video and audio metadata in lists that can be accessed and processed. The predicted description captures the core relationship even if slightly less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit call to 'crawler.arun()' with parameters 'simulate_user=True' and 'override_navigator=True'. This directly maps to the 'AsyncWebCrawler.arun()' method which is responsible for initiating the crawl process using the provided anti-detection options.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that crawler.arun() accepts anti-detection parameters and maps to the AsyncWebCrawler.arun() method, which aligns with the ground truth's explanation of how the method accepts keyword arguments for anti-bot features",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within the implementation of the crawl functionality, the parameters 'simulate_user', 'override_navigator', and 'magic' are checked. When any of these are true, the 'AsyncPlaywrightCrawlerStrategy' injects scripts to override navigator properties and simulate human behavior. Although not directly visible in the usage snippet, these options trigger anti-bot measures in this class.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the code implements anti-bot measures through parameters like simulate_user and override_navigator, which inject scripts to mask automation and simulate human behavior. The predicted description captures the same core functionality and relationship as the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates a call to the 'arun' method on the crawler (using 'await crawler.arun(url=\"https://example.com\")'). This shows that the method is being used to perform the crawl operation.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling, but misses the crucial functionality of media selection and structured access to different media types (images, videos, audios) that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'arun' method returns a CrawlResult object which encapsulates the crawl details. Although the class name 'CrawlResult' is not directly mentioned in the snippet, its presence is implied via the usage of 'result' which holds properties like 'media'.",
    "ground_truth_relationship": "The CrawlResult class implements media selection through its media dictionary field that stores different media types (images, videos, audios) as documented in the usage example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the CrawlResult class and its role in containing crawl results, but misses the key focus of the ground truth which is specifically about the media selection functionality through the media dictionary field.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation snippet explicitly accesses 'result.media' to retrieve media details (images, videos, audios). This directly correlates with the 'media' attribute defined on the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship between the media dictionary and its usage in accessing different media types (images, videos, audios) through result.media, matching the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (i.e., 'async with AsyncWebCrawler() as crawler:'), clearly indicating its role as the entry point for crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler's role as a context manager entry point for crawling, but misses the core functionality around complex page interactions, JavaScript execution, and session management described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun(...)' to perform page crawling, passing parameters such as 'js_code' and 'wait_for', which shows direct usage of the arun() method from AsyncWebCrawler.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the core functionality of arun() for page crawling but misses crucial aspects about handling dynamic content, session management, and complex interactions that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The example accesses the 'cleaned_html' property on the resulting CrawlResult (i.e., 'len(result.cleaned_html)'). This demonstrates that the cleaned_html attribute is part of the public interface used to process the crawl output.",
    "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that cleaned_html is accessed as a property of the result object, but misses the key aspect that it stores the processed HTML content after dynamic page loads to track loaded items.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet makes an explicit call to 'crawler.crawler_strategy.kill_session(session_id)' to clean up the session, thereby directly invoking the kill_session() method of AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is called to clean up the session, but misses the crucial detail about closing page and context objects to prevent memory leaks during multi-page crawling",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation text explicitly mentions 'JsonCssExtractionStrategy' and details the schema definition including keys like 'name', 'baseSelector', and 'fields'. This directly corresponds to the class JsonCssExtractionStrategy, which accepts a schema and uses its 'baseSelector' and 'fields' to extract data from HTML.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship between the code and documentation, explaining how the JsonCssExtractionStrategy class uses the schema structure (baseSelector and fields) to extract data from HTML using BeautifulSoup.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends the ExtractionStrategy class. Although the documentation does not directly mention ExtractionStrategy, its functionality is implicitly based on the abstract extraction interface provided by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy, but misses the crucial aspect of the shared parallel processing capabilities through the run() method that is central to the ground truth description.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler via 'async with AsyncWebCrawler(verbose=True) as crawler', directly demonstrating its usage.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports async context manager usage, showing the same pattern described in the ground truth with __aenter__ and __aexit__ implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows example invocations of the crawler using the 'arun()' method with the 'wait_for' parameter (e.g. wait_for='css:.dynamic-content' and wait_for='js:...'). This directly maps to the 'AsyncWebCrawler.arun()' method as shown in the example usage.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method supports wait conditions through wait_for parameters, showing examples of both CSS and JavaScript waiting functionality that match the ground truth's description. The core relationship between the code and documentation is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' class provides the concrete implementation of the crawl logic used by the 'arun()' method. Within its 'crawl' method, it processes the 'wait_for' parameter by checking for 'css:' and 'js:' prefixes and then calling its internal waiting methods. This underpins the wait conditions functionality described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that AsyncPlaywrightCrawlerStrategy implements wait conditions through its internal methods, handling both CSS and JavaScript-based waiting. The predicted description gets the core functionality right, just using slightly different wording.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncCrawlerStrategy' abstract class defines the contract for crawl operations\u2014including handling of wait conditions\u2014that is implemented by 'AsyncPlaywrightCrawlerStrategy'. Its inclusion in the chain highlights the inheritance relationship that supports the dynamic waiting functionality.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy is an abstract base class that enables wait functionality through its crawl method, with the predicted description accurately reflecting this through inheritance and contract concepts",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of AsyncWebCrawler with proxy-related parameters (either 'proxy' or 'proxy_config') to enhance access. This directly maps to the class AsyncWebCrawler that accepts these parameters in its constructor.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler handles proxy configuration, but incorrectly suggests that 'proxy' and 'proxy_config' are direct parameters of AsyncWebCrawler. The ground truth clarifies that proxy settings are passed through **kwargs and handled by AsyncPlaywrightCrawlerStrategy.",
      "error_type": "parameter_handling_misunderstanding"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the provided examples, the arun() method is invoked on the AsyncWebCrawler instance to perform crawling on the specified URL while employing the provided proxy configuration.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling URLs with the AsyncWebCrawler, but misses the crucial aspect that the proxy configuration is handled through the crawler_strategy.crawl() method rather than directly by arun()",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when instantiated without a custom strategy, defaults to using AsyncPlaywrightCrawlerStrategy. This strategy reads the 'proxy' and 'proxy_config' parameters from the kwargs to configure browser settings appropriately, hence indirectly supporting proxy configuration.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles proxies, but incorrectly suggests this happens indirectly through AsyncWebCrawler rather than directly within the strategy class itself. The strategy class actually implements the proxy functionality directly through ProxySettings objects.",
      "error_type": "misattributed_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. Although this base class is not directly mentioned in the documentation snippet, it is part of the inheritance chain that defines the crawling interface, including proxy configuration handling.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawling, but incorrectly mentions AsyncPlaywrightCrawlerStrategy which isn't referenced in the code or documentation. It also misses describing the core proxy configuration functionality shown in the ground truth.",
      "error_type": "incorrect_class_reference_and_missing_core_functionality"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet contains a direct invocation of 'crawler.arun(...)' with parameters such as word_count_threshold, which clearly maps to the AsyncWebCrawler.arun() method. This usage is explicit since the method is directly called in the example.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method and its parameter usage but focuses only on the direct method invocation aspect rather than explaining its core content filtering functionality described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "While not explicitly mentioned in the code snippet, the variable 'crawler' used in the example is an instance of the AsyncWebCrawler class. This implies that AsyncWebCrawler provides the context and interface for the 'arun()' method call.",
    "ground_truth_relationship": "The documentation describes filtering options that are directly implemented as parameters in the AsyncWebCrawler.arun() method, where word_count_threshold controls minimum block size and kwargs handles exclude_external_links, exclude_external_images, and excluded_tags options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies AsyncWebCrawler's role in providing arun() functionality, but misses the core relationship being described - the direct connection between the documented filtering parameters and their implementation in the arun() method.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates CosineStrategy with parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, top_k) to perform similarity\u2010based clustering and content extraction. This artifact is directly referenced in the code example.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the key relationship between the code and documentation, noting that CosineStrategy is instantiated with the same core parameters and performs similarity-based clustering as shown in both artifacts. The minor differences in wording do not affect the fundamental understanding.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the abstract base class for extraction strategies is inherent in the design, making it a necessary intermediate in the implementation chain.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, and acknowledges the base class's role in the design. While it doesn't detail the parallel processing capabilities mentioned in the ground truth, it captures the core architectural relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example uses 'crawler.arun(...)' to perform the crawling operation with the supplied CosineStrategy instance. This indicates that the AsyncWebCrawler.arun() method is invoked to integrate the extraction strategy into the crawling process.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that crawler.arun() accepts and integrates the extraction strategy (including CosineStrategy) into the crawling process, which aligns with the ground truth's description of how arun() implements crawling logic that can use CosineStrategy for content extraction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses AsyncWebCrawler (via 'from crawl4ai import AsyncWebCrawler') and employs it as an asynchronous context manager ('async with AsyncWebCrawler(verbose=True) as crawler') to perform a web crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for web crawling functionality and is used through an async context manager pattern, which aligns with the ground truth's description of how it implements the core crawling functionality for the cryptocurrency price extraction example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly imports JsonCssExtractionStrategy from 'crawl4ai.extraction_strategy' and creates an instance with a defined schema to extract cryptocurrency prices, which defines selectors and fields for structured data extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of JsonCssExtractionStrategy as a class that uses schema-defined selectors to extract structured cryptocurrency data, which aligns with the ground truth's explanation",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "While not directly imported by name, the example calls the 'arun()' method on the AsyncWebCrawler instance to perform the crawl and retrieve the extracted content, thereby linking the crawler's operation to the extraction result.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that arun() is called on AsyncWebCrawler to perform crawling and extraction. While it's less detailed than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, which defines the common interface for extraction strategies. This inheritance is critical as it guarantees that JsonCssExtractionStrategy adheres to the expected extraction contract.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the core relationship that ExtractionStrategy is a base class that defines an interface/contract that concrete strategies like JsonCssExtractionStrategy inherit and implement. While the predicted focuses more on inheritance and the ground truth provides more implementation details about extract() and run(), they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager (i.e., 'async with AsyncWebCrawler(verbose=True) as crawler:'). This demonstrates the creation and use of the crawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text focuses on the instantiation pattern using async context manager, while the ground truth describes the error handling functionality. While both relate to AsyncWebCrawler, they describe different core aspects of its functionality.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the provided code snippet, the arun() method is directly called on the AsyncWebCrawler instance to perform the crawl and extraction operations. This method call is central to retrieving the crawl result.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the key method but misses the crucial error handling aspect that is central to the ground truth description. The error handling functionality is a major focus of both the code and documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of LLMExtractionStrategy with parameters such as provider, api_token, and instruction. This strategy is used as the extraction strategy in the crawl process.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the LLMExtractionStrategy class but focuses only on initialization aspects, missing the core error handling and parallel processing functionality described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly instantiated in the snippet, its functionality underpins the LLMExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational retry-compatible extraction interface that the documentation's error handling system wraps with the @retry decorator to implement resilient LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the inheritance relationship between ExtractionStrategy and LLMExtractionStrategy, but misses the crucial aspect of the retry-compatible interface that enables error handling described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After calling the arun() method, the resulting object's 'extracted_content' attribute is accessed and processed using json.loads(). This explicit use shows that the extraction result is structured and expected to contain JSON data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content contains data that gets processed as JSON, but misses the key aspect about it being text data from web pages that can be retried on extraction failure, which is a significant part of the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates CosineStrategy with parameters such as 'linkage_method', 'max_dist', and 'model_name' to demonstrate custom clustering functionality.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the custom clustering aspect with parameters but omits the crucial content filtering pipeline functionality that's a major part of the class's implementation according to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy. Although the snippet does not mention ExtractionStrategy directly, its use of CosineStrategy implies that its behavior is built upon the contract defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy extends ExtractionStrategy and captures the inheritance relationship. While it's more concise than the ground truth, it conveys the same core relationship without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet shows 'async with AsyncWebCrawler() as crawler', directly demonstrating the usage of the AsyncWebCrawler class to manage crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and its async context management, but misses crucial aspects about its role in content filtering, extraction strategies, and clustering support that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun(...)', which triggers the asynchronous operation to perform crawling and content extraction. This method is a core part of AsyncWebCrawler's public interface.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as an asynchronous crawling method, but misses significant aspects covered in the ground truth including custom clustering configurations, content filtering, caching, and the full extraction pipeline functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "Post crawling, the snippet accesses 'result.extracted_content' to retrieve the extracted clusters, directly demonstrating the use of this public attribute.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the code accesses extracted_content, but misses the crucial aspect that it contains JSON-serialized clustering and filtering results with specific pricing features, similarity scores and cluster information",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The example checks 'if result.success:' to determine whether the crawl was successful, illustrating the use of the 'success' property in controlling the flow after crawling.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that success is a boolean property used to validate whether the crawl/extraction operation completed successfully before proceeding with result processing",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'await crawler.arun(...)', which explicitly corresponds to the AsyncWebCrawler.arun() method. This method is responsible for crawling a URL while accepting an extraction_strategy parameter.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts an extraction_strategy parameter and is used for crawling URLs, which aligns with the ground truth's explanation of how different strategies can be combined through this method",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet comments '# First use CSS strategy for initial structure' and the parameter 'extraction_strategy=css_strategy' imply that a CSS-based extraction strategy is used. In the available artifacts, this functionality is implemented by the JsonCssExtractionStrategy class.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements CSS-based extraction functionality as part of the documented strategy combination approach",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation\u2019s comment '# Then use LLM for semantic analysis' together with the parameter 'extraction_strategy=llm_strategy' imply that an LLM-based extraction strategy is being applied. This corresponds to the LLMExtractionStrategy class in the available artifacts.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the LLMExtractionStrategy class corresponds to the documented LLM-based extraction functionality for semantic analysis. While it doesn't detail all implementation aspects, it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the creation of an AsyncWebCrawler instance using 'async with AsyncWebCrawler() as crawler:'. This indicates that the AsyncWebCrawler class is the main entry point for the functionality being demonstrated.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is the main class used in the async context manager pattern shown in the documentation. While it doesn't mention all implementation details, it captures the core relationship between the documentation example and the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet calls 'crawler.arun(url=url)' to perform the actual crawling, which directly maps to the 'arun()' method provided by the AsyncWebCrawler class.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that crawler.arun() is used for performing crawling operations and maps to the provided arun() method implementation. While it omits some implementation details like proxy support, it captures the core relationship between the documentation example and code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although 'update_proxy' is not explicitly defined in the documentation snippet, AsyncWebCrawler internally instantiates AsyncPlaywrightCrawlerStrategy (if no alternative strategy is provided). This strategy handles proxy configurations (via its initialization parameters such as 'proxy' and 'proxy_config'), which is relevant to proxy rotation even though the method call 'update_proxy' is only illustrated in the example.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately explain that the AsyncPlaywrightCrawlerStrategy handles proxy configuration for web crawling. The predicted description refers to proxy handling via initialization parameters, while the ground truth specifies it's implemented through the start() method - but both convey the same core functionality of proxy support.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly names the 'Cosine Strategy' and describes its functionality of converting text into vector representations, calculating similarity between chunks, clustering similar content, and filtering based on relevance. This directly maps to the CosineStrategy class, which implements these features by using cosine similarity and clustering techniques.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the high-level relationship between the documentation and code, correctly identifying the core functionality of text vectorization, similarity calculation, and content clustering. While the ground truth provides more implementation details about specific methods, the predicted description correctly captures the main workflow and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is built upon the ExtractionStrategy abstract base class. Although the documentation does not directly mention ExtractionStrategy, it is an integral part of CosineStrategy's design, providing a common interface for extraction strategies in Crawl4AI.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy inherits from ExtractionStrategy, but misses the crucial point about how ExtractionStrategy provides the interface for content extraction and parallel processing functionality through specific methods.",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates 'AsyncWebCrawler' with 'async with AsyncWebCrawler() as crawler:', indicating its direct usage for crawling tasks.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies AsyncWebCrawler's usage in an async context, it fails to acknowledge the class's core functionality of supporting both structured and LLM-based content extraction methods shown in the documentation example.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code explicitly calls 'await crawler.arun(...)' to perform the crawling and content extraction, thereby exercising this method.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling, but misses crucial aspects about its role in handling different extraction strategies and returning structured results for both pattern and LLM-based extraction",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "Within the code, 'JsonCssExtractionStrategy' is directly instantiated with 'article_schema' to perform structured content extraction from the page.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of JsonCssExtractionStrategy with a schema, but misses the crucial aspect of how it implements structured extraction using CSS selectors with baseSelector and fields properties to extract data in a standardized way",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet creates an instance of 'LLMExtractionStrategy' with specific parameters (provider, schema, instruction) for carrying out a semantic analysis of the article content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of LLMExtractionStrategy as a class that handles semantic analysis with configurable parameters, which aligns with how it's used in the documentation example for article analysis",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The code uses 'pattern_result.extracted_content' and 'analysis_result.extracted_content' in json.loads(), implying that the 'extracted_content' attribute (of the CrawlResult returned by arun()) is an important part of the output interface.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that extracted_content is used to store JSON content from the crawl results, as shown in the example where both pattern_result.extracted_content and analysis_result.extracted_content are used with json.loads()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The snippet returns 'pattern_result.media' as part of the combined results, indicating that the 'media' attribute of the CrawlResult is used to access media-related information from the crawled page.",
    "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the 'media' property/attribute from pattern_result is used to store and return media-related information from the crawled page as part of the combined results in the function.",
      "error_type": ""
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation text explicitly mentions the 'TopicSegmentationChunking' class in both the header and the example usage code. It shows an import statement from 'crawl4ai.chunking_strategy' and instantiates the class with the 'num_keywords' parameter, directly linking the documentation to this artifact.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on surface-level class identification and parameter usage, while missing the core functionality of TextTiling algorithm and keyword extraction that the ground truth emphasizes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "The 'TopicSegmentationChunking' class extends the abstract 'ChunkingStrategy' class. Although not directly mentioned in the documentation text, this inheritance relationship is an implicit dependency that forms part of the functionality described.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between TopicSegmentationChunking and ChunkingStrategy abstract class, which matches the ground truth's description of the interface implementation relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(...)', which directly maps to the AsyncWebCrawler.arun() method. This method is responsible for crawling a URL with various cleaning parameters such as word count threshold and overlay removal.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions align on the core relationship that arun() method implements content cleaning functionality through parameters like word_count_threshold to filter and clean crawled content. The predicted description correctly identifies the basic relationship between the documentation and code even if it omits some implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet uses 'print(result.cleaned_html)' to display the cleaned HTML. This explicitly references the cleaned_html attribute defined in the CrawlResult class.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that cleaned_html exists and is used for displaying cleaned HTML, but misses the crucial aspects of it being an Optional[str] that stores sanitized HTML after multiple cleaning steps",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet prints 'result.markdown' to obtain the markdown version of the cleaned content. This directly corresponds to the markdown attribute in the CrawlResult class.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic relationship that markdown provides a clean version of content, but misses the crucial aspect of noise/ad removal and content cleaning that is central to the ground truth explanation",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the documentation does not explicitly mention 'AsyncWebCrawler' by name, the variable 'crawler' used in the example is conventionally an instance of this class, which provides the arun() method.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented content cleaning through its arun method, which uses word_count_threshold and excluded elements (defined in WebScrappingStrategy) to remove noise and preserve meaningful content as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class provides the arun() method, but misses explaining how it actually implements the content cleaning functionality described in the documentation. It focuses only on class identification rather than explaining the cleaning relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The example passes the parameter 'remove_overlay_elements=True', which is handled by the underlying crawling strategy. AsyncPlaywrightCrawlerStrategy implements logic (via its remove_overlay_elements method) to remove popups and overlays, thus supporting content cleaning.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the remove_overlay_elements parameter is handled by AsyncPlaywrightCrawlerStrategy to remove popups and overlays, which aligns with the ground truth's description of the content cleaning functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates JsonCssExtractionStrategy with a defined JSON schema for extracting repeated content patterns, such as news articles.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of using JsonCssExtractionStrategy with a schema for content extraction, but misses the crucial aspect of recursive/nested extraction capability and the fundamental pattern-based nature of the extraction process",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun()' to perform the crawl using the extraction strategy. Although the AsyncWebCrawler class is not explicitly named in the code snippet, its method 'arun()' is implicitly used to initiate the crawling process with the provided extraction strategy.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() is used to execute the crawl with an extraction strategy, which aligns with the ground truth's explanation that arun() processes the extraction strategy to extract structured data from crawled content. While the ground truth provides more implementation details, the core relationship is accurately represented.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends the abstract base class ExtractionStrategy. Although not directly mentioned in the snippet, this inheritance relationship is crucial because it defines the interface that JsonCssExtractionStrategy implements.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship but misses the crucial aspect of parallel processing and pattern-based content extraction functionality that the base class provides.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:'. This indicates that the class is directly used to create a crawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for creating crawler instances, but misses the crucial functionality aspects described in the ground truth - particularly the customizable extraction strategies and asynchronous web crawling capabilities that enable the tech content extraction shown in the example.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the 'arun()' method is called on the AsyncWebCrawler instance (i.e. 'result = await crawler.arun(...)'). This call directs the crawling process and triggers the extraction of content.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling, but misses the crucial aspect of how it works with LLMExtractionStrategy for content filtering and the broader implementation details around caching and extraction strategies",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of LLMExtractionStrategy with parameters including 'provider', 'api_token', and an 'instruction', to extract technology-related content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of LLMExtractionStrategy as demonstrated in the example - it creates an instance with provider, api_token, and instruction parameters to extract specific content. While it's more concise than the ground truth, it doesn't contradict or misunderstand the relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is defined as a subclass of ExtractionStrategy. Though not directly mentioned in the snippet, ExtractionStrategy forms part of the underlying inheritance chain for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and LLMExtractionStrategy, which aligns with the ground truth's explanation of how ExtractionStrategy provides the core framework for LLMExtractionStrategy's content extraction functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl process, the example directly accesses the 'extracted_content' attribute of the resulting CrawlResult object (i.e. 'json.loads(result.extracted_content)'). This attribute is used to retrieve the extracted technology content.",
    "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that extracted_content is used to store filtered/extracted content (specifically tech-related content in the example) that can be parsed as JSON. The predicted description accurately captures this core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun()' with keyword parameters including 'page_timeout' and 'delay_before_return_html'. These parameters control timing, and their appearance in the sample directly maps to the 'AsyncWebCrawler.arun()' method implementation.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts timing-related parameters through its interface, which matches the ground truth's explanation that these parameters are handled via **kwargs and passed to crawler_strategy.crawl(). While the predicted description is less detailed, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly referenced in the snippet, the 'AsyncWebCrawler.arun()' method internally calls the crawl method of the default crawler strategy. 'AsyncPlaywrightCrawlerStrategy' processes the 'page_timeout' and 'delay_before_return_html' parameters to control delays and timeouts, thereby supporting the behavior documented.",
    "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncPlaywrightCrawlerStrategy implements timing control through page_timeout and delay_before_return_html parameters, and accurately explains their relationship to the crawling process.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun()' with filtering parameters. This maps explicitly to the 'AsyncWebCrawler.arun()' method, which is responsible for initiating a crawl with extended keyword arguments.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes that arun() is the main crawling method and handles additional parameters, but misses the key point about domain filtering functionality which is the main focus of the documentation",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet only shows 'crawler.arun()', the 'crawler' instance is implicitly an instance of the 'AsyncWebCrawler' class. This class encapsulates the 'arun()' method and the related crawling behavior.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic class structure and arun() method relationship, but misses the key functionality of domain-based filtering described in the ground truth. The ground truth specifically describes how the class handles domain filtering through exclude_domains parameters, which is a crucial aspect of its functionality.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Internally, the AsyncWebCrawler.arun() method forwards its parameters (including domain filtering options) to its crawler strategy. The default implementation is provided by AsyncPlaywrightCrawlerStrategy, which handles the crawling process.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements domain-based filtering during crawling by accepting exclude_domains and exclude_social_media parameters which are used to control which URLs are processed during the crawling operation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions that AsyncWebCrawler.arun() forwards parameters to AsyncPlaywrightCrawlerStrategy, but fails to explicitly mention the core domain-based filtering functionality described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy as its abstract base. This base class defines the core crawling interface that supports methods such as 'crawl()', which indirectly handles extra parameters passed from higher levels like domain filtering.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational interface for implementing domain-based filtering through its crawl method, which accepts kwargs that can include domain exclusion parameters documented in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy provides an abstract interface supporting domain filtering through its crawl method's kwargs parameter. The predicted description accurately conveys this relationship while adding the reasonable detail about extending behavior.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation example explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:', showing that this class is the starting point for executing Magic Mode operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the entry point for Magic Mode but misses the crucial detail about how Magic Mode is actually implemented through the crawler_strategy parameter and kwargs. It describes the syntax for using the class but not the underlying mechanism.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls 'await crawler.arun(url=\"https://example.com\", magic=True)', explicitly invoking the arun() method with the 'magic' parameter to enable anti-detection features (Magic Mode).",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the arun() method supports Magic Mode functionality through a 'magic' parameter to enable anti-detection features, which aligns with the ground truth's explanation that it handles this via kwargs and the crawler_strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler by default uses AsyncPlaywrightCrawlerStrategy as its crawler strategy. This class implements anti-detection features (such as overriding navigator properties and simulating human behavior) that fulfill the promises of Magic Mode.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that AsyncPlaywrightCrawlerStrategy implements anti-detection features that enable Magic Mode functionality, including stealth configurations and human behavior simulation. While it doesn't list all specific features, it accurately reflects the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the interface for all crawler strategies. AsyncPlaywrightCrawlerStrategy extends this abstract class to implement features such as those used in Magic Mode.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interfaces needed to implement Magic Mode's anti-bot features through its abstract methods for crawling, screenshots, user agent manipulation, and custom hooks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class but misses the crucial connection to Magic Mode's anti-bot features and instead focuses on an implementation detail (AsyncPlaywrightCrawlerStrategy) not mentioned in the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler with parameters such as browser_type, verbose, and headless. This direct usage demonstrates how to select different browser engines in Crawl4AI.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports different browser engines and shows proper usage, but misses the key architectural detail that this is implemented through the crawler_strategy parameter using PlaywrightCrawlerStrategy",
      "error_type": "omitted_key_mechanism"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the snippet, the arun() method is directly invoked on the AsyncWebCrawler instance to perform the crawl. This shows its role in triggering the crawling process.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as handling the crawling process but misses the key aspect of browser selection through crawler_strategy which is central to the ground truth's description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler, when not provided with a custom crawler_strategy, defaults to instantiating AsyncPlaywrightCrawlerStrategy, which uses the browser_type parameter (e.g., 'firefox', 'webkit') to select the appropriate browser engine.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy uses browser_type parameter to select between Firefox, WebKit, and Chromium browsers, with Chromium being the default. While it frames it through AsyncWebCrawler's instantiation, the core relationship described matches the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, establishing an inheritance relationship. This abstract base class defines the core interface and serves as the foundation for the strategy used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Firefox, WebKit, Chromium) must implement to support the browser selection functionality documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core concept that AsyncCrawlerStrategy is an abstract base class defining the interface, which aligns with the ground truth's explanation that it provides the interface methods for different browser implementations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly imports and instantiates AsyncWebCrawler using an async context (with async with AsyncWebCrawler(verbose=True) as crawler). This demonstrates its role as the primary class to initiate crawling.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class and its instantiation pattern, but misses describing its core functionality around content filtering, processing, and cache control that is central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example explicitly calls the arun() method on an AsyncWebCrawler instance to perform the crawling operation. This method encapsulates the crawling workflow which the snippet demonstrates.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is called on AsyncWebCrawler, but misses the crucial connection between the documented parameters and their implementation in the method, which is central to the ground truth relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The arun() method returns a CrawlResult object, whose attributes (like success, markdown, media, links, error_message) are then accessed in the example. This object represents the outcome of the crawl.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that CrawlResult is a return object from arun() that contains the crawling outputs accessed in the example. While it's less detailed than the ground truth, it conveys the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet tests 'if result.success:' to determine if the crawl was successful. This attribute is fundamental to the CrawlResult interface for error handling and flow control.",
    "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that success is a boolean property used to verify if the crawl operation completed successfully before proceeding with processing the results",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "In the example, the 'markdown' attribute of the CrawlResult object is accessed to print a preview of the clean content. This demonstrates its role in content processing.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that markdown is a field containing cleaned content from the crawl that can be accessed via result.markdown. The predicted description is less detailed but conveys the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The example iterates over result.media[\"images\"], indicating that the 'media' attribute contains media items like images discovered during the crawl.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the media dictionary contains media items like images from the crawl that can be accessed and processed, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet accesses result.links[\"internal\"] to iterate and print internal links, highlighting the use of the 'links' attribute for link extraction.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that links are accessed and iterated through result.links['internal'], but misses the key detail that links are categorized into both internal and external links in the dictionary structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "When crawling fails, the snippet prints result.error_message to display the error details. This attribute is essential for debugging and error handling in the crawl outcome.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the same core relationship - that error_message is used to provide error details when crawling fails. The predicted description adequately conveys the same functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the 'async with' syntax with provided proxy and headers parameters. This directly demonstrates how to combine proxy settings with Magic Mode.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncWebCrawler can be instantiated with proxy and header parameters, and used with magic mode. While the ground truth provides more implementation details about kwargs and AsyncPlaywrightCrawlerStrategy, the core relationship and usage pattern is accurately described in the prediction.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet clearly shows a call to the 'arun()' method on the AsyncWebCrawler instance with the 'magic=True' parameter, which is intended to enable anti-detection features.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method can be called with magic mode parameters for anti-detection features, which aligns with the ground truth's explanation of the method implementing magic mode functionality through crawler settings.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler defaults to using AsyncPlaywrightCrawlerStrategy. The 'magic=True' argument enables anti-detection features within the crawl() method of AsyncPlaywrightCrawlerStrategy, thereby linking it to the documentation's intended behavior.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies magic mode enabling anti-detection features, it misses the main focus of the ground truth which is about proxy configuration implementation in AsyncPlaywrightCrawlerStrategy. The predicted text shifts focus to AsyncWebCrawler defaults rather than proxy functionality.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly recommends using 'JsonCssExtractionStrategy' for extracting highly structured data. The advanced usage tips about handling defaults in field definitions and leveraging transforms directly relate to how this class processes a schema to extract data.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the key relationship - the JsonCssExtractionStrategy class implements schema-based data extraction that aligns with the documented tips, particularly around structured data extraction and incremental schema development",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not mentioned explicitly in the snippet, it forms the foundational abstraction for all extraction strategies including JsonCssExtractionStrategy, thereby indirectly supporting the advanced techniques described.",
    "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but focuses on JsonCssExtractionStrategy rather than the parallel processing functionality that is the core focus of the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates LLMExtractionStrategy when setting up the extraction_strategy argument. This verifies its direct usage for extracting structured data.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of LLMExtractionStrategy but misses the crucial aspect of how it implements structured data extraction using LLM models and schema-based parsing, which is a core part of its functionality described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not mentioned outright in the snippet, it is part of the class hierarchy and thus implicitly connected.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ExtractionStrategy and LLMExtractionStrategy, but misses the crucial functionality aspect described in the ground truth about parallel processing logic and structured data extraction purpose",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly creates an instance of AsyncWebCrawler using a context manager (async with AsyncWebCrawler(verbose=True) as crawler:), illustrating its direct role in initiating the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the use of AsyncWebCrawler with a context manager, but misses crucial functionality like processing HTML content and extracting structured data that are central to the class's purpose as described in the ground truth.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "While the method arun() is not imported directly, the code calls crawler.arun() to perform the crawl. This demonstrates implicit usage of the AsyncWebCrawler.arun() method as part of the crawling workflow.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies the usage of arun() through AsyncWebCrawler, it misses significant functionality described in the ground truth regarding the extraction strategy, data processing and result handling.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates the AsyncWebCrawler class using 'async with AsyncWebCrawler(headless=True) as crawler:'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the correct class and async context manager usage, but misses the core functionality of handling protected sites through parameters like magic mode, overlay removal, and timeouts which are central to the documented relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code explicitly calls the 'arun()' method on the AsyncWebCrawler instance with parameters such as magic=True, remove_overlay_elements=True, and page_timeout=60000 to handle crawling of a protected site.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and some parameters used for protected site crawling, but misses major functionality around error handling, caching, and the full range of customizable parameters that are core to the implementation.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After the crawl operation, the returned result's 'markdown' attribute is used to extract processed markdown content, as shown in the return statement 'return result.markdown if result.success else None'.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute stores/contains the extracted text content from crawling and is returned on successful crawls",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks the 'success' attribute of the result (from CrawlResult) to determine if the markdown content should be returned, as indicated by 'if result.success'.",
    "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship where the success flag determines whether markdown content is returned. Both descriptions convey the same conditional return logic.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it as a context manager to initialize the crawler. This shows a direct use of the AsyncWebCrawler class to perform asynchronous crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage and asynchronous crawling capability, but misses crucial aspects about its caching functionality and schema extraction capabilities that are central to the ground truth relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly imports JsonCssExtractionStrategy from the extraction_strategy module and creates an instance (with parameters 'schema' and 'verbose=True') to be used as the extraction strategy during crawling.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on the import and instantiation of the class, while missing the core functionality of schema-based HTML parsing and structured data extraction described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage example, the arun() method of the AsyncWebCrawler instance is directly invoked to perform the crawling operation on a specified URL.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling URLs but misses crucial aspects about its implementation of extraction strategies, caching functionality, and structured results handling that are central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the result returned by the arun() method is accessed via its 'extracted_content' attribute. The snippet uses json.loads(result.extracted_content) to obtain the structured product data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the extracted_content attribute contains data that needs to be JSON-parsed to get structured product data. Both descriptions convey that result.extracted_content contains raw data that requires json.loads() for processing.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, meaning that the extraction functionality demonstrated in the snippet is based on the contract defined by the ExtractionStrategy abstract base class. Although not explicitly referenced in the snippet, it forms the underpinning interface for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy and captures the core inheritance relationship that enables extraction functionality. While it's less detailed than the ground truth, it captures the essential relationship without any contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation presents a CSS Strategy for product listings by providing a schema with a 'baseSelector' and field definitions. This implicitly aligns with the behavior of JsonCssExtractionStrategy, which extracts data based on CSS selectors and a defined schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that JsonCssExtractionStrategy implements CSS-based extraction using a schema with selectors, matching the e-commerce example from the documentation. The predicted description captures the core relationship even if slightly less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly demonstrates a snippet where an instance of LLMExtractionStrategy is created with a provider and a schema derived from an Article model. This direct instantiation clearly maps to the LLMExtractionStrategy artifact.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is instantiated with a provider and schema for article extraction, which aligns with the ground truth's description of schema-based article extraction functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation includes a code example that explicitly instantiates CosineStrategy with parameters for a semantic filter and a top_k value. This direct reference and configuration maps to the CosineStrategy artifact responsible for content analysis via cosine similarity.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CosineStrategy is used for content analysis using cosine similarity and parameters like semantic_filter and top_k, matching the ground truth's explanation of its core functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly instantiates AsyncWebCrawler (using 'async with AsyncWebCrawler(verbose=True) as crawler') to initiate the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for crawling, but misses the crucial aspect of its ability to handle dynamic content through JavaScript execution and extraction strategies, which is the main focus of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code explicitly calls the 'arun' method on the AsyncWebCrawler instance to perform the crawl with parameters like js_code, wait_for, css_selector, and an extraction_strategy.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship showing that AsyncWebCrawler.arun() is called with various parameters for web crawling. While it's less detailed than the ground truth about dynamic content handling, it accurately describes the basic functionality and parameter usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "An instance of LLMExtractionStrategy is explicitly created with parameters (provider, api_token, instruction) to extract content using a language model for summarizing articles.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that LLMExtractionStrategy uses LLM for content extraction with key parameters, but misses that it's a class implementation that enables dynamic web content extraction functionality rather than just an instance creation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler strategy to process JavaScript code and wait conditions for dynamic content handling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the dynamic content extraction capabilities described in the documentation through its js_code execution and smart_wait methods, which handle JavaScript interaction and waiting for elements to load on the page.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy is used for handling dynamic content with JavaScript execution and wait conditions, which aligns with the ground truth's explanation of how the class implements these capabilities through js_code execution and smart_wait methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The example code accesses the 'extracted_content' attribute of the CrawlResult (returned by AsyncWebCrawler.arun()) to retrieve the JSON string containing summarized article data.",
    "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that extracted_content stores the LLM-processed article summaries in JSON format that are extracted from the crawled data. The predicted description correctly identifies it as an attribute of CrawlResult that contains JSON string data, which aligns with the ground truth's description of it serving as a bridge between raw and structured data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy, meaning the extraction functionalities are built upon the abstract interface provided by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that ExtractionStrategy is a base/abstract class that provides the foundation for different extraction strategies, with LLMExtractionStrategy being one implementation. The predicted description captures this inheritance and extension relationship accurately.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract interface for asynchronous crawling operations, including support for executing JavaScript.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the advanced dynamic content extraction capabilities demonstrated in the documentation, particularly the crawl method that's essential for executing JavaScript and handling LLM extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as defining an interface for asynchronous crawling, but incorrectly states it extends AsyncPlaywrightCrawlerStrategy when the code shows it's an abstract base class being extended by others. It also misses the key aspect of LLM extraction capabilities mentioned in the ground truth.",
      "error_type": "incorrect_inheritance_relationship"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation text explicitly refers to 'AsyncWebCrawler' as the asynchronous web crawling component that supports integration with LLMs. It indicates that this class is central to the functionality of extracting structured data from web pages asynchronously.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main class for asynchronous web crawling with LLM integration for data extraction. While it's less specific about the implementation details mentioned in the ground truth (like the arun method), it captures the core functionality and purpose accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation text explicitly marks 'LLMExtractionStrategy' (shown within backticks) as the strategy used to extract structured data via Language Models. This highlights its key role in the overall extraction process employed by AsyncWebCrawler.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that LLMExtractionStrategy is used for extracting structured data using Language Models via AsyncWebCrawler, which aligns with the ground truth's description of the asynchronous web content extraction implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an async context manager with various configuration parameters (browser_type, headless, user_agent, headers, proxy, etc.) to set up the crawling environment.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's instantiation through async context manager and configuration parameters, but misses describing its implementation of the many features mentioned in the documentation (like content handling, timing controls, anti-detection features) which the ground truth explicitly states.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the async context, the 'arun' method is called on the AsyncWebCrawler instance with various parameters (process_iframes, screenshot, page_timeout, delay_before_return_html, magic, simulate_user, js_code, wait_for) to trigger the crawling process.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of arun() with parameters, but misses the crucial implementation aspects mentioned in the ground truth regarding error handling, caching, and content extraction logic.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The snippet accesses the 'screenshot' attribute of the CrawlResult object to obtain a base64 encoded screenshot of the crawled page.",
    "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the screenshot attribute contains a base64-encoded screenshot of the page when enabled. While it omits the specific parameter setting (screenshot=True), this is a minor detail that doesn't change the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The 'success' attribute of the CrawlResult object is accessed to determine whether the crawling operation was successful.",
    "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the success boolean indicates whether the crawling operation was successful and is returned as part of the result",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler defaults to instantiating AsyncPlaywrightCrawlerStrategy when no custom strategy is provided. The provided configuration parameters (like browser_type, headless, proxy, etc.) are passed to this strategy.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes the relationship between AsyncWebCrawler and AsyncPlaywrightCrawlerStrategy, but misses the main focus of the ground truth which is about how the example demonstrates the key features implemented in AsyncPlaywrightCrawlerStrategy. The prediction focuses more on initialization while the ground truth emphasizes feature usage.",
      "error_type": "incomplete_focus"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy, ensuring adherence to the expected crawling interface. This abstraction underpins the crawling functionality used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class for crawling functionality, but incorrectly specifies 'AsyncPlaywrightCrawlerStrategy' which isn't mentioned in the code, and misses describing the full range of interface methods shown in the ground truth.",
      "error_type": "incomplete_and_inaccurate_details"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy (via 'from crawl4ai.extraction_strategy import LLMExtractionStrategy') and demonstrates its use with several LLM provider strings. This clearly indicates that the LLMExtractionStrategy class is central to handling LLM-based extraction in Crawl4AI.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for handling different LLM providers in Crawl4AI, which aligns with the ground truth's explanation of the class implementing provider flexibility through its constructor parameters",
      "error_type": ""
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is defined as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the base class makes it implicitly part of the extraction workflow demonstrated in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is the base class for different LLM-based extraction implementations. While the predicted description is more concise, it correctly identifies the inheritance relationship and the role of ExtractionStrategy in the extraction workflow.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit call to 'crawler.arun(...)', which demonstrates the use of the AsyncWebCrawler.arun() method as the primary interface to initiate web crawling and content retrieval.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the main interface for web crawling, but misses the core focus on HTML cleaning/sanitization functionality emphasized in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The snippet contains 'print(result.cleaned_html)', explicitly accessing the cleaned_html attribute of the CrawlResult object, which provides the sanitized HTML output after cleaning unnecessary elements.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that cleaned_html contains sanitized HTML output after cleaning/removing unnecessary elements. The predicted description references the code example while the ground truth is more general, but they describe the same core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', demonstrating its direct usage for starting and managing the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented error handling through its arun() method which wraps crawling operations in a try-except block, allowing it to work seamlessly with the documented retry decorator pattern for handling API failures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on AsyncWebCrawler instantiation via async with, while the ground truth emphasizes its error handling implementation through arun(). While both discuss AsyncWebCrawler usage, they highlight different aspects of its functionality.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the retry logic, the snippet calls 'crawler.arun(url=..., extraction_strategy=..., bypass_cache=True)' to perform the actual crawling operation. This method is responsible for processing the URL and returning the crawl result.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements comprehensive error handling by catching all exceptions in a try-except block and returning a CrawlResult object with error details, which aligns with the documentation's emphasis on error handling for external API interactions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the arun method but focuses only on the crawling operation, missing the crucial error handling aspect that is central to the ground truth description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The example code instantiates LLMExtractionStrategy with parameters (provider, api_token, instruction) and passes it as the extraction_strategy to the arun() method. This shows its direct role in configuring LLM-based extraction.",
    "ground_truth_relationship": "The code implements error handling and retries through a custom LLMExtractionStrategy class that uses ThreadPoolExecutor for parallel processing and includes error handling in its extract() method, which appends error information to extracted_content when exceptions occur.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies LLMExtractionStrategy's role in configuring LLM-based extraction, but misses the core focus of the ground truth which is about error handling, retries, and parallel processing implementation",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors and implement retry logic. Here's an example of how you might do this:\n\n```python\nimport asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass LLMExtractionError(Exception):\n    pass\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def extract_with_retry(crawler, url, extraction_strategy):\n    try:\n        result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n        return json.loads(result.extracted_content)\n    except Exception as e:\n        raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        try:\n            content = await extract_with_retry(\n                crawler,\n                \"https://www.example.com\",\n                LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract and summarize main points\"\n                )\n            )\n            print(\"Extracted content:\", content)\n        except LLMExtractionError as e:\n            print(f\"Extraction failed after retries: {e}\")\n\nasyncio.run(main())\n```\n\nThis example uses the `tenacity` library to implement a retry mechanism with exponential backoff, which can help handle temporary failures or rate limiting from the LLM API.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After calling crawler.arun(), the code accesses 'result.extracted_content' to obtain the extracted content, which is then parsed as JSON. This attribute forms part of the public interface of the crawl result.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the text data retrieved from web pages that gets processed through LLM extraction, which can be retried using the documented retry mechanism if the extraction fails.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content is accessed from the result and parsed as JSON, but misses the key aspect that this content comes from LLM extraction of web pages and can be retried on failure, which is a crucial part of the ground truth relationship.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly refers to 'JsonCssExtractionStrategy' to demonstrate advanced extraction features for a complex e-commerce HTML structure. The HTML example with classes like 'category', 'product', and more implies the need to map CSS selectors to specific data fields, which is the primary role of JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class uses BeautifulSoup to parse and extract structured data from HTML elements like the documented e-commerce product listings by mapping CSS selectors to desired fields in a schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly convey that JsonCssExtractionStrategy uses CSS selectors to extract structured data from HTML elements in an e-commerce context, with the predicted description accurately emphasizing the mapping of CSS selectors to data fields",
      "error_type": ""
    }
  },
  {
    "document_text": "## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product categories, each containing multiple products. Each product has details, reviews, and related items. This complex structure will allow us to demonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n```html\n<div class=\"category\">\n  <h2 class=\"category-name\">Electronics</h2>\n  <div class=\"product\">\n    <h3 class=\"product-name\">Smartphone X</h3>\n    <p class=\"product-price\">$999</p>\n    <div class=\"product-details\">\n      <span class=\"brand\">TechCorp</span>\n      <span class=\"model\">X-2000</span>\n    </div>\n    <ul class=\"product-features\">\n      <li>5G capable</li>\n      <li>6.5\" OLED screen</li>\n      <li>128GB storage</li>\n    </ul>\n    <div class=\"product-reviews\">\n      <div class=\"review\">\n        <span class=\"reviewer\">John D.</span>\n        <span class=\"rating\">4.5</span>\n        <p class=\"review-text\">Great phone, love the camera!</p>\n      </div>\n      <div class=\"review\">\n        <span class=\"reviewer\">Jane S.</span>\n        <span class=\"rating\">5</span>\n        <p class=\"review-text\">Best smartphone I've ever owned.</p>\n      </div>\n    </div>\n    <ul class=\"related-products\">\n      <li>\n        <span class=\"related-name\">Phone Case</span>\n        <span class=\"related-price\">$29.99</span>\n      </li>\n      <li>\n        <span class=\"related-name\">Screen Protector</span>\n        <span class=\"related-price\">$9.99</span>\n      </li>\n    </ul>\n  </div>\n  <!-- More products... -->\n</div>\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the documentation snippet, JsonCssExtractionStrategy inherits from ExtractionStrategy. This implies that the documented advanced extraction functionality is underpinned by the abstract interface defined in ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as an abstract base class with methods to extract and process structured data from HTML content like the documented e-commerce website example, where complex nested elements (products, reviews, features) need to be parsed systematically.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class, but focuses mainly on JsonCssExtractionStrategy inheritance rather than ExtractionStrategy's core purpose of structured HTML data extraction described in the ground truth.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy (via 'from crawl4ai.extraction_strategy import LLMExtractionStrategy') and shows its usage with various provider strings. This indicates the class is directly used to implement LLM-based extraction for different providers.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements provider flexibility by accepting different LLM services (OpenAI, Hugging Face, Ollama) through its constructor's provider parameter and handling their authentication via api_token, which directly corresponds to the documented usage examples showing multiple provider configurations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy supports multiple providers and is imported/used for LLM-based extraction, aligning with the ground truth's explanation of the class's provider flexibility and authentication handling.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Using LLMExtractionStrategy with Different Providers Crawl4AI supports multiple LLM providers for extraction: ```python from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\") # OpenAI await extract_structured_data_using_llm(\"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")) # Hugging Face await extract_structured_data_using_llm( \"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\") ) # Ollama await extract_structured_data_using_llm(\"ollama/llama3.2\") # With custom headers custom_headers = { \"Authorization\": \"Bearer your-custom-token\", \"X-Custom-Header\": \"Some-Value\" } await extract_structured_data_using_llm(extra_headers=custom_headers) ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its functionalities are inherited by LLMExtractionStrategy, making it an implicit part of the extraction process discussed in the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing different LLM provider-specific extraction strategies, which the documentation demonstrates through examples of OpenAI, HuggingFace, and Ollama implementations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy inherits from ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy serving as the foundation for LLM provider-specific implementations. While the predicted version is more concise, it captures the core inheritance relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using an 'async with' statement (async with AsyncWebCrawler(verbose=True) as crawler) to set up the crawler environment.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements screenshot functionality through its arun method, which accepts a 'screenshot' boolean parameter and returns the captured image data in base64 format, which is then decoded and saved in the documented capture_and_save_screenshot function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the async context manager functionality of AsyncWebCrawler, while the ground truth describes its core screenshot functionality. While the predicted text correctly identifies an aspect of the class, it misses the main point about screenshot capabilities.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet directly calls the arun() method on the AsyncWebCrawler instance (await crawler.arun(url=..., screenshot=True, bypass_cache=True)), thereby invoking the functionality that captures screenshots.",
    "ground_truth_relationship": "The arun() method implements screenshot capture functionality by accepting a screenshot boolean parameter and populating screenshot_data from either the cache or a new crawl which is then returned in the CrawlResult for base64 decoding and saving as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is called with screenshot functionality, but misses crucial details about how screenshots are handled (caching, screenshot_data population, and returning in CrawlResult for base64 decoding)",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "After executing arun(), the code accesses the 'screenshot' attribute of the resulting CrawlResult object. This attribute holds the Base64\u2010encoded screenshot data which is then decoded and saved.",
    "ground_truth_relationship": "The CrawlResult.screenshot field stores the base64-encoded screenshot data that is later decoded and saved to a file in the capture_and_save_screenshot function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the screenshot field contains base64-encoded screenshot data that gets decoded and saved to a file",
      "error_type": ""
    }
  },
  {
    "document_text": "### Taking Screenshots \ud83d\udcf8\n\nCapture screenshots of web pages easily:\n\n```python\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            screenshot=True,\n            bypass_cache=True\n        )\n        \n        if result.success and result.screenshot:\n            import base64\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, 'wb') as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks the 'success' attribute on the result to determine if the crawl operation (and thus the screenshot capture) was successful.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used to verify if the screenshot capture operation completed successfully before attempting to save the screenshot data to disk.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the success boolean property is used to verify if the screenshot capture operation was successful before proceeding with saving the image.",
      "error_type": null
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet provides a detailed Python schema with 'baseSelector' and 'fields' properties used to extract a complex, nested structure. This schema mirrors the expected input for JsonCssExtractionStrategy, which uses BeautifulSoup to parse HTML based on such definitions, handling nested objects, simple lists, and nested lists.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction by recursively parsing HTML elements using BeautifulSoup's select() method to match the CSS selectors defined in the documented schema structure.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy uses BeautifulSoup to parse HTML based on a schema structure with selectors, handling various types of nested data. While the ground truth emphasizes the recursive parsing aspect, the predicted description covers the core relationship between the code and schema.",
      "error_type": ""
    }
  },
  {
    "document_text": "Now, let's create a schema to extract this complex structure:\n\n```python\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"feature\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\",\n                            \"selector\": \"span.reviewer\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\",\n                            \"selector\": \"span.rating\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\",\n                            \"selector\": \"p.review-text\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\",\n                            \"selector\": \"span.related-name\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\",\n                            \"selector\": \"span.related-price\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\nThis schema demonstrates several advanced features:\n\n1. **Nested Objects**: The `details` field is a nested object within each product.\n2. **Simple Lists**: The `features` field is a simple list of text items.\n3. **Nested Lists**: The `products` field is a nested list, where each item is a complex object.\n4. **Lists of Objects**: The `reviews` and `related_products` fields are lists of objects.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "ExtractionStrategy is the abstract base class that outlines the extraction interface. The provided schema example complies with this interface and is intended to be used by concrete implementations like JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class provides the core functionality for implementing the schema-based extraction pattern shown in the documentation by defining abstract methods that process HTML content into structured data following the nested object and list patterns defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as an abstract base class for extraction, but misses the crucial aspect of schema-based pattern implementation and structured data processing that's core to the ground truth relationship.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet shows JSON examples that define extraction schemas using keys such as 'type', 'selector', and 'fields' (with values like 'nested', 'list', and 'nested_list'). These examples align with the expected input for an extraction strategy based on CSS rules. JsonCssExtractionStrategy is designed to process such schema definitions and extract structured data from HTML using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements a BeautifulSoup-based parser that processes nested JSON schemas with CSS selectors to extract structured data according to the documented field types (nested, list, nested_list) and their hierarchical relationships.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the JsonCssExtractionStrategy class and its role in processing JSON schemas with CSS selectors to extract data from HTML, matching the ground truth's explanation. While it's slightly less detailed, it covers the essential functionality without any contradictions.",
      "error_type": "none"
    }
  },
  {
    "document_text": "Let's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array for the nested structure:\n\n```python\n{\n    \"name\": \"details\",\n    \"selector\": \"div.product-details\",\n    \"type\": \"nested\",\n    \"fields\": [\n        {\n            \"name\": \"brand\",\n            \"selector\": \"span.brand\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"model\",\n            \"selector\": \"span.model\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n```python\n{\n    \"name\": \"features\",\n    \"selector\": \"ul.product-features li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"feature\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```\n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n```python\n{\n    \"name\": \"products\",\n    \"selector\": \"div.product\",\n    \"type\": \"nested_list\",\n    \"fields\": [\n        // ... fields for each product\n    ]\n}\n```\n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the list:\n\n```python\n{\n    \"name\": \"related_products\",\n    \"selector\": \"ul.related-products li\",\n    \"type\": \"list\",\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"selector\": \"span.related-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \"span.related-price\",\n            \"type\": \"text\"\n        }\n    ]\n}\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends the abstract base class ExtractionStrategy. Although the documentation does not explicitly mention ExtractionStrategy, any extraction functionality based on a schema (including nested objects and lists) relies on the contract defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract class provides the core framework for implementing different extraction patterns (nested, list, nested_list) described in the documentation through its extract() method that processes HTML content into structured data blocks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify ExtractionStrategy as the abstract base class that provides the foundation for implementing different extraction patterns. The predicted text captures the core relationship of ExtractionStrategy being the base for extraction functionality, even though it focuses more on JsonCssExtractionStrategy specifically.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(headless=True) as crawler:', demonstrating its role as the primary class for handling the crawling process on protected sites.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented protected site crawling functionality through its arun method, which supports the parameters shown in the example like headless mode, magic mode, overlay removal, and custom timeouts for handling protected sites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for crawling protected sites and accurately describes the basic usage pattern with async context manager. While it doesn't detail all the supported parameters, it captures the core relationship and usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun()' method on the AsyncWebCrawler instance with parameters such as 'magic=True', 'remove_overlay_elements=True', and 'page_timeout=60000', indicating how the crawling process is initiated and configured.",
    "ground_truth_relationship": "The code implements arun() with extensive error handling, caching, and customizable parameters that enable protected site crawling features shown in the documentation example like magic mode, overlay removal, and configurable timeouts.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core idea of configuring and calling arun() with parameters, but misses key aspects like error handling, caching, and the extensive customization capabilities mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The returned object from the 'arun()' method is accessed via its 'markdown' attribute to retrieve the processed content, as seen from the conditional 'return result.markdown if result.success else None'.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted text output from crawling protected sites as shown in the example where it's returned upon successful crawls.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute stores/returns the extracted text content from successful crawls",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Handling Protected Sites\n\n```python\nasync def crawl_protected_site(url: str):\n    async with AsyncWebCrawler(headless=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            magic=True,\n            remove_overlay_elements=True,  # Remove popups/modals\n            page_timeout=60000            # Increased timeout for protection checks\n        )\n        \n        return result.markdown if result.success else None\n```\n",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks the 'success' attribute of the result to determine whether the crawl operation was successful, serving as a gatekeeper for content retrieval.",
    "ground_truth_relationship": "The success flag directly determines whether protected site content should be returned as markdown or None in the crawling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the success flag determines whether content is returned, with the predicted description capturing the core gatekeeper role of the success attribute",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using a 'proxy' parameter. This demonstrates that AsyncWebCrawler supports proxy configuration as part of its public interface.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements proxy support through its crawler_strategy parameter, which accepts proxy configurations as shown in the documentation's HTTP and SOCKS5 proxy examples during initialization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler supports proxy configuration through initialization parameters, which aligns with the ground truth's explanation that this is implemented via the crawler_strategy parameter. The main relationship regarding proxy support is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Proxy Setup\n\nSimple proxy configuration:\n\n```python\n# Using proxy URL\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nasync with AsyncWebCrawler(\n    proxy=\"socks5://proxy.example.com:1080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code calls the arun() method on the AsyncWebCrawler instance to perform a crawl operation. This confirms that after configuring the proxy via AsyncWebCrawler, the arun() method is used to initiate the crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality referenced in the proxy setup documentation by accepting and utilizing proxy configurations through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used to perform crawling, but misses the key relationship with proxy configuration handling through crawler_strategy that is central to the ground truth description",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly demonstrates how to instantiate the LLMExtractionStrategy by passing the provider, api_token, and instruction parameters. This direct usage shows that LLMExtractionStrategy is the class used to customize the LLM provider.",
    "ground_truth_relationship": "The code implements the documented LLM provider customization by allowing users to specify a provider and API token through the LLMExtractionStrategy class constructor, which then uses these parameters to initialize the extraction process with the specified LLM service.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship showing that LLMExtractionStrategy allows customization of LLM providers through constructor parameters provider and api_token, which aligns with the ground truth's explanation of the implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use any LLM provider you want. Just pass the correct model name and API token:\n\n```python\nextraction_strategy=LLMExtractionStrategy(\n    provider=\"your_llm_provider/model_name\",\n    api_token=\"your_api_token\",\n    instruction=\"Your extraction instruction\"\n)\n```\n\nThis flexibility allows you to integrate with various LLM providers and tailor the extraction process to your specific needs.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, LLMExtractionStrategy is a subclass of ExtractionStrategy. This implies that ExtractionStrategy forms the foundational interface or behavior for extraction strategies, including the LLM-based one.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for implementing custom LLM providers through its extensible design, which aligns with the documentation's description of using different LLM providers via the litellm library.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that ExtractionStrategy is the base class that enables LLM-based extraction functionality. While it doesn't explicitly mention litellm integration, it accurately describes the foundational relationship between ExtractionStrategy and LLM implementations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit call to 'crawler.arun(...)' with html2text options for markdown conversion. This directly demonstrates the usage of the 'AsyncWebCrawler.arun()' method as part of the public API.",
    "ground_truth_relationship": "The documentation shows the html2text customization options that can be passed as kwargs to the arun() method, which processes these options when crawling and converting webpage content as shown in the code's **kwargs parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation shows how to use the arun() method with html2text options, which aligns with the ground truth's explanation of how html2text customization options can be passed as kwargs to the arun() method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Customization\n\n### HTML to Text Options\n\nConfigure markdown conversion:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    html2text={\n        \"escape_dot\": False,\n        \"body_width\": 0,\n        \"protect_links\": True,\n        \"unicode_snob\": True\n    }\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not explicitly mentioned by name, the usage of 'crawler.arun(...)' implies that the 'crawler' object is an instance of the AsyncWebCrawler class. This class instantiation forms the context for the method call and thus is implicitly traced.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements HTML-to-text customization through its arun() method which accepts HTML conversion parameters as keyword arguments (**kwargs) that are passed to the underlying crawler strategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the relationship between 'crawler' and AsyncWebCrawler class, but misses the core functionality of HTML-to-text customization which is the main point of the ground truth. It focuses on object instantiation rather than the customization capabilities.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with verbose=True as shown in 'async with AsyncWebCrawler(verbose=True) as crawler:'. This explicitly demonstrates the usage of the AsyncWebCrawler class.",
    "ground_truth_relationship": "The verbose parameter in the AsyncWebCrawler class enables detailed logging through conditional print statements that track crawler initialization, warmup status, and crawling progress throughout the code's execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly shows how to enable verbose mode through initialization, but misses the key aspect of what verbose mode actually does (logging crawler initialization, warmup status, and progress via print statements)",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Logging and Debugging\n\nEnable verbose mode for detailed logging:\n\n```python\nasync with AsyncWebCrawler(verbose=True) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet shows the invocation of the arun() method on the AsyncWebCrawler instance via 'result = await crawler.arun(url=\"https://example.com\")', explicitly demonstrating its role in performing the crawl operation.",
    "ground_truth_relationship": "The code implements verbose logging by conditionally printing crawl timing and status information when verbose=True is set, matching the documentation's guidance for enabling detailed logging output.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on describing the invocation pattern of arun(), while the ground truth describes the implementation of verbose logging functionality within the code. These are fundamentally different relationships.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet describes a Cosine Strategy that uses similarity-based clustering, vector representation, and ranking of content sections. The CosineStrategy class (artifact id 7) directly implements this functionality by applying cosine similarity measures and clustering logic, matching the documented workflow.",
    "ground_truth_relationship": "The code implements the documented 5-step Cosine Strategy workflow through the extract() method, which breaks content into chunks (step 1), generates embeddings via get_embeddings() (step 2), performs similarity calculations in hierarchical_clustering() (step 3), groups content using cluster labels (step 4), and ranks content using filter_clusters_by_word_count() (step 5).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the high-level workflow of the Cosine Strategy, mentioning similarity-based clustering, vector representation, and content ranking. While it's less detailed than the ground truth, it correctly identifies the core functionality and relationship between the documentation and code.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Cosine Strategy\n\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n\n## How It Works\n\nThe Cosine Strategy:\n1. Breaks down page content into meaningful chunks\n2. Converts text into vector representations\n3. Calculates similarity between chunks\n4. Clusters similar content together\n5. Ranks and filters content based on relevance",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends ExtractionStrategy. This inheritance indicates that the foundational extraction framework (ExtractionStrategy, artifact id 21) supports the specialized cosine-based extraction logic implemented in CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundational structure for implementing the documented Cosine Strategy by defining the interface for content extraction and parallel processing through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that ExtractionStrategy serves as the foundational abstract base class that defines the interface and structure for implementing extraction strategies, including the Cosine Strategy. The predicted description captures this inheritance relationship accurately.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a usage example where 'crawler.arun(url=\"https://example.com\")' is called to initiate crawling. This method is responsible for performing the crawl and returning a result that includes media information.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality described in the documentation, handling both regular and lazy-loaded media content through its customizable parameters like wait_for and delay_before_return_html while supporting caching and screenshot capabilities.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the core crawling method but fails to mention its key capabilities for handling media content (lazy-loading, caching, screenshots) that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Media Processing\n\nCrawl4AI provides comprehensive media extraction and analysis capabilities. It automatically detects and processes various types of media elements while maintaining their context and relevance.\n\n### Image Processing\nThe library handles various image scenarios, including:\n- Regular images\n- Lazy-loaded images\n- Background images\n- Responsive images\n- Image metadata and context\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nfor image in result.media[\"images\"]:\n    # Each image includes rich metadata\n    print(f\"Source: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Context: {image['context']}\")  # Surrounding text\n    print(f\"Relevance score: {image['score']}\")  # 0-10 score\n```\n\n### Handling Lazy-Loaded Content\nCrawl4aai already handles lazy loading for media elements. You can also customize the wait time for lazy-loaded content:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:img[data-src]\",  # Wait for lazy images\n    delay_before_return_html=2.0   # Additional wait time\n)\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "In the code example, after the crawl the result object is used to access the 'media' attribute (result.media) where media details such as images and their metadata are stored. This attribute is part of the public interface of the CrawlResult class.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores media-related data like images and their metadata (source, alt text, description, context, relevance score) extracted during web crawling.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately describe the media dictionary property as storing media-related data and metadata from web crawling. The predicted description captures the core relationship even if it doesn't list all the specific metadata fields.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the use of the AsyncWebCrawler class in a 'with' block. The code example instantiates AsyncWebCrawler with parameters such as headless, verbose, and sleep_on_close, clearly demonstrating its role as the main entry point for configuring and initiating a crawl.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its __init__ method, where headless, verbose, and sleep_on_close parameters are passed via kwargs to the underlying AsyncPlaywrightCrawlerStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is the main class for configuring and running web crawls, and accurately describes its usage with configuration parameters through a 'with' block, which aligns with how the class implements these options in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Configuration\n\nCommon browser settings:\n\n```python\nasync with AsyncWebCrawler(\n    headless=True,           # Run in headless mode (no GUI)\n    verbose=True,           # Enable detailed logging\n    sleep_on_close=False    # No delay when closing browser\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the same code example, after instantiating AsyncWebCrawler, the arun() method is explicitly invoked to perform the crawling action. This indicates that the method is a key part of the public API of AsyncWebCrawler for executing a crawl.",
    "ground_truth_relationship": "The code implements an async crawler method that accepts the documented configuration parameters like headless, verbose, and sleep_on_close through its constructor while providing additional flexibility through optional parameters such as word_count_threshold, extraction_strategy, and chunking_strategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as a key method of AsyncWebCrawler but misses the crucial aspect of how it integrates with the constructor's configuration parameters. The ground truth emphasizes the relationship between constructor parameters and arun()'s functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet begins with 'async with AsyncWebCrawler() as crawler:', which explicitly instantiates and uses the AsyncWebCrawler class for dynamic page interactions.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented complex page interactions through its arun method, which supports session management, JavaScript execution, and dynamic content loading through parameters like session_id, js_code, and wait_for as shown in the example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's basic usage pattern with async context manager, but misses crucial functionality around session management, JavaScript execution, and dynamic content loading that are core to the class's purpose as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls the arun() method (e.g., result = await crawler.arun(...)) to load pages, handle cookie consent, and process dynamic content. This shows an explicit use of the arun() method to perform crawling.",
    "ground_truth_relationship": "The arun() method implements the documented dynamic page crawling by accepting parameters for URL, JavaScript code execution, session management, and wait conditions, which enables the complex interactions shown in the example like handling cookie consent and infinite scroll pagination.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling pages, but misses crucial capabilities around JavaScript execution, session management and wait conditions that enable the complex dynamic page interactions described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "After the arun() method returns a crawl result, the snippet accesses 'result.cleaned_html' to determine the number of items loaded (via len(result.cleaned_html)). This shows an explicit reference to a public attribute of the CrawlResult model.",
    "ground_truth_relationship": "The cleaned_html attribute stores the processed HTML content after each dynamic page load iteration as shown in the example where it's used to track the number of loaded items.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core idea that cleaned_html contains processed HTML content that can be accessed after page loads to check item counts. The predicted description accurately conveys the relationship even if it's slightly more focused on the specific usage example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complex Interactions Example\n\nHere's an example of handling a dynamic page with multiple interactions:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        # Initial page load\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Handle cookie consent\n            js_code=\"document.querySelector('.cookie-accept')?.click();\",\n            wait_for=\"css:.main-content\"\n        )\n\n        # Load more content\n        session_id = \"dynamic_session\"  # Keep session for multiple interactions\n        \n        for page in range(3):  # Load 3 pages of content\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                session_id=session_id,\n                js_code=[\n                    # Scroll to bottom\n                    \"window.scrollTo(0, document.body.scrollHeight);\",\n                    # Store current item count\n                    \"window.previousCount = document.querySelectorAll('.item').length;\",\n                    # Click load more\n                    \"document.querySelector('.load-more')?.click();\"\n                ],\n                # Wait for new items\n                wait_for=\"\"\"() => {\n                    const currentCount = document.querySelectorAll('.item').length;\n                    return currentCount > window.previousCount;\n                }\"\"\",\n                # Only execute JS without reloading page\n                js_only=True if page > 0 else False\n            )\n            \n            # Process content after each load\n            print(f\"Page {page + 1} items:\", len(result.cleaned_html))\n            \n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The snippet ends with a call to 'await crawler.crawler_strategy.kill_session(session_id)', explicitly invoking the kill_session() method on the underlying crawler strategy (an instance of AsyncPlaywrightCrawlerStrategy) to clean up the session.",
    "ground_truth_relationship": "The kill_session method is used at the end of the complex interaction example to clean up browser resources by closing the page and context objects associated with a specific session_id, preventing memory leaks during multi-page dynamic content crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core purpose of using kill_session() to clean up sessions at the end of the interaction example, even though it doesn't detail all the specific cleanup actions (closing page/context).",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "NlpSentenceChunking",
    "predicted_relationship": "The documentation snippet explicitly names and shows an example of using 'NlpSentenceChunking' to split text into sentences, as seen in the code sample which imports and instantiates the class.",
    "ground_truth_relationship": "The code implements sentence chunking by using NLTK's sent_tokenize function to split text into sentences, which directly fulfills the documentation's promise of using NLP models for accurate sentence boundary detection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that NlpSentenceChunking is used for splitting text into sentences, which aligns with the ground truth's explanation of using NLTK's sent_tokenize for sentence boundary detection.",
      "error_type": ""
    }
  },
  {
    "document_text": "### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring accurate sentence boundaries.\n\n#### When to Use\n- Ideal for texts where sentence boundaries are crucial.\n- Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n- None.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import NlpSentenceChunking\n\nchunker = NlpSentenceChunking()\n\n# Sample text\ntext = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "NlpSentenceChunking is implemented as a subclass of ChunkingStrategy. Although the documentation does not explicitly mention the base class, this relationship is inherent in the implementation.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class defines the base chunking interface that NlpSentenceChunking must implement through its required chunk() method, which takes text input and returns a list of sentence chunks.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between NlpSentenceChunking and ChunkingStrategy, even though it's less detailed than the ground truth about the specific method implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a usage example that calls 'await crawler.arun(...)' with the 'wait_for' parameter. This clearly indicates that the 'arun()' method of AsyncWebCrawler is in use to trigger wait conditions during crawling.",
    "ground_truth_relationship": "The arun() method implements waiting functionality through its **kwargs parameter, which accepts the 'wait_for' conditions documented in both CSS and JavaScript formats to control when crawling proceeds.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method handles wait conditions through the documented usage examples. While it doesn't explicitly mention the **kwargs parameter, it accurately describes the core relationship between arun() and wait functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, the wait condition functionality (both CSS-based and JavaScript-based waiting) is implemented inside the crawl method of AsyncPlaywrightCrawlerStrategy. This class processes the 'wait_for' parameter (by delegating to methods like 'smart_wait') and is used by AsyncWebCrawler.arun(), thus supporting the documented behavior.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through its smart_wait method, which handles both CSS-based waiting using page.wait_for_selector and JavaScript-based waiting using page.evaluate for custom conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that AsyncPlaywrightCrawlerStrategy implements the documented wait conditions through smart_wait method, supporting both CSS and JavaScript-based waiting. While it adds some implementation details about delegation, the fundamental relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Wait Conditions\n\n### CSS-Based Waiting\n\nWait for elements to appear:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"css:.dynamic-content\"  # Wait for element with class 'dynamic-content'\n)\n```\n\n### JavaScript-Based Waiting\n\nWait for custom conditions:\n\n```python\n# Wait for number of elements\nwait_condition = \"\"\"() => {\n    return document.querySelectorAll('.item').length > 10;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_condition}\"\n)\n\n# Wait for dynamic content to load\nwait_for_content = \"\"\"() => {\n    const content = document.querySelector('.content');\n    return content && content.innerText.length > 100;\n}\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=f\"js:{wait_for_content}\"\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy. This inheritance establishes the interface for crawl operations, including the handling of the 'wait_for' parameter. Although the abstract base does not implement waiting logic itself, it is a necessary part of the trace as it defines the contract used by its subclass.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the base interface that enables waiting functionality through its crawl method's **kwargs parameter, which can accept the wait_for conditions described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that AsyncCrawlerStrategy is an abstract base class that defines the interface enabling wait functionality through its crawl method parameters, even though they phrase it slightly differently.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates CosineStrategy with parameters such as 'semantic_filter', 'word_count_threshold', and 'sim_threshold'. This shows direct usage of this class.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters for semantic filtering, word count thresholds, and similarity thresholds exactly as shown in the basic usage documentation example, allowing users to create instances with custom filtering criteria for web content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship by describing how CosineStrategy is used with configurable parameters for filtering and processing web content, matching the ground truth's explanation of the class's functionality and usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not explicitly mentioned in the snippet, its functionality is inherited by CosineStrategy, making it an implicit part of the extraction process.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing functionality shown in the documentation example, where specific strategies like CosineStrategy inherit and implement the extract method with their own parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between CosineStrategy and ExtractionStrategy, and while it's more concise than the ground truth, it doesn't contradict or misunderstand the core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet shows 'async with AsyncWebCrawler() as crawler:' which directly instantiates AsyncWebCrawler and uses it as an asynchronous context manager to perform web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented functionality by providing an async context manager with arun() method that accepts URL and extraction strategy parameters, exactly matching the basic usage example shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the async context manager usage but misses crucial aspects about the extraction strategy functionality that is a key part of the documented relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the asynchronous context, the code calls 'crawler.arun(...)', directly invoking the arun() method of AsyncWebCrawler to initiate the crawling and extraction process.",
    "ground_truth_relationship": "The code implements an asynchronous crawling method that accepts the extraction_strategy parameter shown in the documentation example, processing the URL with configurable thresholds and returning a CrawlResult containing extracted content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic async invocation but misses crucial aspects about extraction strategy processing and result handling that are central to the relationship shown in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl is performed via AsyncWebCrawler.arun(), the 'extracted_content' attribute of the result is accessed. This property provides the extracted content from the crawled webpage.",
    "ground_truth_relationship": "The extracted_content property holds the text content gathered by the crawler after applying the specified semantic filtering and clustering strategy.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that extracted_content holds content from crawling, but misses the key aspect of semantic filtering and clustering strategy being applied to that content",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet includes a direct usage example where an AsyncWebCrawler object is instantiated with an async context manager. This confirms that the AsyncWebCrawler class is explicitly referenced for enabling Magic Mode.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements Magic Mode's anti-bot protections through its crawler_strategy parameter which defaults to AsyncPlaywrightCrawlerStrategy, handling browser automation masking and human behavior simulation through its arun() method's **kwargs parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in Magic Mode implementation but misses the key point that it's handled through the crawler_strategy parameter and AsyncPlaywrightCrawlerStrategy. The prediction suggests direct implementation rather than delegation.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example explicitly calls the arun() method on the AsyncWebCrawler object, passing magic=True. This demonstrates the use of the arun() method to activate anti-detection features (Magic Mode).",
    "ground_truth_relationship": "The arun() method accepts a 'magic' parameter in its **kwargs which, when set to True, enables the anti-bot protection features described in the documentation by delegating the actual crawling behavior to the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is used to handle anti-detection features (Magic Mode) through its parameters, which aligns with the ground truth's explanation of how the method accepts and processes magic-related parameters via kwargs.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Magic Mode & Anti-Bot Protection\n\nCrawl4AI provides powerful anti-detection capabilities, with Magic Mode being the simplest and most comprehensive solution.\n\n## Magic Mode\n\nThe easiest way to bypass anti-bot protections:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enables all anti-detection features\n    )\n```\n\nMagic Mode automatically:\n- Masks browser automation signals\n- Simulates human-like behavior\n- Overrides navigator properties\n- Handles cookie consent popups\n- Manages browser fingerprinting\n- Randomizes timing patterns\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly referenced in the documentation snippet, AsyncPlaywrightCrawlerStrategy is the default crawling strategy used by AsyncWebCrawler. When the magic=True parameter is passed to AsyncWebCrawler.arun(), it is forwarded internally to AsyncPlaywrightCrawlerStrategy.crawl() where conditions checking for 'magic' (alongside 'override_navigator' and 'simulate_user') trigger anti-detection features such as overriding navigator properties.",
    "ground_truth_relationship": "The code implements Magic Mode by combining anti-bot features like stealth browser configurations, navigator property overrides, and human-like behavior simulations through the magic=True parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the magic=True parameter triggers anti-detection features in AsyncPlaywrightCrawlerStrategy's crawl method. Both descriptions highlight the anti-bot protection functionality and how it gets activated.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', indicating direct instantiation and use.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements the cryptocurrency price extraction example by providing the core crawling functionality used in the arun() method to fetch and process the Coinbase webpage according to the specified JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the instantiation of AsyncWebCrawler but misses the core purpose of implementing cryptocurrency price extraction functionality. While not wrong, it focuses on a minor implementation detail rather than the main relationship between the code and documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'await crawler.arun(...)' on the AsyncWebCrawler instance, demonstrating usage of the 'arun()' method. Although not directly imported by name, its invocation is a key part of the crawling workflow.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality demonstrated in the documentation example by handling the URL request to Coinbase, applying the specified JsonCssExtractionStrategy, and returning structured cryptocurrency price data through CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() in the example but misses crucial aspects of its core functionality - handling the URL request, applying extraction strategy, and returning structured data",
      "error_type": "incomplete_description"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly imports and instantiates JsonCssExtractionStrategy with a defined extraction schema to extract cryptocurrency prices, showing direct usage of this extraction strategy.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes a schema-defined structure to extract cryptocurrency data from Coinbase's HTML using CSS selectors, where the example code demonstrates creating a schema with baseSelector for table rows and field selectors for crypto name, symbol, and price.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic concept of JsonCssExtractionStrategy being used to extract cryptocurrency data, but misses the crucial aspect of how it processes the schema structure with baseSelector and field selectors for specific crypto data points",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase explore page.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n    # Define the extraction schema\n    schema = {\n        \"name\": \"Coinbase Crypto Prices\",\n        \"baseSelector\": \".cds-tableRow-t45thuk\",\n        \"fields\": [\n            {\n                \"name\": \"crypto\",\n                \"selector\": \"td:nth-child(1) h2\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"symbol\",\n                \"selector\": \"td:nth-child(1) p\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"td:nth-child(2)\",\n                \"type\": \"text\",\n            }\n        ],\n    }\n\n    # Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # Use the AsyncWebCrawler with the extraction strategy\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.coinbase.com/explore\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        # Parse the extracted content\n        crypto_prices = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n        print(json.dumps(crypto_prices[0], indent=2))\n\n    return crypto_prices\n\n# Run the async function\nasyncio.run(extract_structured_data_using_css_extractor())",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy, meaning that it implements the common interface for extraction strategies. This base relationship is implicit in the usage since the extraction strategy conforms to the overall framework.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core functionality shown in the Coinbase example by defining abstract extract() and run() methods that concrete strategies like JsonCssExtractionStrategy inherit to implement specific data extraction patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, noting that concrete strategies implement the common interface. While it's more concise than the ground truth, it captures the essential relationship without contradictions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet directly imports and instantiates LLMExtractionStrategy to extract structured data. This is evidenced by the line 'from crawl4ai.extraction_strategy import LLMExtractionStrategy' and its instantiation with specific parameters like provider, api_token, schema, and instruction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured data extraction by accepting a provider (like Ollama or HuggingFace), schema, and instruction parameters exactly as shown in the documentation, while handling the internal complexity of chunking, rate limiting, and parallel processing for different LLM providers.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of LLMExtractionStrategy but misses crucial aspects about its internal complexity including chunking, rate limiting, and parallel processing for different LLM providers that are central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the ExtractionStrategy abstract class. Although ExtractionStrategy is not directly mentioned in the snippet, its role is implicit as it defines the interface that LLMExtractionStrategy implements for performing extraction.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing different extraction methods including the LLM-based extraction shown in the documentation through its abstract extract() method and parallel processing run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses the crucial aspect that ExtractionStrategy provides core infrastructure and parallel processing functionality through its extract() and run() methods.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls 'await crawler.arun(...)' to initiate the crawling process, which directly corresponds to the AsyncWebCrawler.arun() method. This method is responsible for handling the crawling workflow including the use of the extraction strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented structured data extraction by accepting an extraction_strategy parameter that can be configured with LLMExtractionStrategy to process crawled content through various LLM providers.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality but misses the key aspect of structured data extraction through LLM providers that is central to the ground truth relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Structured Data Extraction\n\nCrawl4AI offers two powerful approaches for structured data extraction:\n\n### 1. LLM-Based Extraction\n\nUse any LLM (OpenAI, HuggingFace, Ollama, etc.) to extract structured data with high accuracy:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[dict]\n    relationships: List[dict]\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # or \"huggingface/...\", \"ollama/...\"\n    api_token=\"your-token\",   # not needed for Ollama\n    schema=KnowledgeGraph.schema(),\n    instruction=\"Extract entities and relationships from the content\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nknowledge_graph = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After calling the crawl method, the snippet accesses 'result.extracted_content' to retrieve the structured data. This indicates that the extracted_content attribute of the CrawlResult class is used to hold the output from the extraction strategy.",
    "ground_truth_relationship": "The extracted_content field in CrawlResult stores the structured data output produced by LLMExtractionStrategy as a JSON string, which can then be parsed into a Pydantic model.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that extracted_content holds the output from the extraction strategy, and this aligns with the ground truth's explanation that it stores structured data as JSON that can be parsed. The minor omission of JSON formatting details doesn't change the core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the 'async with' context manager.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented multi-format extraction by providing an arun() method that accepts different extraction strategies and processes HTML content to return structured data, markdown content, and media elements as shown in the comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies a valid aspect (usage of async with context manager) but misses the core functionality of AsyncWebCrawler - its ability to handle multiple extraction strategies and return various content formats as shown in the example.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet directly calls the arun() method on the AsyncWebCrawler instance to perform crawling and obtain crawl results.",
    "ground_truth_relationship": "The documentation demonstrates three different use cases of AsyncWebCrawler.arun() by showing how it can extract content using different extraction strategies (no strategy, LLM strategy, and JSON CSS strategy) while the code shows the underlying implementation that handles these different strategies through its extraction_strategy parameter.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the crucial aspect of how it handles different extraction strategies, which is a core part of the relationship shown in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly creates an instance of LLMExtractionStrategy for structured data extraction by passing parameters such as provider, schema, and instruction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the specific functionality shown in the documentation example where it processes URLs with custom providers, schemas, and instructions to extract structured data using language models as demonstrated in the 'crawler.arun' call with LLMExtractionStrategy parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy shown in the documentation - creating an instance with provider, schema, and instruction parameters for structured data extraction. While more brief than the ground truth, it doesn't contradict or misunderstand the relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy (and also JsonCssExtractionStrategy) extends ExtractionStrategy, making ExtractionStrategy the underlying base class for these extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for the different extraction methods (LLMExtractionStrategy, JsonCssExtractionStrategy) demonstrated in the documentation's comprehensive example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that ExtractionStrategy is the base class for different extraction strategy implementations, which matches the ground truth's explanation of ExtractionStrategy providing the foundation for different extraction methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates JsonCssExtractionStrategy (with a provided schema) to extract repeated patterns from the content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the pattern extraction functionality shown in the documentation's example by using CSS selectors to systematically extract structured data from repeated HTML elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy is used for pattern extraction with a provided schema, which aligns with the ground truth's explanation of using CSS selectors to extract structured data from repeated HTML elements.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation accesses the fit_markdown attribute from the crawl result object to obtain the main content in a formatted markdown structure.",
    "ground_truth_relationship": "The fit_markdown property from CrawlResult is shown being used in the documentation example to store and access the main extracted content from a webpage within the returned dictionary's main_content field.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that fit_markdown is used to store/access the main extracted content from a webpage in the example's returned dictionary",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The extracted_content attribute is accessed from the crawl results of both LLM and pattern extraction calls, serving as the container for structured and pattern-based extraction data.",
    "ground_truth_relationship": "The extracted_content field is used to store the crawled data in string format, which the example shows being parsed as JSON for both LLM and pattern-based extraction strategies.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that extracted_content stores crawled data as a string that is parsed as JSON for LLM and pattern-based extractions. The predicted version correctly identifies its role as a container for structured and pattern extraction data.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to use multiple output formats together:\n\n```python\nasync def crawl_content(url: str):\n    async with AsyncWebCrawler() as crawler:\n        # Extract main content with fit markdown\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=10,\n            exclude_external_links=True\n        )\n        \n        # Get structured data using LLM\n        llm_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=YourSchema.schema(),\n                instruction=\"Extract key information\"\n            )\n        )\n        \n        # Get repeated patterns (if any)\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(your_schema)\n        )\n        \n        return {\n            \"main_content\": result.fit_markdown,\n            \"structured_data\": json.loads(llm_result.extracted_content),\n            \"pattern_data\": json.loads(pattern_result.extracted_content),\n            \"media\": result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The media field is accessed from the crawl result to retrieve any media content gathered during the crawling process.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted media items which the documentation shows being returned as part of the final output in the \"media\" field of the crawl_content function's response.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly states that media content is accessed from the crawl result, which aligns with the ground truth showing media being returned as part of the function's output. The core relationship is captured even if less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet directly shows code examples where CosineStrategy is instantiated with various parameters (such as word_count_threshold, top_k, verbose, semantic_filter, sim_threshold, and max_dist). This explicit usage demonstrates best practices for configuring the strategy.",
    "ground_truth_relationship": "The documentation's recommended parameter values for different content types (articles, reviews, comments) are directly implemented in the CosineStrategy class through configurable parameters like word_count_threshold, sim_threshold, and max_dist which control text filtering and clustering behavior.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions focus on how CosineStrategy's configurable parameters enable customization for different content types and scenarios. The predicted description correctly identifies the key parameters being used, and while it is less specific about exact threshold values for different content types, it captures the core relationship between the documentation's recommendations and the class implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Adjust Thresholds Iteratively**\n   - Start with default values\n   - Adjust based on results\n   - Monitor clustering quality\n\n2. **Choose Appropriate Word Count Thresholds**\n   - Higher for articles (100+)\n   - Lower for reviews/comments (20+)\n   - Medium for product descriptions (50+)\n\n3. **Optimize Performance**\n   ```python\n   strategy = CosineStrategy(\n       word_count_threshold=10,  # Filter early\n       top_k=5,                 # Limit results\n       verbose=True             # Monitor performance\n   )\n   ```\n\n4. **Handle Different Content Types**\n   ```python\n   # For mixed content pages\n   strategy = CosineStrategy(\n       semantic_filter=\"product features\",\n       sim_threshold=0.4,      # More flexible matching\n       max_dist=0.3,          # Larger clusters\n       top_k=3                # Multiple relevant sections\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is defined as a subclass of ExtractionStrategy. Although the documentation does not mention ExtractionStrategy directly, its functionality is implicitly involved through inheritance, forming the foundational extraction framework upon which CosineStrategy is built.",
    "ground_truth_relationship": "The ExtractionStrategy class implements a flexible framework that supports the documented best practices by allowing configurable thresholds, word counts, and optimization parameters through its kwargs constructor parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies inheritance but misses the main point about ExtractionStrategy providing a configurable framework for best practices implementation",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "SlidingWindowChunking",
    "predicted_relationship": "The documentation snippet explicitly names 'SlidingWindowChunking' in the header and demonstrates its usage in the code example by importing and instantiating it, clearly indicating that this is the class that implements the sliding window chunking strategy.",
    "ground_truth_relationship": "The code implements the sliding window algorithm by using list slicing with window_size and step parameters to create overlapping text chunks, directly corresponding to the documentation's description of a sliding window approach for preserving context.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses only on naming/import aspects without explaining the core sliding window functionality, while the ground truth correctly describes the algorithmic relationship between code and documentation",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping chunks. Each chunk has a fixed length, and the window slides by a specified step size.\n\n#### When to Use\n- Ideal for creating overlapping chunks to preserve context.\n- Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n- `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n- `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import SlidingWindowChunking\n\nchunker = SlidingWindowChunking(window_size=10, step=5)\n\n# Sample text\ntext = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "While not directly mentioned in the text snippet, 'SlidingWindowChunking' is implemented as a subclass of 'ChunkingStrategy'. This inheritance is an implicit dependency that underpins the functionality described in the documentation.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the core interface through its chunk method that SlidingWindowChunking implements to create overlapping text segments using the sliding window approach.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between SlidingWindowChunking and ChunkingStrategy, even though it describes it more implicitly. Both descriptions convey that SlidingWindowChunking implements the abstract chunking interface defined by ChunkingStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation snippet directly shows the usage of 'result.fit_markdown' to obtain formatted markdown content, which is ideally suited for articles. This explicit reference indicates that the attribute 'fit_markdown' from the CrawlResult model is used to store or represent article content.",
    "ground_truth_relationship": "The CrawlResult.fit_markdown property implements the documented Best Practice #1 by providing an optional string property that stores content formatted specifically for blog posts and articles.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that fit_markdown is an optional string property used for storing formatted content specifically suited for articles and blog posts, aligning with Best Practice #1.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The code sample demonstrates filtering on 'result.media[\"images\"]', showing that the media attribute of the CrawlResult class is directly involved in handling media elements such as images.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property enables filtering and organizing media elements like images based on relevance scores as shown in the documentation's Best Practices section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that the media dictionary property allows filtering of media elements (specifically images) based on relevance scores. The predicted description correctly identifies the core relationship even though it doesn't mention the specific scoring threshold.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet uses 'result.links[\"internal\"]' to extract content links with context, directly referencing the 'links' attribute in the CrawlResult model which stores link data extracted during crawling.",
    "ground_truth_relationship": "The CrawlResult.links dictionary structure enables the filtering of internal content links as shown in the best practices documentation where links are filtered by type.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the CrawlResult.links structure allows filtering internal content links. The predicted description provides a concrete example of using result.links['internal'] which aligns with the ground truth's description of the links dictionary enabling content link filtering.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Use Fit Markdown for Articles**\n   ```python\n   # Perfect for blog posts, news articles, documentation\n   content = result.fit_markdown\n   ```\n\n2. **Handle Media Appropriately**\n   ```python\n   # Filter by relevance score\n   relevant_images = [\n       img for img in result.media[\"images\"]\n       if img['score'] > 5\n   ]\n   ```\n\n3. **Combine Link Analysis with Content**\n   ```python\n   # Get content links with context\n   content_links = [\n       link for link in result.links[\"internal\"]\n       if link['type'] == 'content'\n   ]\n   ```\n\n4. **Clean Content with Purpose**\n   ```python\n   # Customize cleaning based on your needs\n   result = await crawler.arun(\n       url=url,\n       word_count_threshold=20,      # Adjust based on content type\n       keep_data_attributes=False,   # Remove data attributes\n       process_iframes=True         # Include iframe content\n   )\n   ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation provides an example calling 'crawler.arun()' with several parameters for crawling and content cleaning. This explicitly maps to the AsyncWebCrawler.arun() method which is responsible for crawling operations including processing iframes and adjusting thresholds.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the documented best practices by accepting parameters like word_count_threshold to clean content, supporting media processing through screenshot capture, and handling link extraction through its processing pipeline.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method implements the documented functionality, including crawling operations and parameter handling. While it doesn't mention all features like media processing, it gets the core relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates CosineStrategy with parameters such as 'linkage_method', 'max_dist', and 'model_name' for custom clustering and multilingual support.",
    "ground_truth_relationship": "The CosineStrategy class implements advanced text clustering by combining semantic filtering, word count thresholds, and hierarchical clustering using cosine similarity, exactly matching the documented custom clustering and content filtering pipeline features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the custom clustering functionality but misses crucial details about the content filtering pipeline, word count thresholds, and sim_threshold parameters which are significant parts of the class's functionality according to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy extends the abstract base class ExtractionStrategy, thereby inheriting its extraction interface. This relationship is implicit and ensures that advanced extraction strategies follow a common contract.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core infrastructure for implementing specialized content extraction strategies like the CosineStrategy shown in the documentation, enabling features such as custom clustering and content filtering through its abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core inheritance relationship between ExtractionStrategy and CosineStrategy, and captures the essential idea that the base class provides an interface for specialized extraction strategies. While it doesn't mention specific features like clustering and filtering, it accurately describes the fundamental relationship structure.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet uses 'async with AsyncWebCrawler()' to create a crawler instance, demonstrating the usage of this class for asynchronous web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the core functionality for the documented content filtering pipeline by implementing an asynchronous web crawling mechanism with customizable extraction strategies, caching, and support for clustering through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in asynchronous web crawling but misses crucial aspects about customizable extraction strategies, content filtering pipeline, and clustering support mentioned in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Inside the asynchronous context, the method 'arun()' of AsyncWebCrawler is called with the CosineStrategy instance passed as the extraction_strategy, illustrating its role in the crawling and extraction pipeline.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the asynchronous execution engine that processes both custom clustering configurations and content filtering parameters defined in the CosineStrategy documentation, handling the extraction pipeline while managing caching, HTML processing, and error handling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic interaction between AsyncWebCrawler.arun() and CosineStrategy, but misses crucial aspects like custom clustering configurations, content filtering, caching, and error handling mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl operation, the extracted content is accessed via the 'extracted_content' attribute on the CrawlResult object. This field holds the clustering and extraction results that are then processed further.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the JSON-serialized clustering and filtering results that contain pricing features, similarity scores, and cluster information from the web crawler's execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that extracted_content is an attribute containing clustering and extraction results from the crawl operation, which aligns with the ground truth's explanation of it storing JSON-serialized clustering and filtering results.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Features\n\n### Custom Clustering\n```python\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n### Content Filtering Pipeline\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code snippet checks 'result.success' to determine if the crawling and extraction process was successful, thus using a key public attribute of CrawlResult as an outcome flag.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation's extraction function to validate whether pricing features were successfully extracted before processing the results.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that result.success is used as a boolean flag to validate successful extraction before processing the results",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet is titled 'Tips for Using JsonCssExtractionStrategy', directly naming this class. It advises users to inspect pages and test CSS selectors, which is part of the expected usage when employing this extraction strategy in the code.",
    "ground_truth_relationship": "The code implements a CSS-based extraction strategy that directly aligns with the documentation's tips by using BeautifulSoup selectors to parse HTML elements according to a predefined schema, which explains why the docs emphasize inspecting and testing CSS selectors before use.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the documentation provides tips for using JsonCssExtractionStrategy and recognizes the core relationship between the code's CSS selector functionality and the documentation's guidance on selector usage.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is derived from the ExtractionStrategy base class. Although not explicitly mentioned in the text, the underlying extraction mechanism is defined by ExtractionStrategy, connecting the tips with the broader design of extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing JSON/CSS extraction with error handling and parallel processing capabilities that the documentation's tips guide users to properly utilize.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship with ExtractionStrategy, but misses the crucial aspects of parallel processing and error handling capabilities that are central to the ground truth description",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Tips for Using JsonCssExtractionStrategy\n\n1. **Inspect the Page**: Use browser developer tools to identify the correct CSS selectors.\n2. **Test Selectors**: Verify your selectors in the browser console before using them in the script.\n3. **Handle Dynamic Content**: If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n4. **Error Handling**: Always check the `result.success` flag and handle potential failures.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The documentation advises developers to check the 'result.success' flag to handle potential failures. This directly corresponds to the 'success' attribute in the CrawlResult class, which indicates whether a crawl operation has been successful.",
    "ground_truth_relationship": "The success boolean flag in CrawlResult directly implements the error handling guidance from the documentation by providing a way to verify if extraction operations completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the success flag and error handling functionality described in the ground truth",
      "error_type": ""
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions and demonstrates the use of JsonCssExtractionStrategy for CSS-based data extraction. The Python example imports this class and shows its instantiation with a schema, clearly evidencing its role in the extraction process.",
    "ground_truth_relationship": "The code implements the documented CSS-based extraction by using BeautifulSoup's select() method to find elements matching the schema's baseSelector and extract specified fields from each matched element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's purpose for CSS-based extraction but omits the crucial implementation detail that it uses BeautifulSoup's select() method for the actual extraction process",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### [CSS-Based Extraction](css.md)\n\n`JsonCssExtractionStrategy` extracts data using CSS selectors. This is fast, reliable, and perfect for consistently structured pages.\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\n\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- E-commerce product listings\n- News article collections\n- Structured content pages\n- High-performance needs",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy inherits from ExtractionStrategy, establishing an implicit link as the abstract base that defines the extraction interface. Although not directly mentioned in the snippet, this base class is fundamental to the implementation of any extraction strategy including the CSS-based one.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core interface and parallel processing capabilities that JsonCssExtractionStrategy extends to implement the CSS-based extraction functionality described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core inheritance relationship between ExtractionStrategy and JsonCssExtractionStrategy, and accurately describes ExtractionStrategy as the abstract base class that defines the extraction interface. While it doesn't mention the parallel processing capabilities, this omission is minor since the main relationship is correctly understood.",
      "error_type": ""
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "TopicSegmentationChunking",
    "predicted_relationship": "The documentation snippet explicitly names and demonstrates the use of the 'TopicSegmentationChunking' class. The usage example in the documentation shows instantiating the class with a parameter (num_keywords=3) and calling its 'chunk' method, directly linking the documentation to this class implementation.",
    "ground_truth_relationship": "The code implements the TextTiling algorithm through NLTK's TextTilingTokenizer class while adding keyword extraction functionality based on term frequency, directly fulfilling the documentation's promise of topic-based text segmentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies the class and basic usage but misses discussing the key functionality of keyword extraction and TextTiling algorithm implementation, which are crucial aspects mentioned in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text into topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n- Perfect for long documents with distinct topics.\n- Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n- `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import TopicSegmentationChunking\n\nchunker = TopicSegmentationChunking(num_keywords=3)\n\n# Sample text\ntext = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Even though 'ChunkingStrategy' is not explicitly mentioned in the text snippet, 'TopicSegmentationChunking' extends it. This inheritance relationship implies that the abstract chunking functionality defined in 'ChunkingStrategy' underpins the implementation of 'TopicSegmentationChunking'.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the interface that TopicSegmentationChunking must implement through the chunk() method to perform text segmentation using the TextTiling algorithm.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between ChunkingStrategy and TopicSegmentationChunking, and understands that the abstract functionality defined in ChunkingStrategy is implemented by TopicSegmentationChunking.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a usage example by calling 'crawler.arun()' with parameters such as word_count_threshold, excluded_tags, and remove_overlay_elements. This directly matches the signature of AsyncWebCrawler.arun() which initiates the crawling process and returns the crawl result.",
    "ground_truth_relationship": "The arun() method implements the documented content cleaning features by accepting parameters like word_count_threshold and excluded_tags which control how text blocks are filtered and HTML elements are removed during the crawling process.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies the arun() method and its parameters, but misses the key cleaning functionality described in the ground truth. The ground truth specifically focuses on how arun() implements content cleaning features, while the predicted just describes the method signature.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The example code prints 'result.cleaned_html', clearly referencing the 'cleaned_html' attribute of the CrawlResult object. This attribute holds the cleaned HTML output derived after the crawling and cleaning process.",
    "ground_truth_relationship": "The cleaned_html Optional[str] property stores the sanitized HTML output after Crawl4AI applies its content cleaning steps including basic cleaning, content relevance checks, and layout analysis.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately explains that cleaned_html is an attribute containing cleaned HTML output after crawling, which aligns with the ground truth's description of it being a property storing sanitized HTML after content cleaning steps.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet demonstrates printing 'result.markdown', showing that the crawl result includes a markdown representation of the cleaned content. This directly traces to the markdown attribute in the CrawlResult data model.",
    "ground_truth_relationship": "The markdown property in CrawlResult provides a cleaned, text-based version of the crawled content after removing noise elements like ads and popups as described in the Content Cleaning documentation.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the markdown attribute and its presence in the result, but misses the key purpose of providing cleaned content after noise removal that's central to the ground truth's explanation",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Although not directly referenced by name in the snippet, the CrawlResult class is the container for the crawl output. Its attributes, such as 'cleaned_html' and 'markdown', are used to access the processed content, indicating that this data model is central to the content cleaning process.",
    "ground_truth_relationship": "The CrawlResult class stores the outputs of content cleaning operations through its cleaned_html, markdown, and fit_markdown fields that correspond to the different cleaning approaches described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies CrawlResult as the container for crawl outputs and specifically mentions cleaned_html and markdown attributes that store cleaned content, which aligns with the ground truth's description of the class storing content cleaning outputs.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Cleaning\n\n### Understanding Clean Content\nWhen crawling web pages, you often encounter a lot of noise - advertisements, navigation menus, footers, popups, and other irrelevant content. Crawl4AI automatically cleans this noise using several approaches:\n\n1. **Basic Cleaning**: Removes unwanted HTML elements and attributes\n2. **Content Relevance**: Identifies and preserves meaningful content blocks\n3. **Layout Analysis**: Understands page structure to identify main content areas\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Remove blocks with fewer words\n    excluded_tags=['form', 'nav'],  # Remove specific HTML tags\n    remove_overlay_elements=True    # Remove popups/modals\n)\n\n# Get clean content\nprint(result.cleaned_html)  # Cleaned HTML\nprint(result.markdown)      # Clean markdown version\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler by default instantiates an AsyncPlaywrightCrawlerStrategy to perform the actual crawling. This strategy implements the cleaning operations (for instance, by removing overlays when remove_overlay_elements=True), which is an essential part of generating clean content as described in the documentation.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the content cleaning functionality through its remove_overlay_elements method, which programmatically removes unwanted HTML elements like popups, modals, and cookie notices using JavaScript selectors that match the documented cleaning approaches.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly notes that AsyncPlaywrightCrawlerStrategy handles cleaning operations, but incorrectly implies that AsyncWebCrawler instantiates it by default, which isn't mentioned in the ground truth and adds unverified information about the class hierarchy.",
      "error_type": "added_incorrect_assumption"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and instantiates it with verbose=True within an async context. This establishes the starting point for the web crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality shown in the documentation through its arun() method, which processes URLs, handles caching, and supports the JsonCssExtractionStrategy for advanced schema extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's initialization and setup, but misses crucial functionality like URL processing, caching, and schema extraction that are central to the class's purpose as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet explicitly imports and instantiates JsonCssExtractionStrategy with a schema parameter (and verbose flag) to perform advanced extraction of complex product data.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class processes HTML content using a schema-based approach where it selects elements with BeautifulSoup and extracts structured data according to the field definitions shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that JsonCssExtractionStrategy uses a schema for extraction, but misses the key aspect of using BeautifulSoup for element selection and structured data extraction according to field definitions",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the async context, the snippet calls the arun() method on the AsyncWebCrawler instance to perform the crawling and extraction operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the asynchronous web crawling functionality showcased in the documentation by accepting extraction strategies, handling caching, and returning structured results that enable the documented JSON data extraction workflow.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic idea of using arun() for crawling, but misses crucial aspects about handling extraction strategies, caching, and returning structured results that are central to the method's functionality as described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code asserts the truth of result.success to ensure the crawl operation was successful. This attribute is a public interface member of the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult.success boolean property is used in the documentation example to validate that the crawling operation completed successfully before processing the extracted product data.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that result.success is a boolean property used to verify successful completion of the crawling operation before proceeding with data processing",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses result.extracted_content and then parses it as JSON. This attribute carries the structured output from the advanced extraction process.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the raw extracted data as a string that must be JSON-parsed to access the structured product information shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the key concept that extracted_content contains raw extracted data as a string that requires JSON parsing to access the structured information. The predicted description accurately conveys this relationship without any significant misunderstandings.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_complex_product_data():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=extraction_strategy,\n            bypass_cache=True,\n        )\n\n        assert result.success, \"Failed to crawl the page\"\n\n        product_data = json.loads(result.extracted_content)\n        print(json.dumps(product_data, indent=2))\n\nasyncio.run(extract_complex_product_data())\n```\n\nThis will produce a structured JSON output that captures the complex hierarchy of the product catalog, including nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy. Although not directly mentioned, this inheritance links the advanced extraction process to the common extraction interface defined by ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for specialized extraction strategies like JsonCssExtractionStrategy shown in the documentation, enabling structured data extraction from HTML content through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, which aligns with the ground truth's explanation of ExtractionStrategy as the foundation for specialized extraction strategies like JsonCssExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun(...)' with parameters such as 'word_count_threshold' and various filtering flags. This explicitly maps to the AsyncWebCrawler.arun() method, which accepts these keyword arguments and performs the filtering as part of its crawl process.",
    "ground_truth_relationship": "The documented content filtering parameters like word_count_threshold, excluded_tags, and link filtering options are implemented as optional kwargs in the arun() method and processed during HTML extraction and processing stages of the crawler.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts content filtering parameters as kwargs and uses them in the crawling process, which aligns with the ground truth's explanation of how these parameters are implemented and processed.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "In the documentation snippet, the variable 'crawler' is used to invoke the 'arun()' method. This implies that the crawler instance is an instance of the AsyncWebCrawler class, which provides the public interface used for content filtering.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements content filtering through its arun method by accepting parameters like word_count_threshold, excluded_tags, and various exclusion flags that control what content gets included in the final crawl result.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the AsyncWebCrawler class provides content filtering through arun(), but focuses too narrowly on the crawler instance/interface aspect while missing the core explanation of how the filtering functionality actually works through parameters.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Content Filtering\n\nControl what content is included or excluded:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n    \n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n    \n    # Link filtering\n    exclude_external_links=True,    # Remove external links\n    exclude_social_media_links=True,  # Remove social media links\n    \n    # Media filtering\n    exclude_external_images=True   # Remove external images\n)\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result of calling 'crawler.arun()' is stored in a variable named 'result'. This is of type CrawlResult, a data model that encapsulates the final filtered content (such as HTML, extracted content, media, etc.) returned by the crawler.",
    "ground_truth_relationship": "The CrawlResult class provides storage for filtered content by defining fields like cleaned_html, links, and media that hold the results after applying the documented content filtering parameters like excluded_tags and link filtering options.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that CrawlResult stores the filtered output from crawler.arun() with fields for content. While it's less detailed than the ground truth, it conveys the same core relationship between the class and filtered content storage.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', making this class the entry point for the crawling operation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the integrated JavaScript execution described in the documentation through its arun method, which accepts js_code and extracts content using the specified extraction_strategy while managing browser sessions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as an entry point for crawling operations, but misses the crucial aspect of its integrated JavaScript execution capabilities and extraction strategy implementation described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the example, the method arun() is called on the crawler instance (e.g., 'result = await crawler.arun(...)'). This clearly demonstrates that the AsyncWebCrawler.arun() method is used to execute a crawling task and retrieve results.",
    "ground_truth_relationship": "The arun() method implements the integrated JavaScript execution and waiting functionality by accepting js_code as a parameter through **kwargs which allows it to execute the documented JavaScript that handles pagination and content loading.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling and retrieving results, but misses the crucial aspect of integrated JavaScript execution and waiting functionality mentioned in the ground truth. It describes basic usage rather than the key relationship with JavaScript integration.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet creates an instance of JsonCssExtractionStrategy by passing a schema (e.g., extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)) to extract commit information. This demonstrates its direct use for content extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic that processes the HTML content according to the schema defined in the documentation's example, where it extracts commit information using the specified baseSelector and field selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core concept that JsonCssExtractionStrategy implements structured data extraction based on a schema with selectors, using the same commit extraction example. The predicted description, while more concise, correctly identifies its main purpose and usage.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example calls kill_session() through the crawler's strategy (i.e., 'await crawler.crawler_strategy.kill_session(session_id)'). This explicitly shows the use of a session termination method provided by AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The kill_session method is used at the end of the integrated crawling process to clean up browser resources by closing the page and context objects associated with the crawling session.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core purpose of kill_session() as a cleanup method for closing/terminating browser sessions and their associated resources. The predicted description accurately identifies the usage pattern through the crawler strategy, while the ground truth provides more implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 2: Integrated JavaScript Execution and Waiting Instead of using separate hooks, you can integrate the waiting logic directly into your JavaScript execution. This approach can be more concise and easier to manage for some scenarios. Here's an example: ```python async def integrated_js_and_wait_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"integrated_session\" all_commits = [] js_next_page_and_wait = \"\"\" (async () => { const getCurrentCommit = () => { const commits = document.querySelectorAll('li.commit-item h4'); return commits.length > 0 ? commits[0].textContent.trim() : null; }; const initialCommit = getCurrentCommit(); const button = document.querySelector('a.pagination-next'); if (button) button.click(); while (true) { await new Promise(resolve => setTimeout(resolve, 100)); const newCommit = getCurrentCommit(); if (newCommit && newCommit !== initialCommit) { break; } } })(); \"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page_and_wait if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(integrated_js_and_wait_crawl()) ``` This approach combines the JavaScript for clicking the \"next\" button and waiting for new content to load into a single script.",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "In the snippet, after invoking crawler.arun(), the returned object's extracted_content attribute is accessed (e.g., 'json.loads(result.extracted_content)'). This evidences its role as part of the public interface for obtaining extracted commit data.",
    "ground_truth_relationship": "The CrawlResult.extracted_content property stores the JSON-formatted commit data that was extracted from the webpage using the JsonCssExtractionStrategy schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that extracted_content is accessed from the crawler.arun() result and contains the extracted data, which aligns with the ground truth's explanation that it stores JSON-formatted commit data extracted via the strategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to the 'arun()' method on the crawler object, where parameters like 'screenshot' and 'screenshot_wait_for' are provided to enable and delay the screenshot capture.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements screenshot functionality by accepting a 'screenshot' boolean parameter and storing the captured screenshot data in the screenshot_data variable, which is later included in the CrawlResult object returned to the user.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of the arun() method accepting screenshot parameters and handling screenshot capture, even though it describes it from a usage perspective rather than implementation details.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "After crawling, the returned result (an instance of CrawlResult) contains a 'screenshot' attribute that holds the base64 encoded image, which the snippet checks and decodes.",
    "ground_truth_relationship": "The CrawlResult class enables screenshot functionality through its screenshot field which stores Base64-encoded image data that is captured according to the documented configuration parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that CrawlResult stores screenshot data in base64 format in its screenshot attribute. While it doesn't mention configuration parameters, this is a minor detail that doesn't affect the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncPlaywrightCrawlerStrategy implements the screenshot functionality through its 'take_screenshot' method. When the 'screenshot' flag is enabled in the arun() call, this method is invoked to capture the page screenshot with enhanced error handling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements screenshot functionality through its take_screenshot method, which captures a full-page base64-encoded image and includes error handling that generates a black error image if the screenshot fails.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality that AsyncPlaywrightCrawlerStrategy implements screenshot capabilities through the take_screenshot method, and that it's triggered when the screenshot flag is enabled. While it doesn't mention the base64 encoding or error image generation specifically, these are minor implementation details that don't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Screenshot Capabilities\n\nCapture page screenshots with enhanced error handling:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    screenshot=True,                # Enable screenshot\n    screenshot_wait_for=2.0        # Wait 2 seconds before capture\n)\n\nif result.screenshot:  # Base64 encoded image\n    import base64\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class that defines the 'take_screenshot' method. AsyncPlaywrightCrawlerStrategy inherits from this class, ensuring that the implementation complies with the defined interface for screenshot capture.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the take_screenshot() method that implements the documented screenshot capability, allowing derived crawler classes to capture page screenshots with customizable wait times.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify that AsyncCrawlerStrategy defines the take_screenshot abstract method that enables screenshot functionality for derived crawler classes. The predicted description captures the core relationship even though it omits details about wait times and customization.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports AsyncWebCrawler and uses it as a context manager (using 'async with') to initiate a crawling session.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the web crawling functionality demonstrated in the documentation by providing asynchronous methods to crawl URLs, process HTML content, and extract structured data using strategies like LLMExtractionStrategy.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used as a context manager, but misses the broader functionality of the class in terms of crawling, processing HTML, and extracting structured data that is central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls the 'arun()' method on an AsyncWebCrawler instance to perform the crawling task and to process extraction using an extraction strategy.",
    "ground_truth_relationship": "The example documentation shows a specific usage of AsyncWebCrawler.arun() to extract OpenAI model pricing data, where the code processes the URL with an LLMExtractionStrategy instance and handles caching, HTML processing, and error management to return a CrawlResult.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic function of arun() for crawling and extraction, but misses crucial aspects about handling caching, processing HTML, error management, and returning a CrawlResult that are essential parts of the ground truth relationship",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly imports and instantiates LLMExtractionStrategy to extract structured data from the crawled content based on a provided schema and instructions.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the structured data extraction functionality demonstrated in the documentation by using provider-based LLM models to parse HTML content according to a predefined Pydantic schema, as shown in the OpenAIModelFee example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of LLMExtractionStrategy as a class for extracting structured data using LLM models with schemas and instructions, which aligns with the ground truth's explanation of the class's implementation for parsing HTML content according to Pydantic schemas.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data (model names and their fees) from the OpenAI pricing page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_openai_fees():\n    url = 'https://openai.com/api/pricing/'\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=url,\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\", # Or use ollama like provider=\"ollama/nemotron\"\n                api_token=os.getenv('OPENAI_API_KEY'),\n                schema=OpenAIModelFee.model_json_schema(),\n                extraction_type=\"schema\",\n                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                            \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                            'One extracted model JSON format should look like this: '\n                            '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n            ),\n            bypass_cache=True,\n        )\n\n    model_fees = json.loads(result.extracted_content)\n    print(f\"Number of models extracted: {len(model_fees)}\")\n\n    with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_fees, f, indent=2)\n\nasyncio.run(extract_openai_fees())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends ExtractionStrategy, which forms the foundational extraction framework even though ExtractionStrategy itself is not directly instantiated in the example.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables concrete implementations like LLMExtractionStrategy to extract structured data from web content, as demonstrated in the documentation example with OpenAI model fees.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy, but misses the crucial aspect of the base class providing core interface and parallel processing functionality that enables structured data extraction",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(url=\"https://example.com\")', making an explicit reference to the 'AsyncWebCrawler.arun()' method responsible for initiating a crawl.",
    "ground_truth_relationship": "The code implements error handling by returning a CrawlResult object with success and error_message fields that can be checked exactly as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses solely on the method call syntax, while missing the core relationship about error handling pattern shown in the documentation. However, it does correctly identify the arun() method being referenced.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Although not mentioned by name in the snippet, the returned object from 'crawler.arun()' is used with attributes like 'success', 'error_message', and 'status_code', indicating it is an instance of the 'CrawlResult' class.",
    "ground_truth_relationship": "The CrawlResult class provides the necessary success, error_message, and status_code fields that enable error handling as demonstrated in the documentation's code example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the CrawlResult class provides the attributes (success, error_message, status_code) used in the error handling example, even though it phrases it more indirectly.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The code snippet explicitly checks 'if not result.success:' to determine if the crawl was unsuccessful, directly using this attribute from the CrawlResult object.",
    "ground_truth_relationship": "The boolean success property in CrawlResult enables error handling by providing a simple way to check if a crawl operation completed successfully, as demonstrated in the documentation's error handling example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the success boolean is used for error handling by checking if a crawl was unsuccessful, aligning with the ground truth's explanation of using the success property to verify successful completion.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "The snippet prints out the error message using 'result.error_message', which explicitly accesses this public attribute to report the reason for a crawl failure.",
    "ground_truth_relationship": "The error_message field in CrawlResult stores the failure reason shown in the documentation's error handling example when crawls are unsuccessful.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that error_message is used to store and display failure reasons when crawls are unsuccessful, with the predicted description accurately describing how the error message is accessed and used",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Errors\n\nAlways check if the crawl was successful:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```",
    "artifact_title": "CrawlResult.status_code",
    "predicted_relationship": "The status code is explicitly printed with 'print(f\"Status code: {result.status_code}\")' to provide HTTP status details when a crawl fails, demonstrating its use in error diagnostics.",
    "ground_truth_relationship": "The status_code field stores HTTP response codes from crawl attempts, enabling error handling as shown in the documentation where failed crawls can be diagnosed using result.status_code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that status_code is used for error diagnostics in failed crawls, which aligns with the ground truth's explanation of using status_code for error handling and crawl diagnostics.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet contains a code example that directly calls 'crawler.arun()' using an await statement. This indicates that the AsyncWebCrawler.arun() method is explicitly used to perform a crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes HTML content asynchronously and converts it to markdown format, with options to include links as shown in the documentation example through the kwargs parameter passed to aprocess_html().",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies arun() as an async method used for crawling, but misses the crucial markdown conversion functionality that is central to the ground truth description",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Standard Markdown\n\nHTML converted to clean markdown format. Great for:\n- Content analysis\n- Documentation\n- Readability\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    include_links_on_markdown=True  # Include links in markdown\n)\nprint(result.markdown)\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet prints 'result.markdown', which points to the 'markdown' attribute of the CrawlResult class. This attribute is used to hold the clean markdown output, and its direct reference in the example makes the connection explicit.",
    "ground_truth_relationship": "The markdown property on CrawlResult stores the HTML-to-Markdown converted text output as an optional string value that can be accessed after crawling a webpage.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the markdown attribute holds clean markdown output, and while it doesn't specifically mention HTML-to-Markdown conversion, it captures the core relationship of markdown being an attribute that stores the markdown content.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an async context manager 'async with AsyncWebCrawler(verbose=True) as crawler:' which explicitly instantiates the AsyncWebCrawler class to perform crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class provides the foundational infrastructure for implementing custom execution hooks through its crawler_strategy attribute, which can be set and accessed during crawling operations as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies AsyncWebCrawler as an async context manager but misses the key purpose of supporting custom execution hooks through crawler_strategy, which is the main focus of the ground truth relationship.",
      "error_type": "key_feature_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the loop, the snippet calls 'crawler.arun(...)' to perform crawling on each page. This demonstrates explicit use of the arun() method on the AsyncWebCrawler instance.",
    "ground_truth_relationship": "The arun() method implements the core asynchronous crawling functionality that enables custom hooks like 'on_execution_started' to be executed during the crawling process through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling but misses the crucial aspect of how it enables custom hook execution through crawler_strategy, which is a key part of the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.set_hook()",
    "predicted_relationship": "The snippet explicitly calls 'crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)' to set a custom hook for handling execution events. This directly demonstrates the usage of the set_hook() method.",
    "ground_truth_relationship": "The set_hook method enables the custom hook functionality described in the documentation by storing the provided hook callback in a dictionary that gets executed at specific points in the crawling lifecycle.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies the core usage of set_hook for setting custom hooks, but misses the crucial implementation detail about storing hooks in a dictionary for lifecycle execution that the ground truth emphasizes",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "At the end of the crawling process, the snippet calls 'await crawler.crawler_strategy.kill_session(session_id)' to terminate the session. This is an explicit invocation of the kill_session() method.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects associated with a specific session ID, as demonstrated in the documentation's crawler cleanup after commit crawling.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description accurately identifies that kill_session is called to terminate a session, but misses the crucial aspect of the cleanup process (closing page and context objects) that the ground truth emphasizes",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly named in the snippet, AsyncWebCrawler instantiates a default crawler_strategy which is an instance of AsyncPlaywrightCrawlerStrategy. This class underpins the methods (set_hook and kill_session) that are explicitly called.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements custom hooks through its set_hook() and execute_hook() methods, which enable execution of custom functions at specific stages of the crawling process like 'on_execution_started' as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the relationship between AsyncPlaywrightCrawlerStrategy and hook functionality, but introduces a potential misunderstanding by focusing on AsyncWebCrawler's instantiation rather than directly describing how AsyncPlaywrightCrawlerStrategy implements the hook system.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of the crawling process. This is particularly useful for handling complex loading scenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n```python\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = commit.strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/example/repo/commits/main\"\n        session_id = \"commit_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a.pagination-next');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                css_selector=\"li.commit-item\",\n                js_code=js_next_page if page > 0 else None,\n                bypass_cache=True,\n                js_only=page > 0\n            )\n\n            commits = result.extracted_content.select(\"li.commit-item\")\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\nThis technique uses a custom `on_execution_started` hook to ensure new content has loaded before proceeding to the next step.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, which is implicitly used by AsyncWebCrawler, extends AsyncCrawlerStrategy. This shows the inheritance relationship where AsyncCrawlerStrategy defines the abstract interface for hooks and session management.",
    "ground_truth_relationship": "The set_hook method in AsyncCrawlerStrategy enables the documented custom hook functionality by allowing users to register callback functions like on_execution_started that execute at specific crawling stages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on inheritance relationships between AsyncCrawlerStrategy and AsyncPlaywrightCrawlerStrategy, while the ground truth describes the functional purpose of set_hook for registering callback functions. Though both discuss the strategy class, they emphasize different aspects of its functionality.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows a call to 'crawler.arun()' with the parameters 'simulate_user=True' and 'override_navigator=True'. This directly maps to the AsyncWebCrawler.arun() method, as seen in Artifact 5, which accepts these parameters to control anti-bot behavior.",
    "ground_truth_relationship": "The arun() method accepts keyword arguments like simulate_user and override_navigator which enable fine-grained control over anti-bot features as documented in the Manual Anti-Bot Options section.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the arun() method accepts keyword arguments for configuring anti-bot features, with the predicted description correctly referencing the actual method and its parameters shown in the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Manual Anti-Bot Options\n\nWhile Magic Mode is recommended, you can also configure individual anti-detection features:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True    # Mask automation signals\n)\n```\n\nNote: When `magic=True` is used, you don't need to set these individual options.\n",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Inside the crawl() method of AsyncPlaywrightCrawlerStrategy (Artifact 1), the options 'simulate_user', 'override_navigator', and 'magic' are checked to inject anti-detection scripts. Although these features are not directly mentioned in the snippet, they are implicitly used by the crawler strategy when the arun() method propagates these parameters.",
    "ground_truth_relationship": "The code implements manual anti-bot options through parameters like simulate_user and override_navigator in the crawl method, which inject scripts to override navigator properties and simulate user interactions when these flags are set to true.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly explain that the code implements anti-bot options through parameters like simulate_user and override_navigator that inject scripts and simulate user behavior. The predicted description accurately captures the core functionality, even if it phrases it slightly differently.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy and instantiates it with a schema to extract repeated content patterns such as product listings or news feeds.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based content extraction by using a schema definition to recursively select and extract nested data from repeating HTML elements using CSS selectors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the core purpose of using JsonCssExtractionStrategy with a schema for pattern extraction, but misses the crucial aspect of recursive/nested data extraction capability mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on the crawler instance, passing the instantiated JsonCssExtractionStrategy. This demonstrates how the extraction strategy is integrated into the crawling process.",
    "ground_truth_relationship": "The arun() method processes the JsonCssExtractionStrategy schema by accepting an extraction_strategy parameter and applying it to crawled HTML content to extract structured data according to the defined selectors and field patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun accepts and uses the JsonCssExtractionStrategy, but misses the crucial aspect of how it processes the schema to extract structured data according to defined selectors and patterns",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Pattern-Based Selection\n\nFor repeated content patterns (like product listings, news feeds):\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"News Articles\",\n    \"baseSelector\": \"article.news-item\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n        {\"name\": \"category\", \"selector\": \".category\", \"type\": \"text\"},\n        {\n            \"name\": \"metadata\",\n            \"type\": \"nested\",\n            \"fields\": [\n                {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n            ]\n        }\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticles = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is a subclass of ExtractionStrategy, meaning it implements the abstract extraction interface defined by ExtractionStrategy. This inheritance is implicit in the usage even though the base class is not directly mentioned in the snippet.",
    "ground_truth_relationship": "The ExtractionStrategy base class defines the core interface and parallel processing logic that enables pattern-based content extraction, which concrete implementations like JsonCssExtractionStrategy use to extract structured data from repeated HTML elements as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between JsonCssExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of parallel processing and the core purpose of pattern-based content extraction that the base class enables",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates invoking the 'arun' method on the crawler instance (i.e. 'crawler.arun(...)'). This call is directly shown in the usage example for crawling a URL with a specified extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the documented combining strategies pattern by accepting an extraction_strategy parameter that allows different strategies to be passed in sequentially as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling with strategies, but misses the key relationship that it specifically enables combining different strategies sequentially as shown in the documentation.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the example, the variable 'css_strategy' is passed as the extraction_strategy argument. Although not named explicitly in the snippet, 'css_strategy' implies a CSS-based extraction strategy, which maps to the 'JsonCssExtractionStrategy' artifact that implements extraction using CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements one part of the documented combination approach by using CSS selectors to extract structured data from HTML, which can then be combined with other strategies like LLM processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as using CSS selectors for extraction, but misses the key point about it being one part of a larger combination approach with other strategies like LLM processing, as described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining Strategies\n\nYou can combine strategies for more powerful extraction:\n\n```python\n# First use CSS strategy for initial structure\ncss_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=css_strategy\n)\n\n# Then use LLM for semantic analysis\nllm_result = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=llm_strategy\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The example uses 'llm_strategy' as the extraction_strategy parameter in a separate call. This implies the use of a language model based extraction approach, which corresponds to the 'LLMExtractionStrategy' artifact.",
    "ground_truth_relationship": "The LLMExtractionStrategy class enables the documented functionality of combined crawling strategies by providing a specialized extraction method that can process content after initial CSS-based extraction, using LLM models for semantic analysis through its extract() and run() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic concept of using LLM-based extraction, but misses the key aspect of combining strategies and using LLM specifically for semantic analysis after CSS-based extraction",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly demonstrates the instantiation and usage of LLMExtractionStrategy in the code sample, where it is imported and used to extract structured data with parameters for provider, schema, and instruction.",
    "ground_truth_relationship": "The code implements the documented LLM extraction functionality by initializing a strategy with provider, schema, and instruction parameters, then using these to process HTML content through language models to extract structured data according to the specified schema or instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses mainly on the initialization and usage example, while missing the key implementation detail that the strategy processes HTML content through language models. It only shows the surface-level interface without capturing the core extraction functionality.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the abstract base class ExtractionStrategy. While ExtractionStrategy is not explicitly mentioned in the snippet, it is implicitly involved as the inheritance contract for all extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the core infrastructure that enables the documented LLMExtractionStrategy to implement flexible content extraction through language models by defining required extract() and run() methods that process HTML content into structured data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, but misses the crucial aspect of how ExtractionStrategy provides the infrastructure for content extraction through its defined methods that LLMExtractionStrategy implements.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### [LLM-Based Extraction](llm.md)\n\n`LLMExtractionStrategy` uses Language Models to extract structured data from web content. This approach is highly flexible and can understand content semantically.\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/llama2\",\n    schema=Product.schema(),\n    instruction=\"Extract product details from the page\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/product\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Complex data structures\n- Content requiring interpretation\n- Flexible content formats\n- Natural language processing",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code sample calls 'crawler.arun()', indicating that the asynchronous web crawler's arun() method is used to execute the extraction process with the provided extraction strategy. This method integrates the user-specified extraction strategy into the crawling workflow.",
    "ground_truth_relationship": "The arun() method implements the core crawling logic that enables the LLMExtractionStrategy to process web content by handling URL fetching, caching, and passing the retrieved HTML to the specified extraction strategy for structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that arun() executes the crawler with the extraction strategy, while the ground truth elaborates with more implementation details. The main concept is aligned even though the predicted version is less detailed.",
      "error_type": ""
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly mentions 'JsonCssExtractionStrategy' as the strategy used to extract structured data via CSS selectors. This directly maps to the class which implements this functionality.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the documented functionality by using BeautifulSoup to process HTML and extract data through CSS selectors defined in a schema, where baseSelector finds repeating elements and fields specify what to extract from each element.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy as using CSS selectors for data extraction, but misses crucial implementation details about using BeautifulSoup and the schema structure with baseSelector and fields that are central to how it works",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet instructs on using the JsonCssExtractionStrategy with the AsyncWebCrawler, indicating that AsyncWebCrawler is a primary consumer of extraction strategies within the Crawl4AI framework.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements JsonCssExtractionStrategy by checking for this strategy type in the aprocess_html method and executing its run() method to extract structured data using CSS selectors from the crawled HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler using JsonCssExtractionStrategy, but incorrectly presents it as a primary consumer when the code shows it's just one of several possible extraction strategies handled conditionally in aprocess_html",
      "error_type": "overstated_importance"
    }
  },
  {
    "document_text": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows you to extract structured data from web pages using CSS selectors. This method is particularly useful when you need to extract specific data points from a consistent HTML structure, such as tables or repeated elements. Here's how to use it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies:\n1. A base CSS selector for the repeating elements\n2. Fields to extract from each element, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services like LLMs for extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although the snippet does not directly mention 'ExtractionStrategy', the JsonCssExtractionStrategy extends this abstract base class, which is a fundamental part of the extraction framework.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing JSON CSS extraction through its abstract extract method and parallel processing run method, which aligns with the documentation's description of structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy extends ExtractionStrategy, but misses the key functionality aspects described in the ground truth about structured data extraction and parallel processing capabilities.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly demonstrates usage of the arun() method by calling 'crawler.arun(url=\"https://example.com\")'. This shows that the method is responsible for returning the complete raw HTML of the webpage, as described in the documentation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the raw HTML retrieval functionality by fetching unmodified webpage content and storing it in the CrawlResult.html property, which can be accessed as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the arun() method is responsible for retrieving and returning raw HTML from webpages, which aligns with the ground truth's explanation of the functionality. Both descriptions focus on the same core relationship between the method and its HTML retrieval purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Raw HTML\n\nOriginal, unmodified HTML from the webpage. Useful when you need to:\n- Preserve the exact page structure\n- Process HTML with your own tools\n- Debug page issues\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.html)  # Complete HTML including headers, scripts, etc.\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "The code snippet prints 'result.html', accessing the 'html' attribute from the CrawlResult object. This attribute holds the complete raw HTML output returned by the crawl process, which matches the documentation's reference to 'Raw HTML'.",
    "ground_truth_relationship": "The CrawlResult.html property stores the complete unmodified HTML content from a crawled webpage as a string, allowing access to the raw markup for custom processing or debugging.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the html attribute contains the complete raw HTML content from the crawled webpage, matching the ground truth's core explanation of storing unmodified HTML as a string",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The code snippet directly instantiates AsyncWebCrawler using an async context manager with various browser and identity configuration parameters (browser_type, headless, verbose, user_agent, headers, and proxy).",
    "ground_truth_relationship": "The AsyncWebCrawler class implements all the configuration options shown in the documentation example, including browser setup, identity management, proxy configuration, content handling, timing controls, and anti-detection features through its __init__ and arun methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic instantiation of AsyncWebCrawler with configuration parameters, but misses significant functionality described in the ground truth, particularly the content handling, timing controls, and anti-detection features implemented through the arun method.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet directly calls the 'arun' method on the AsyncWebCrawler instance to perform the crawling process with specific keyword arguments for dynamic content, timing, and anti-detection measures.",
    "ground_truth_relationship": "The documented example demonstrates how to use the arun() method with various configuration options, while the actual code implementation shows the core logic for processing these configurations through error handling, caching, and content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic usage of arun() for crawling, but misses the key aspect that the code shows the core implementation logic (caching, error handling, content extraction) while the documentation demonstrates configuration usage examples",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.screenshot",
    "predicted_relationship": "The example retrieves the 'screenshot' attribute from the returned result, representing the captured image of the crawled page.",
    "ground_truth_relationship": "The CrawlResult.screenshot property stores the base64-encoded screenshot data that is requested through the screenshot=True parameter in the crawl_with_advanced_config example function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that screenshot is an attribute of the result, but misses the important detail that it's base64-encoded data and that it's triggered by the screenshot=True parameter",
      "error_type": "missing_key_details"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks the 'success' attribute of the CrawlResult to determine whether the crawling operation succeeded.",
    "ground_truth_relationship": "The success boolean field in CrawlResult is returned as part of the final dictionary in the comprehensive example to indicate whether the crawling operation completed successfully.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that the success boolean indicates whether the crawling operation completed successfully. The predicted description is more concise but captures the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler internally instantiates AsyncPlaywrightCrawlerStrategy as its default crawler strategy when no explicit strategy is provided. The configuration parameters passed to AsyncWebCrawler are relayed to AsyncPlaywrightCrawlerStrategy, which handles the actual crawling logic.",
    "ground_truth_relationship": "The example in the documentation demonstrates how to use key features implemented in the AsyncPlaywrightCrawlerStrategy class, such as browser configuration, proxy setup, content handling, and anti-detection mechanisms through a comprehensive crawl_with_advanced_config function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that AsyncPlaywrightCrawlerStrategy is used for crawling, but incorrectly suggests it is only instantiated internally by AsyncWebCrawler. The ground truth shows it's a standalone class whose features can be used directly or through AsyncWebCrawler.",
      "error_type": "incorrect_relationship_assumption"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine various browser configurations:\n\n```python\nasync def crawl_with_advanced_config(url: str):\n    async with AsyncWebCrawler(\n        # Browser setup\n        browser_type=\"chromium\",\n        headless=True,\n        verbose=True,\n        \n        # Identity\n        user_agent=\"Custom User Agent\",\n        headers={\"Accept-Language\": \"en-US\"},\n        \n        # Proxy setup\n        proxy=\"http://proxy.example.com:8080\"\n    ) as crawler:\n        result = await crawler.arun(\n            url=url,\n            # Content handling\n            process_iframes=True,\n            screenshot=True,\n            \n            # Timing\n            page_timeout=60000,\n            delay_before_return_html=2.0,\n            \n            # Anti-detection\n            magic=True,\n            simulate_user=True,\n            \n            # Dynamic content\n            js_code=[\n                \"window.scrollTo(0, document.body.scrollHeight);\",\n                \"document.querySelector('.load-more')?.click();\"\n            ],\n            wait_for=\"css:.dynamic-content\"\n        )\n        \n        return {\n            \"content\": result.markdown,\n            \"screenshot\": result.screenshot,\n            \"success\": result.success\n        }\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract base class AsyncCrawlerStrategy, which defines the standardized interface for crawling strategies. This inheritance establishes the architectural foundation for all crawler strategies used by AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that enable the comprehensive browser configurations shown in the documentation example, including crawling, screenshot capture, and user agent management.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class that defines an interface, but incorrectly specifies AsyncPlaywrightCrawlerStrategy as the inheriting class, which isn't shown in the code. The core interface concept is right but introduces an unsupported implementation detail.",
      "error_type": "unsupported_implementation_detail"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The E-commerce Scraping use case shows a CSS-based extraction approach by defining a schema with CSS selectors. Although the snippet does not explicitly instantiate a class, this pattern aligns with the design of JsonCssExtractionStrategy, which uses a schema (including 'baseSelector' and 'fields') to extract product listing data.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS-based e-commerce scraping example from the documentation by processing HTML elements according to a schema that defines base selectors and field mappings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements a CSS-based extraction approach using a schema with selectors, matching the e-commerce scraping example from the documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The News Article Extraction example directly creates an instance of LLMExtractionStrategy by invoking its constructor with parameters such as a provider and a schema derived from an Article model. This explicit instantiation demonstrates its intended use for LLM-based content extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class demonstrated in the documentation implements schema-based extraction for articles by accepting a provider and schema parameter in its initialization, which directly corresponds to the 'News Article Extraction' use case shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that LLMExtractionStrategy implements schema-based extraction through initialization with provider and schema parameters, specifically for article content extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "In the Content Analysis example, the snippet explicitly instantiates CosineStrategy with parameters such as 'semantic_filter' and 'top_k'. This demonstrates its role in performing topic analysis using cosine similarity metrics.",
    "ground_truth_relationship": "The CosineStrategy class implements content analysis functionality by using cosine similarity and semantic filtering to cluster and analyze text content, as shown in the documentation's third example where it filters content based on 'technology trends' and returns top_k results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core functionality of CosineStrategy as a content analysis tool using cosine similarity metrics, matching the ground truth's explanation of its clustering and filtering capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Common Use Cases\n\n1. **E-commerce Scraping**\n   ```python\n   # CSS Strategy for product listings\n   schema = {\n       \"name\": \"Products\",\n       \"baseSelector\": \".product\",\n       \"fields\": [\n           {\"name\": \"name\", \"selector\": \".title\", \"type\": \"text\"},\n           {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n       ]\n   }\n   ```\n\n2. **News Article Extraction**\n   ```python\n   # LLM Strategy for article content\n   class Article(BaseModel):\n       title: str\n       content: str\n       author: str\n       date: str\n\n   strategy = LLMExtractionStrategy(\n       provider=\"ollama/llama2\",\n       schema=Article.schema()\n   )\n   ```\n\n3. **Content Analysis**\n   ```python\n   # Cosine Strategy for topic analysis\n   strategy = CosineStrategy(\n       semantic_filter=\"technology trends\",\n       top_k=5\n   )\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not directly mentioned in the documentation snippet, ExtractionStrategy is the abstract base class that underpins all the extraction strategies (JsonCssExtractionStrategy, LLMExtractionStrategy, and CosineStrategy) demonstrated in the examples.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as the foundation for the three documented use cases (CSS, LLM, and Cosine strategies) by providing a common interface for extracting structured data through its abstract extract() method and parallel processing capabilities in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as the abstract base class that underlies all the example strategies shown in the documentation (CSS, LLM, and Cosine). While it doesn't explicitly mention the extract() method and parallel processing capabilities, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using the 'async with' statement. This class is used to initiate the web crawling process that combines JavaScript execution with LLM extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented dynamic content extraction functionality through its arun method, which accepts js_code and wait_for parameters to execute JavaScript on web pages and coordinates with extraction strategies for content processing.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for web crawling and mentions the async with usage, but misses the crucial aspect of JavaScript execution functionality and extraction strategy coordination that's central to the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the method 'arun()' of the AsyncWebCrawler instance is explicitly called with parameters such as js_code, wait_for, css_selector, and an extraction_strategy, demonstrating its role in processing dynamic content.",
    "ground_truth_relationship": "The documented example showcases how the AsyncWebCrawler.arun() method handles dynamic web content by accepting optional parameters like js_code, wait_for, and css_selector that allow JavaScript execution and selective content extraction through the LLMExtractionStrategy.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the arun() method handles dynamic web content through parameters like js_code, wait_for, and css_selector, and works with extraction strategies. The predicted description captures the core functionality even if it's slightly less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The code snippet explicitly instantiates LLMExtractionStrategy to define the extraction strategy, passing parameters like provider, api_token, and an instruction for summarization. This strategy is used to perform LLM-based content summarization.",
    "ground_truth_relationship": "The code implements the LLMExtractionStrategy class that enables the functionality shown in the documentation's example of extracting dynamic content from web pages using LLM-based extraction with customizable providers, API tokens, and instructions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that LLMExtractionStrategy handles LLM-based extraction with parameters, it fails to capture that this is a class implementation that enables the functionality shown in the documentation example. It presents it more as an instantiation rather than a class definition.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy extends the ExtractionStrategy base class, which provides the general interface for extraction strategies. Although ExtractionStrategy is not directly mentioned in the snippet, it is implicitly part of the extraction process being used.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundation for implementing different content extraction methods, including the LLMExtractionStrategy shown in the documentation example that uses GPT-4 to summarize articles.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy as a base class and that it provides a general interface for extraction strategies. This aligns with the ground truth which describes ExtractionStrategy as the foundation for different content extraction methods, including LLMExtractionStrategy.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM extraction to handle dynamic content:\n\n```python\nasync def extract_dynamic_content():\n    js_code = \"\"\"\n    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n    if (loadMoreButton) {\n        loadMoreButton.click();\n        await new Promise(resolve => setTimeout(resolve, 2000));\n    }\n    \"\"\"\n\n    wait_for = \"\"\"\n    () => {\n        const articles = document.querySelectorAll('article.tease-card');\n        return articles.length > 10;\n    }\n    \"\"\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            wait_for=wait_for,\n            css_selector=\"article.tease-card\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Summarize each article, focusing on technology-related content\"\n            ),\n            bypass_cache=True,\n        )\n\n    summaries = json.loads(result.extracted_content)\n    print(f\"Number of summarized articles: {len(summaries)}\")\n\n    with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summaries, f, indent=2)\n\nasyncio.run(extract_dynamic_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the crawl operation, the returned CrawlResult object's 'extracted_content' property is accessed to obtain the summarized content. This property is part of the public interface of the CrawlResult class.",
    "ground_truth_relationship": "The extracted_content field stores the LLM-processed article summaries that are later parsed as JSON in the example documentation, serving as the bridge between raw crawled data and structured output.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies extracted_content as a property for accessing content, but misses the key aspect that it specifically stores LLM-processed article summaries as JSON data",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using an async context manager (async with AsyncWebCrawler(verbose=True) as crawler), demonstrating its usage for web crawling.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with methods like arun() that directly support all the documented functionality shown in the example, including content filtering, processing, and cache control through corresponding parameters.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its async context manager usage, but it misses significant functionality described in the ground truth, particularly the core implementation details about content filtering, processing, and cache control that are central to the class's purpose.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the sample code, the arun() method of the AsyncWebCrawler instance is called (result = await crawler.arun(...)). This demonstrates how the crawling process is initiated.",
    "ground_truth_relationship": "The documented example demonstrates usage patterns of AsyncWebCrawler.arun() by showing how its various parameters like word_count_threshold, bypass_cache, and other content filtering options are implemented in the actual method definition through parameter validation, cache checking, and content processing logic.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() but misses the crucial connection between the documented parameters and their implementation in the method definition, which is the core relationship highlighted in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The result returned from the arun() method is an instance of CrawlResult. Although not directly instantiated in the snippet, its presence is implicit because its attributes are used later in the code.",
    "ground_truth_relationship": "The CrawlResult class defines the data structure that holds all the crawling outputs shown in the example code, including the success status, markdown content, media items, and links that are accessed in the documented example's result handling section.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult is the return type from arun() and its attributes are used in the example code, which aligns with the ground truth's explanation of CrawlResult being the data structure holding crawling outputs used in the example.",
      "error_type": null
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet explicitly accesses the 'markdown' attribute of the CrawlResult instance (result.markdown[:500]) to display the cleaned content.",
    "ground_truth_relationship": "The CrawlResult.markdown field contains the cleaned content from the web crawl which is accessed in the example code via result.markdown[:500] to display the first 500 characters of crawled content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the markdown attribute of the CrawlResult is used to access the cleaned content via result.markdown[:500]. While the ground truth provides slightly more context about what the markdown field contains, the predicted description captures the essential usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The sample code iterates over result.media[\"images\"] to process images, which indicates the use of the 'media' attribute from the CrawlResult object.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores extracted media elements like images, which can then be accessed and processed as shown in the example documentation where image sources are printed using result.media['images'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that the media dictionary attribute stores and allows access to image data, as demonstrated in the example code through result.media['images']",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The documentation snippet processes internal links by iterating over result.links[\"internal\"], directly demonstrating the use of the 'links' attribute from the CrawlResult.",
    "ground_truth_relationship": "The CrawlResult.links dictionary attribute stores categorized links ('internal' and 'external') that were found during crawling, as demonstrated in the example where internal links are accessed via result.links['internal'].",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic fact that links are accessed through result.links['internal'], but omits the important aspect that links are categorized into both internal and external links in the dictionary structure",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.error_message",
    "predicted_relationship": "In the error handling branch of the sample code, result.error_message is used to report crawl failures, indicating direct usage of the 'error_message' attribute from the CrawlResult.",
    "ground_truth_relationship": "The error_message field in CrawlResult enables error handling in the example code by providing the failure reason when result.success is False.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that error_message is used to convey failure reasons during error handling when the crawl is unsuccessful. The predicted description focuses on the specific code usage while the ground truth provides more context about success checking, but the fundamental relationship described is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Complete Example\n\nHere's a more comprehensive example showing common usage patterns:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            # Content filtering\n            word_count_threshold=10,\n            excluded_tags=['form', 'header'],\n            exclude_external_links=True,\n            \n            # Content processing\n            process_iframes=True,\n            remove_overlay_elements=True,\n            \n            # Cache control\n            bypass_cache=False  # Use cache if available\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.success",
    "predicted_relationship": "The snippet checks if the crawl was successful using result.success before processing and displaying the content, which shows direct usage of the 'success' attribute in the CrawlResult.",
    "ground_truth_relationship": "The success boolean property in CrawlResult is used to check if the crawl operation completed successfully before processing the extracted content, media and links from the crawled page.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture that success is a boolean used to verify if the crawl operation completed successfully before processing the results. The predicted version accurately describes the core relationship and usage pattern shown in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "FixedLengthWordChunking",
    "predicted_relationship": "The documentation snippet explicitly names FixedLengthWordChunking and provides a usage example which demonstrates how to instantiate it with a parameter (chunk_size). This directly corresponds to the FixedLengthWordChunking class implementation in the code.",
    "ground_truth_relationship": "The code implements the documented chunking strategy by splitting input text into word tokens and using list slicing with the chunk_size parameter to create fixed-length segments joined back into strings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the class name and parameter usage but misses the core functionality of splitting text into fixed-length word chunks which is the main relationship described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of words. This ensures each chunk has approximately the same length.\n\n#### When to Use\n- Suitable for processing large texts where uniform chunk size is important.\n- Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n- `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import FixedLengthWordChunking\n\nchunker = FixedLengthWordChunking(chunk_size=10)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks of fixed length.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "FixedLengthWordChunking extends the ChunkingStrategy base class. Although the documentation does not directly mention ChunkingStrategy, this inheritance relationship is implicit in FixedLengthWordChunking's design and its expected behavior of a chunking strategy.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that FixedLengthWordChunking implements to provide word-based text chunking functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that FixedLengthWordChunking extends/implements the ChunkingStrategy base class, which aligns with the ground truth's description of their inheritance relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports 'AsyncWebCrawler' and uses it with an async context manager, demonstrating its instantiation and usage for session-based crawling.",
    "ground_truth_relationship": "The documented session-based crawling example demonstrates the usage of the AsyncWebCrawler class's core methods like __aenter__, arun, and crawler_strategy.kill_session for handling persistent browser sessions with specific user interactions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler usage with async context manager, but misses the key aspect of session management and user interactions that are central to the ground truth's description.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code example, the 'arun()' method is explicitly called on the AsyncWebCrawler instance to perform crawling tasks with parameters like session_id, js_code, and css_selector.",
    "ground_truth_relationship": "The code implements session-based crawling through the arun() method by accepting a session_id parameter which maintains state across multiple crawl requests, exactly as demonstrated in the documentation example where a consistent session_id is used across multiple crawl operations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core functionality of the arun() method for crawling with parameters, which aligns with the ground truth's explanation of session-based crawling through the arun() method. While the predicted version is less detailed about session maintenance, it captures the essential relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        session_id = \"my_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                bypass_cache=True\n            )\n            \n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\nThis example demonstrates:\n1. Using a consistent `session_id` across multiple `arun` calls\n2. Executing JavaScript to load more content after the first page\n3. Using a CSS selector to extract specific content\n4. Properly closing the session after crawling",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example concludes by explicitly calling 'kill_session()' on the crawler_strategy attribute of AsyncWebCrawler to properly terminate the session, ensuring proper session management.",
    "ground_truth_relationship": "The kill_session method directly implements the session cleanup mentioned in the documentation's step 4 by closing both the page and context objects and removing the session from memory when crawling is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is called to terminate the session, but misses the key implementation details about closing both page and context objects and removing the session from memory that are central to the ground truth description",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates JsonCssExtractionStrategy with a schema to extract repeated elements (product listings), as seen in the line 'strategy = JsonCssExtractionStrategy(schema)'. This clearly indicates that the extraction strategy is used for pattern\u2010based extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements pattern-based web scraping by iterating through elements matching a base selector and extracting specified fields according to a schema defining CSS selectors and data types.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy is used for pattern-based extraction of repeated elements using a schema, which aligns with the ground truth's explanation of the class implementing pattern-based web scraping using base selectors and field extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage example, the code shows 'result = await crawler.arun(...)'. Although the text does not explicitly name the class, it is implicit that the arun() method (implemented in AsyncWebCrawler.arun()) is being invoked to perform the crawl that leverages the extraction strategy.",
    "ground_truth_relationship": "The arun() method processes web pages by accepting an extraction_strategy parameter which implements the JsonCssExtractionStrategy shown in the documentation to extract structured data using CSS selectors.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method is used to perform crawling with extraction strategies, matching the ground truth's explanation of how arun() processes web pages using extraction strategies like JsonCssExtractionStrategy. The omission of specific details about CSS selectors is minor.",
      "error_type": ""
    }
  },
  {
    "document_text": "### 2. Pattern-Based Extraction\n\nFor pages with repetitive patterns (e.g., product listings, article feeds), use JsonCssExtractionStrategy:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Product Listing\",\n    \"baseSelector\": \".product-card\",  # Repeated element\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\nproducts = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The snippet accesses 'result.extracted_content' to obtain the extraction output. This shows that the extracted content generated by the chosen extraction strategy is stored in this attribute of the CrawlResult object.",
    "ground_truth_relationship": "The extracted_content field stores the JSON-formatted results of pattern-based scraping, where each match from the baseSelector is transformed into an array of objects with the specified fields from the schema.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content stores the extraction output, but misses the crucial aspect that it specifically stores JSON-formatted results from pattern-based scraping with structured field mappings.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## CSS Selectors\n\nThe simplest way to extract specific content:\n\n```python\n# Extract specific content using CSS selector\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\".main-article\"  # Target main article content\n)\n\n# Multiple selectors\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    css_selector=\"article h1, article .content\"  # Target heading and content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet contains a usage example that explicitly calls the 'arun()' method on a 'crawler' object with a 'css_selector' parameter. This directly maps to the 'AsyncWebCrawler.arun()' method which includes the 'css_selector' parameter in its signature, indicating its role in extracting content using CSS selectors.",
    "ground_truth_relationship": "The code implements CSS selector-based content extraction through the css_selector parameter in the arun method, which is passed through to the HTML processing logic to filter and extract specific elements from the crawled webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that the arun method accepts a css_selector parameter for extracting specific content using CSS selectors. While it focuses more on the example usage vs implementation details, it conveys the same fundamental functionality described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the creation of an AsyncWebCrawler instance by passing a 'proxy_config' dictionary as a keyword argument. This directly shows that the AsyncWebCrawler class is used when setting up an authenticated proxy.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts a proxy_config parameter in its initialization which gets passed to the underlying AsyncPlaywrightCrawlerStrategy to enable authenticated proxy connections as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler accepts proxy_config for authenticated proxy setup, but misses the key detail that this config is passed through to the underlying AsyncPlaywrightCrawlerStrategy rather than being handled directly by AsyncWebCrawler.",
      "error_type": "incomplete_mechanism"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "After the AsyncWebCrawler instance is created, the snippet calls the 'arun()' method to perform the crawl, which confirms that this method is used for initiating the crawl process.",
    "ground_truth_relationship": "The code's arun() method implements proxy support by accepting proxy configuration through its crawler_strategy.crawl() function call, which aligns with the documentation showing how to initialize AsyncWebCrawler with proxy_config for authenticated proxy access.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() as the method for crawling but misses the key relationship regarding proxy support and configuration described in the ground truth",
      "error_type": "important_omission"
    }
  },
  {
    "document_text": "## Authenticated Proxy\n\nUse proxy with authentication:\n\n```python\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler does not require an explicit strategy to be passed; if none is provided, it defaults to using AsyncPlaywrightCrawlerStrategy. The provided 'proxy_config' is forwarded to this strategy, which internally handles proxy authentication. Although not directly mentioned in the snippet, its role is integral in processing the proxy parameters.",
    "ground_truth_relationship": "The code implements authenticated proxy support by configuring browser_args with ProxySettings containing server, username, and password from the proxy_config dictionary during browser initialization in the start() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the code handles authenticated proxy configuration through proxy_config parameters (server, username, password) during browser initialization. The predicted description correctly identifies the core functionality even if it doesn't detail the exact implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using custom 'user_agent' and 'headers' parameters, as shown in the 'async with AsyncWebCrawler(...) as crawler:' example.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements identity management through its arun method which accepts user_agent and **kwargs parameters that allow customization of crawler headers and user agent strings as demonstrated in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of identity management through AsyncWebCrawler's ability to use custom user agents and headers, which aligns with the ground truth. While it focuses on the instantiation examples rather than the implementation details, it correctly conveys the main relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage examples, the 'arun' method is explicitly invoked on the AsyncWebCrawler instance to perform the crawling action, demonstrating its role in executing the crawl with the given configuration.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements custom identity management by accepting user_agent and **kwargs parameters which allow modification of crawler headers as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description recognizes arun() as the main execution method but misses the key relationship about identity management and header customization that is central to the ground truth",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly referenced in the snippet, AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler strategy. This strategy internally manages parameters like 'user_agent' and 'headers', which are configured in the examples.",
    "ground_truth_relationship": "The code implements identity management by allowing custom user agents and headers to be passed through the AsyncPlaywrightCrawlerStrategy class constructor and applied to browser contexts using set_custom_headers() and update_user_agent() methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy to manage identity parameters like user_agent and headers, which aligns with the ground truth's explanation of how the class handles identity management through these parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Identity Management\n\nControl how your crawler appears to websites:\n\n```python\n# Custom user agent\nasync with AsyncWebCrawler(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Custom headers\nheaders = {\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\"\n}\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. While the abstract interface is not directly mentioned in the snippet, it underpins the crawling functionality and identity management supported by the strategy implementation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface for implementing crawler identity management through methods like update_user_agent() which enables the documented functionality of customizing how the crawler appears to websites.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract interface, but it incorrectly focuses on an unmentioned AsyncPlaywrightCrawlerStrategy and doesn't emphasize the core identity management functionality that's central to the ground truth.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler using an async context manager (async with AsyncWebCrawler() as crawler). This highlights its role in initializing a session for multiple crawl requests.",
    "ground_truth_relationship": "The AsyncWebCrawler code implements session management through the session_id parameter in the arun method, which gets stored in the CrawlResult object and can be passed between sequential requests as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on async context manager usage for initialization, while the ground truth emphasizes session_id parameter for maintaining state between requests. While both discuss session management, they describe different aspects.",
      "error_type": "focus_mismatch"
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method on the AsyncWebCrawler instance with a session_id parameter. This demonstrates how subsequent crawl requests use the existing session by passing the same session_id.",
    "ground_truth_relationship": "The code implements session management by accepting a session_id parameter in the arun() method's kwargs and assigning it to the crawl_result.session_id field, enabling state persistence across multiple requests as demonstrated in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core functionality of session management through session_id, which aligns with the ground truth's explanation of how the code handles session persistence across requests.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Session Usage\n\nUse `session_id` to maintain state between requests:\n\n```python\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n    \n    # First request\n    result1 = await crawler.arun(\n        url=\"https://example.com/page1\",\n        session_id=session_id\n    )\n    \n    # Subsequent request using same session\n    result2 = await crawler.arun(\n        url=\"https://example.com/page2\",\n        session_id=session_id\n    )\n    \n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The documentation snippet directly demonstrates calling kill_session(session_id) on the crawler_strategy. This maps to the kill_session() method in AsyncPlaywrightCrawlerStrategy, which is responsible for cleaning up sessions.",
    "ground_truth_relationship": "The kill_session method implements the cleanup functionality shown in the documentation by closing both the browser page and context objects, then removing them from the session tracking dictionary when a session is no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that kill_session is used for cleanup, but focuses mainly on the API call pattern rather than explaining what the method actually does (closing page/context and removing from sessions)",
      "error_type": "incomplete_explanation"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun()'. This indicates that the AsyncWebCrawler.arun() method is being used to initiate a crawl, with its result then inspected for success and error information.",
    "ground_truth_relationship": "The code's async_def arun() method implements comprehensive error handling through nested try-catch blocks that align with the documentation's example, returning CrawlResult objects with success/failure states and error messages for both extraction and general execution failures.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies arun() correctly as the core method but overly simplifies the error handling relationship by only mentioning result inspection, while the ground truth emphasizes the comprehensive nested try-catch implementation and CrawlResult return states",
      "error_type": "oversimplification"
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The code snippet accesses properties like 'success', 'extracted_content', and 'error_message' on the result object. This object is of type CrawlResult, indicating its role in encapsulating the outcome of the crawl operation.",
    "ground_truth_relationship": "The CrawlResult class provides essential fields like success, error_message, and extracted_content that directly support the error handling flow shown in the documentation by enabling status checking and error reporting during the crawler execution.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that CrawlResult class provides key properties (success, extracted_content, error_message) that are used for handling crawl operation outcomes, which aligns with the ground truth's explanation of how these fields support error handling flow.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly names 'Cosine Strategy' as particularly effective when content is inconsistent and semantic understanding is required. This directly references the CosineStrategy class which implements an extraction strategy based on cosine similarity.",
    "ground_truth_relationship": "The CosineStrategy class implements semantic-based content extraction using cosine similarity and hierarchical clustering, aligning with the documentation's description of handling inconsistent content structures and enabling semantic understanding through its filter_documents_embeddings and hierarchical_clustering methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately identifies that CosineStrategy is designed for handling inconsistent content and semantic understanding, which aligns with the ground truth's explanation of its core functionality using cosine similarity and hierarchical clustering methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Error Handling\n\n```python\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\nThe Cosine Strategy is particularly effective when:\n- Content structure is inconsistent\n- You need semantic understanding\n- You want to find similar content blocks\n- Structure-based extraction (CSS/XPath) isn't reliable\n\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The arun() method accepts an 'extraction_strategy' parameter and CosineStrategy extends this base class. This connection indicates that the overall extraction functionality adheres to the contract defined by the ExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure for implementing different content extraction methods, including the Cosine Strategy mentioned in the documentation, through its abstract extract() method and parallel processing capabilities in run().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies ExtractionStrategy as a base class for extraction functionality but focuses on arun() which isn't shown in the code. It misses the key aspects of extract() and run() methods that are central to the strategy pattern implementation.",
      "error_type": "missing_core_functionality"
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet demonstrates a direct call to the 'arun' method on the crawler object with parameters such as simulate_user, override_navigator, and magic. This directly corresponds to the 'AsyncWebCrawler.arun()' method, which is used to initiate crawling with configuration flags for anti-detection.",
    "ground_truth_relationship": "The arun() method accepts various stealth-related keyword arguments (like simulate_user, override_navigator, magic) which are passed through **kwargs to the crawler_strategy.crawl() function to implement the documented anti-detection features.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method accepts anti-detection parameters and uses them for crawling, which aligns with the ground truth's explanation of how stealth-related arguments are passed through kwargs to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Anti-Detection Features\n\nEnable stealth features to avoid bot detection:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    simulate_user=True,        # Simulate human behavior\n    override_navigator=True,   # Mask automation signals\n    magic=True               # Enable all anti-detection features\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly invoked in the snippet, the provided anti-detection parameters (simulate_user, override_navigator, and magic) are handled inside the 'crawl' method of 'AsyncPlaywrightCrawlerStrategy'. Within the 'AsyncWebCrawler.arun()' method, these parameters are forwarded to the crawler strategy, which by default is an instance of 'AsyncPlaywrightCrawlerStrategy'. This class implements the stealth features by injecting scripts to override navigation properties.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements anti-detection features through context.add_init_script() which injects JavaScript to override navigator properties, simulate plugins, and mask automation signals when override_navigator, simulate_user, or magic parameters are set to True.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncPlaywrightCrawlerStrategy class implements anti-detection features through script injection when certain parameters are set, even if it refers to the arun() method which isn't directly shown. The core functionality of injecting scripts to override navigator properties and mask automation is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows usage of the crawler by calling 'crawler.arun' with a js_code parameter. This directly maps to the 'AsyncWebCrawler.arun()' method, where such parameters are accepted and processed.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution functionality by accepting custom JavaScript commands through its **kwargs parameter, which are then passed to the crawler_strategy.crawl() method for execution during the page crawl.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() as the key method but implies js_code is a direct parameter when it's actually handled through **kwargs and passed to crawler_strategy.crawl()",
      "error_type": "parameter_handling_misunderstanding"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within the 'crawl' method of AsyncPlaywrightCrawlerStrategy, the js_code parameter (provided via js_code or js) is checked and executed using page.evaluate(). Although not directly mentioned in the snippet, this class underpins the execution of the custom JavaScript as passed in the example.",
    "ground_truth_relationship": "The code implements JavaScript execution functionality through the crawl() method, which accepts js_code parameter that can handle both single commands and arrays of commands, evaluating them on the page using page.evaluate().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of JavaScript execution through the crawl() method using page.evaluate(). While it doesn't mention the ability to handle arrays of commands explicitly like the ground truth does, it gets the main relationship right.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\nExecute custom JavaScript before crawling:\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. The abstract definition in AsyncCrawlerStrategy specifies the crawl interface (including parameters such as js_code) that is implemented for custom JS execution.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational crawl() method that enables JavaScript execution during web scraping as shown in the documentation's arun() examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that AsyncCrawlerStrategy provides the base crawling functionality that enables JavaScript execution during web scraping. The predicted description mentions the abstract interface while the ground truth focuses on the functionality, but they describe the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(...)', which explicitly matches the 'arun' method defined in the AsyncWebCrawler class. The parameters (such as exclude_domains, exclude_social_media_domains, and exclude_social_media_links) are provided as keyword arguments to this method, indicating its role in domain\u2010based filtering.",
    "ground_truth_relationship": "The arun() method's **kwargs parameter accepts domain filtering options like exclude_domains and exclude_social_media_domains documented in the example code snippet, which get passed through to the crawler_strategy.crawl() function for domain-based content control.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method accepts domain filtering parameters through **kwargs and matches the documented functionality. While it's less specific about how these parameters flow through to crawler_strategy.crawl(), it captures the core relationship between the code and documentation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Domain-Based Filtering\n\nControl content based on domains:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_domains=[\"ads.com\", \"tracker.com\"],\n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],  # Custom social media domains to exclude\n    exclude_social_media_links=True\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not named explicitly in the snippet, the 'crawler' object is understood to be an instance of AsyncWebCrawler. This class defines the public interface that includes the 'arun()' method used in the example, making it an implicit part of the domain-based filtering functionality.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements domain-based filtering through its arun method which accepts exclude_domains and exclude_social_media parameters that are passed to the underlying crawler strategy for URL filtering during web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the AsyncWebCrawler class and its arun() method, but fails to mention the key domain-based filtering functionality through exclude_domains and exclude_social_media parameters that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates an AsyncWebCrawler with custom headers by calling 'AsyncWebCrawler(headers=headers)' which directly maps to the AsyncWebCrawler class implementation.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements custom header support through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy to configure HTTP headers like X-Forwarded-For and Cache-Control as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that the AsyncWebCrawler can be initialized with custom headers, even though it doesn't explicitly mention the internal kwargs passing to AsyncPlaywrightCrawlerStrategy. The core functionality relationship is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Custom Headers\n\nAdd security-related headers:\n\n```python\nheaders = {\n    \"X-Forwarded-For\": \"203.0.113.195\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Pragma\": \"no-cache\"\n}\n\nasync with AsyncWebCrawler(headers=headers) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'crawler.arun(url=\"https://example.com\")' on the instantiated AsyncWebCrawler. This call clearly maps to the 'AsyncWebCrawler.arun()' method, which is responsible for executing the crawl operation.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method accepts custom headers through its crawler_strategy component, allowing users to set security-related headers like X-Forwarded-For and Cache-Control as shown in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that the code relates to AsyncWebCrawler.arun() method and crawling, but misses the key aspect about custom headers functionality that is central to the ground truth relationship",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows a call to the 'arun()' method using 'crawler.arun(...)' with parameters that control link filtering (e.g., exclude_external_links, exclude_social_media_links, etc.). This directly demonstrates how the method is used to implement smart link filtering.",
    "ground_truth_relationship": "The arun method accepts filtering parameters through **kwargs which allows for the documented link filtering options like exclude_external_links and exclude_domains to be passed through to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles link filtering functionality through its parameters, even though it doesn't specifically mention the **kwargs implementation detail. The core relationship between the method and its link filtering capability is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Smart Link Filtering\nControl which links are included in the results:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    exclude_external_links=True,          # Remove external links\n    exclude_social_media_links=True,      # Remove social media links\n    exclude_social_media_domains=[                # Custom social media domains\n        \"facebook.com\", \"twitter.com\", \"instagram.com\"\n    ],\n    exclude_domains=[\"ads.example.com\"]   # Exclude specific domains\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although the snippet calls 'arun()' on an instance named 'crawler', the instance is of type 'AsyncWebCrawler'. This class is the container that provides the 'arun()' method, and its inclusion is necessary to trace the complete usage chain.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements link filtering through kwargs passed to the arun method, which processes URL filtering parameters documented in the smart link filtering example to control which links are included in the crawl results.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on the class hierarchy and method ownership, while the ground truth describes the actual link filtering functionality implemented through kwargs parameters in the arun method. These are completely different aspects of the code.",
      "error_type": "wrong_functionality_focus"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation sample code directly calls 'crawler.arun()' with the 'js_code' parameter, demonstrating how JavaScript commands are passed. This usage explicitly shows that the 'arun()' method is intended to handle JavaScript execution within the crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented JavaScript execution capability by accepting js_code as a parameter through **kwargs and forwarding it to the crawler_strategy.crawl() method for executing single or multiple JavaScript commands on the target webpage.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles JavaScript execution, but misses that it does this indirectly through the crawler_strategy.crawl() method rather than directly executing the JS code itself.",
      "error_type": "omitted_crucial_mechanism"
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the documentation snippet, the 'AsyncPlaywrightCrawlerStrategy' contains the implementation of its 'crawl' method where the 'js_code' parameter is checked and executed using 'page.evaluate'. This underpins the JavaScript execution demonstrated in the documentation.",
    "ground_truth_relationship": "The code implements JavaScript execution by allowing both single and multiple JS commands to be passed through the js_code parameter in the crawl method, which are then executed using Playwright's page.evaluate() function.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that JavaScript execution is implemented through the js_code parameter and page.evaluate() in the crawl method. While it's more concise than the ground truth, it doesn't contradict or misunderstand the functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## JavaScript Execution\n\n### Basic Execution\n\n```python\n# Single JavaScript command\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n)\n\n# Multiple commands\njs_commands = [\n    \"window.scrollTo(0, document.body.scrollHeight);\",\n    \"document.querySelector('.load-more').click();\",\n    \"document.querySelector('#consent-button').click();\"\n]\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_commands\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncCrawlerStrategy' serves as an abstract base class that defines the 'crawl' method's contract. 'AsyncPlaywrightCrawlerStrategy' extends this base class to implement JavaScript execution, thereby forming the fundamental structure that supports the functionality demonstrated in the documentation.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core methods that enable JavaScript execution through its crawl() method, which the documentation demonstrates using through examples of scrolling and clicking elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify AsyncCrawlerStrategy as the abstract base class that enables JavaScript execution through the crawl method, though they emphasize slightly different aspects of the same core relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows the use of the arun() method (via 'result = await crawler.arun(url=\"https://example.com\")'), demonstrating its role in initiating a crawl and returning a response object.",
    "ground_truth_relationship": "The arun() method implementation directly returns a CrawlResult object containing all the documented properties like html, cleaned_html, markdown, success, status_code, media and links by processing the crawled webpage content through various extraction and chunking strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() returns a response object, but misses the crucial processing aspects and the rich set of properties contained in the CrawlResult object that are central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Understanding the Response\n\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](../api/crawl-result.md) for complete details):\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown)     # Markdown version\nprint(result.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The snippet details that the arun() method returns a CrawlResult object and then demonstrates accessing its properties (html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links). This confirms CrawlResult as the response class providing these interfaces.",
    "ground_truth_relationship": "The CrawlResult class defines all the properties demonstrated in the documentation example through type-annotated fields, including html, cleaned_html, markdown, fit_markdown, success, status_code, media, and links.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that CrawlResult is the response class containing the properties demonstrated in the documentation example, matching the ground truth that defines these fields through type annotations.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "In the resource management section of the documentation, the code snippet explicitly calls 'await crawler.crawler_strategy.kill_session(session_id)' to clean up sessions. This directly maps to the 'kill_session()' method defined in the AsyncPlaywrightCrawlerStrategy, as provided in artifact id 2.",
    "ground_truth_relationship": "The kill_session() method implements the documented resource management best practice by closing browser contexts and pages to prevent memory leaks when sessions are no longer needed.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship between the code and documentation - that kill_session() implements the documented resource cleanup functionality. Both describe the same purpose of session cleanup and resource management.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Session Best Practices\n\n1. **Session Naming**:\n```python\n# Use descriptive session IDs\nsession_id = \"login_flow_session\"\nsession_id = \"product_catalog_session\"\n```\n\n2. **Resource Management**:\n```python\ntry:\n    # Your crawling code\n    pass\nfinally:\n    # Always clean up sessions\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n3. **State Management**:\n```python\n# First page: login\nresult = await crawler.arun(\n    url=\"https://example.com/login\",\n    session_id=session_id,\n    js_code=\"document.querySelector('form').submit();\"\n)\n\n# Second page: verify login success\nresult = await crawler.arun(\n    url=\"https://example.com/dashboard\",\n    session_id=session_id,\n    wait_for=\"css:.user-profile\"  # Wait for authenticated content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation's state management example demonstrates invoking 'await crawler.arun(...)' with parameters like 'session_id' and 'js_code'. This clearly corresponds to the 'arun()' method of AsyncWebCrawler, which manages the crawling process and session handling, as specified in artifact id 5.",
    "ground_truth_relationship": "The arun() method stores session_id from kwargs which enables stateful crawling across multiple requests as shown in the documentation's state management examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the core relationship - that the arun() method handles session management via parameters like session_id as shown in the documentation examples. While it doesn't explicitly mention state persistence across requests, it correctly identifies the connection between arun() and session handling.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates the AsyncWebCrawler class using proxy and headers parameters. This demonstrates its direct use in the code sample.",
    "ground_truth_relationship": "The AsyncWebCrawler class constructor accepts proxy and headers parameters shown in the documentation through its **kwargs argument, which are then passed to the AsyncPlaywrightCrawlerStrategy for implementing the proxy and magic mode features.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is being instantiated with proxy and headers parameters, but misses the key point that these parameters are passed through **kwargs to AsyncPlaywrightCrawlerStrategy rather than being directly used by AsyncWebCrawler itself.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet directly calls the 'arun()' method on the crawler instance with parameters including 'magic=True', which is used to enable anti-detection features.",
    "ground_truth_relationship": "The arun() method implements the documented proxy and magic mode functionality by accepting kwargs parameters that configure crawler settings like proxies and enabling anti-detection features through the crawler_strategy object.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() accepts parameters and mentions anti-detection features, but incorrectly implies direct magic=True usage when the code actually shows kwargs handling for more general parameter passing and configuration",
      "error_type": "oversimplification"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses a default crawling strategy when no custom strategy is provided. It instantiates AsyncPlaywrightCrawlerStrategy internally, which processes parameters like 'magic' to simulate user interactions for anti-detection.",
    "ground_truth_relationship": "The code implements proxy configuration in the AsyncPlaywrightCrawlerStrategy class by setting up proxy settings during browser launch, which directly corresponds to the documentation's example of combining proxy settings with magic mode for enhanced protection.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on the AsyncWebCrawler's use of AsyncPlaywrightCrawlerStrategy and magic mode for anti-detection, but misses the core relationship about proxy configuration implementation shown in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Combining with Magic Mode\n\nFor maximum protection, combine proxy with Magic Mode:\n\n```python\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\",\n    headers={\"Accept-Language\": \"en-US\"}\n) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        magic=True  # Enable all anti-detection features\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is a concrete implementation derived from the abstract base class AsyncCrawlerStrategy. This inheritance relationship ensures that the crawling interface\u2014used to enable features like magic mode\u2014is maintained.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods that enable advanced crawling features like Magic Mode and proxy support shown in the documentation through its abstract crawl and configuration methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncCrawlerStrategy is an abstract base class that defines the interface for crawling functionality, which enables features mentioned in the documentation. While it mentions a specific implementation (AsyncPlaywrightCrawlerStrategy) not shown in the code, this doesn't contradict the core relationship described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(...)', which directly demonstrates the creation of a crawler instance.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements user simulation through the simulate_user and override_navigator parameters in its arun method, which are passed via **kwargs to the underlying crawler_strategy for executing realistic browser behaviors.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler instantiation, but misses the core functionality about user simulation behavior described in the ground truth. The ground truth focuses on the user simulation capabilities while the prediction focuses on class instantiation.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the arun() method on the AsyncWebCrawler instance to perform the crawl, passing parameters like simulate_user and override_navigator.",
    "ground_truth_relationship": "The documentation describes user simulation features while the arun() method implements these behaviors through the **kwargs parameter which can accept simulate_user and override_navigator flags to control browser automation behavior.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that the arun() method is used to perform crawling with parameters, while the ground truth elaborates that these parameters specifically control browser automation behavior through kwargs. The core relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler uses AsyncPlaywrightCrawlerStrategy as its default crawler strategy. This strategy\u2019s crawl() method handles the simulate_user and override_navigator parameters to inject user behavior such as random mouse movements and navigator modifications.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements user simulation through mouse movements, clicks, and navigator property overrides when simulate_user=True or override_navigator=True are passed as parameters in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncPlaywrightCrawlerStrategy implements user simulation functionality through simulate_user and override_navigator parameters. Both descriptions convey the same essential functionality of simulating user behavior and modifying navigator properties.",
      "error_type": ""
    }
  },
  {
    "document_text": "### User Simulation \ud83c\udfad\n\nSimulate real user behavior to avoid detection:\n\n```python\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(\n        url=\"YOUR-URL-HERE\",\n        bypass_cache=True,\n        simulate_user=True,  # Causes random mouse movements and clicks\n        override_navigator=True  # Makes the browser appear more like a real user\n    )\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, which defines the abstract interface for crawling. This inheritance relationship underpins the implementation used for user simulation behaviors, although it is not directly referenced in the snippet.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable user simulation features referenced in the documentation through abstract methods like set_hook and update_user_agent which are used to implement browser behavior customization.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the inheritance relationship with AsyncCrawlerStrategy, but misses the key point about how the abstract methods specifically enable user simulation features through hooks and agent customization.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation explicitly imports and instantiates CosineStrategy from 'crawl4ai.extraction_strategy'. The snippet shows that CosineStrategy is used to perform similarity\u2010based clustering for extracting relevant content sections.",
    "ground_truth_relationship": "The code implements the CosineStrategy class with configurable parameters (semantic_filter, word_count_threshold, sim_threshold, max_dist, and top_k) exactly as documented in the example code snippet, using these values to control the similarity-based clustering and content extraction process.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that CosineStrategy performs similarity-based clustering for content extraction, but misses explaining the key configurable parameters and their role in controlling the clustering process, which is a significant aspect highlighted in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage example, the initialized CosineStrategy instance is passed to the crawler's 'arun()' method. Although the method name is not explicitly mentioned as part of the extraction strategy, its invocation shows that it integrates the extraction strategy into the crawling process.",
    "ground_truth_relationship": "The arun() method implements the main crawling logic that applies the documented CosineStrategy by accepting an extraction_strategy parameter which can be set to a CosineStrategy instance for similarity-based content clustering and extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that CosineStrategy is passed to and integrated with the arun() method for crawling, which aligns with the ground truth's explanation of arun() accepting and implementing extraction strategies.",
      "error_type": ""
    }
  },
  {
    "document_text": "### [Cosine Strategy](cosine.md)\n\n`CosineStrategy` uses similarity-based clustering to identify and extract relevant content sections.\n\n```python\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Content focus\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3,                   # Similarity threshold\n    max_dist=0.2,                        # Maximum cluster distance\n    top_k=3                             # Number of top clusters to extract\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/reviews\",\n    extraction_strategy=strategy\n)\n```\n\n**Best for:**\n- Content similarity analysis\n- Topic clustering\n- Relevant content extraction\n- Pattern recognition in text",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy derives from ExtractionStrategy, meaning that the abstract behavior for content extraction is defined within ExtractionStrategy. The documentation's focus on CosineStrategy indirectly relies on this base class.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational structure and parallel processing capabilities that enable derived strategies like CosineStrategy to implement specialized content extraction methods through the abstract extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core inheritance relationship between CosineStrategy and ExtractionStrategy, and correctly indicates that the base class defines abstract behavior that derived classes implement. While it doesn't mention parallel processing, this omission doesn't change the fundamental relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly uses the 'crawler.arun()' call to handle dynamic content (e.g., executing js_code scripts for scrolling and form interaction). This clearly maps to the AsyncWebCrawler.arun() method as defined in artifact 5.",
    "ground_truth_relationship": "The arun() method implements dynamic content handling and form interactions by accepting js_code and wait_for parameters that allow execution of JavaScript commands for scrolling, clicking, and form manipulation while waiting for specified selectors or conditions to be met.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method's role in handling dynamic content, but misses key details about waiting for conditions and form interactions that are central to how it accomplishes this functionality",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "When AsyncWebCrawler.arun() is called, it internally delegates the crawling task to its crawler_strategy, which by default is an instance of AsyncPlaywrightCrawlerStrategy. This class is responsible for processing parameters like 'js_code' and 'wait_for' to handle dynamic content. Although not directly mentioned in the snippet, its functionality is implicitly required to execute the provided JavaScript codes.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements dynamic content loading through its smart_wait method which handles both scroll-to-bottom and form interaction patterns by executing JavaScript code and waiting for specified selectors or conditions as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles dynamic content by processing js_code and wait_for parameters. While it's less detailed than the ground truth, it captures the core functionality of executing JavaScript and handling dynamic content loading.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\n### Load More Content\n\nHandle infinite scroll or load more buttons:\n\n```python\n# Scroll and wait pattern\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=[\n        # Scroll to bottom\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # Click load more if exists\n        \"const loadMore = document.querySelector('.load-more'); if(loadMore) loadMore.click();\"\n    ],\n    # Wait for new content\n    wait_for=\"js:() => document.querySelectorAll('.item').length > previousCount\"\n)\n```\n\n### Form Interaction\n\nHandle forms and inputs:\n\n```python\njs_form_interaction = \"\"\"\n    // Fill form fields\n    document.querySelector('#search').value = 'search term';\n    // Submit form\n    document.querySelector('form').submit();\n\"\"\"\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=js_form_interaction,\n    wait_for=\"css:.results\"  # Wait for results to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy, establishing an inheritance relationship. This abstract base class defines the interface that supports dynamic content handling. Its presence in the chain is critical to understand the architectural design underlying the crawling mechanism.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods like crawl() and set_hook() that enable the documented dynamic content handling and form interactions through its extensible interface.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class, but incorrectly focuses on an inheritance relationship with AsyncPlaywrightCrawlerStrategy that isn't shown in the code or ground truth. The core purpose of providing foundational methods for dynamic content handling is present though.",
      "error_type": "added_incorrect_relationship"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet shows an explicit instantiation of AsyncWebCrawler with 'async with AsyncWebCrawler(headless=False) as crawler:', directly indicating its use as the primary crawling class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements overlay removal through its arun() method which accepts a remove_overlay_elements parameter that gets passed to the underlying crawler strategy for handling webpage overlays during content extraction.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main crawling class but misses the key relationship about overlay removal functionality, instead focusing only on instantiation syntax.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the method 'arun()' is explicitly called on the AsyncWebCrawler instance (result = await crawler.arun(...)), demonstrating how the crawler is used to perform a crawl with various options such as bypassing the cache and removing overlay elements.",
    "ground_truth_relationship": "The code implements the documented overlay removal functionality through the arun() method which accepts remove_overlay_elements as a parameter and processes it along with other crawling configurations like word_count_threshold and screenshot options.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies arun() usage and some parameters, but misses the core overlay removal functionality that is central to the ground truth relationship",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler instantiates a crawling strategy internally; by default it uses AsyncPlaywrightCrawlerStrategy to perform tasks such as removing overlay elements and capturing screenshots when parameters like remove_overlay_elements and screenshot are provided.",
    "ground_truth_relationship": "The code implements overlay removal in the remove_overlay_elements method using JavaScript to detect and remove elements with high z-index, fixed positions, and common overlay selectors like cookie banners, popups, and modals, matching the documented functionality for handling overlays.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly mentions the AsyncPlaywrightCrawlerStrategy handles overlay removal and screenshots, but incorrectly suggests AsyncWebCrawler instantiates this internally. The ground truth focuses specifically on how the overlay removal is implemented using JavaScript to detect and remove elements.",
      "error_type": "implementation_details_mismatch"
    }
  },
  {
    "document_text": "### Handling Overlays and Fitting Content Remove overlay elements and fit content appropriately: ```python async with AsyncWebCrawler(headless=False) as crawler: result = await crawler.arun( url=\"your-url-here\", bypass_cache=True, word_count_threshold=10, remove_overlay_elements=True, screenshot=True ) ```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy to implement the standardized asynchronous crawling interface. This base class establishes the contract that AsyncPlaywrightCrawlerStrategy fulfills even though it is not explicitly mentioned in the snippet.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface that enables overlay removal and content fitting through its crawl method, which the documentation demonstrates being used via the AsyncWebCrawler implementation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that AsyncCrawlerStrategy defines an abstract interface for crawling functionality, including overlay handling. The predicted description correctly identifies it as a base class that establishes the contract for implementation, even though it mentions a specific implementation class not shown.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates JsonCssExtractionStrategy with a schema to extract dynamic structured data. The usage of backticks in the text indicates direct reference to this extraction strategy for handling dynamic content via JavaScript execution.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class enables structured data extraction from dynamic web pages by processing a schema that defines CSS selectors for base elements and their fields, which directly supports the documented advanced usage pattern of combining with JavaScript execution for dynamic content scraping.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that JsonCssExtractionStrategy processes schema-based CSS selectors for structured data extraction from dynamic web pages, with explicit support for JavaScript execution. The predicted description captures the core relationship and usage pattern.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the base class is implicitly involved as it defines the contract for extraction strategies.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing specialized extraction strategies like JsonCssExtractionStrategy, which is demonstrated in the documentation example for extracting dynamic cryptocurrency data.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions identify ExtractionStrategy as the base class that provides the foundation for implementing specialized extraction strategies like JsonCssExtractionStrategy. The predicted description captures this core relationship accurately, even if it's expressed more briefly.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet explicitly uses AsyncWebCrawler with an async context manager (async with AsyncWebCrawler(verbose=True) as crawler), demonstrating its role in performing the crawl and extraction process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the dynamic data extraction functionality described in the documentation through its arun method, which supports JavaScript execution (js_code), waiting conditions (wait_for), and custom extraction strategies (JsonCssExtractionStrategy).",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted text correctly identifies AsyncWebCrawler's async context manager functionality, but misses the core relationship described in the ground truth regarding its support for dynamic data extraction through JavaScript execution, waiting conditions, and custom extraction strategies",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage example, the arun() method of the AsyncWebCrawler instance is called (crawler.arun(...)). This method is responsible for performing the crawl, integrating the extraction strategy, and handling JavaScript execution parameters.",
    "ground_truth_relationship": "The arun() method implements support for dynamic content extraction by accepting parameters for js_code execution, extraction_strategy, and screenshot capabilities as shown in the documentation's advanced usage example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles crawling and integrates extraction strategy functionality, which aligns with the ground truth's description of its support for dynamic content extraction and strategy integration. While the ground truth provides more specific details about js_code and screenshot capabilities, the core relationship is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the `JsonCssExtractionStrategy` with JavaScript execution:\n\n```python\nasync def extract_dynamic_structured_data():\n    schema = {\n        \"name\": \"Dynamic Crypto Prices\",\n        \"baseSelector\": \".crypto-row\",\n        \"fields\": [\n            {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n            {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n        ]\n    }\n\n    js_code = \"\"\"\n    window.scrollTo(0, document.body.scrollHeight);\n    await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n    \"\"\"\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            extraction_strategy=extraction_strategy,\n            js_code=js_code,\n            wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n            bypass_cache=True,\n        )\n\n        crypto_data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n\nasyncio.run(extract_dynamic_structured_data())\n```\n\nThis advanced example demonstrates how to:\n1. Execute JavaScript to trigger dynamic content loading.\n2. Wait for a specific condition (20 rows loaded) before extraction.\n3. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract structured data from a wide variety of web pages, making it a valuable tool in your web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies, check out the[Advanced JsonCssExtraction](./css-advanced.md).",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After the asynchronous crawl, the result object's extracted_content attribute is accessed (json.loads(result.extracted_content)). This attribute holds the structured data extracted from the page, indirectly linking the extraction process with the output format defined in CrawlResult.",
    "ground_truth_relationship": "The extracted_content attribute stores the dynamic crypto pricing data after it's scraped using JsonCssExtractionStrategy and processed through JavaScript execution.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that extracted_content stores the structured data obtained from web scraping. The predicted description correctly explains how the extracted_content is accessed and used, while being more technical about the implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet shows an explicit example of instantiating CosineStrategy with parameters such as semantic_filter, word_count_threshold, sim_threshold, max_dist, linkage_method, top_k, model_name, and verbose. Each parameter in the code example directly corresponds to the constructor parameters defined in the CosineStrategy class.",
    "ground_truth_relationship": "The code implements a CosineStrategy class that exactly matches the documented configuration parameters, including semantic_filter, word_count_threshold, max_dist, linkage_method, top_k, model_name, and sim_threshold, which are all initialized in the constructor with the same default values shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - that the CosineStrategy class parameters directly match those shown in the documentation example, with the same parameter names and default values.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Configuration Options\n\n### Core Parameters\n\n```python\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is implemented as a subclass of ExtractionStrategy. Although ExtractionStrategy is not directly mentioned in the snippet, its role as the base class for CosineStrategy makes it an implicit part of the documented functionality.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class serves as a foundation for implementing configurable extraction strategies like CosineStrategy, where the documented parameters would be passed through the kwargs argument in the constructor.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify the core relationship between ExtractionStrategy as the base class and CosineStrategy as an implementing subclass, with configuration parameters being passed through kwargs.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly shows usage examples instantiating CosineStrategy with parameters such as sim_threshold, word_count_threshold, and top_k. These parameters match the constructor signature in the CosineStrategy class.",
    "ground_truth_relationship": "The documentation's parameter details section directly maps to the CosineStrategy class's __init__ method parameters, where semantic_filter, sim_threshold, word_count_threshold, and top_k are initialized with their described functionalities implemented in the corresponding class methods like filter_documents_embeddings() and filter_clusters_by_word_count().",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the parameter mapping between documentation and constructor, but misses the crucial implementation relationship showing how these parameters are used in the class methods to achieve their described functionality.",
      "error_type": "incomplete_relationship"
    }
  },
  {
    "document_text": "### Parameter Details\n\n1. **semantic_filter**\n   - Sets the target topic or content type\n   - Use keywords relevant to your desired content\n   - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n\n2. **sim_threshold**\n   - Controls how similar content must be to be grouped together\n   - Higher values (e.g., 0.8) mean stricter matching\n   - Lower values (e.g., 0.3) allow more variation\n   ```python\n   # Strict matching\n   strategy = CosineStrategy(sim_threshold=0.8)\n   \n   # Loose matching\n   strategy = CosineStrategy(sim_threshold=0.3)\n   ```\n\n3. **word_count_threshold**\n   - Filters out short content blocks\n   - Helps eliminate noise and irrelevant content\n   ```python\n   # Only consider substantial paragraphs\n   strategy = CosineStrategy(word_count_threshold=50)\n   ```\n\n4. **top_k**\n   - Number of top content clusters to return\n   - Higher values return more diverse content\n   ```python\n   # Get top 5 most relevant content clusters\n   strategy = CosineStrategy(top_k=5)\n   ```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "CosineStrategy is defined as a subclass of ExtractionStrategy. Although ExtractionStrategy itself is not directly mentioned in the snippet, it forms part of the extraction strategy hierarchy implemented by CosineStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy class serves as a base template for implementing various content extraction configurations described in the parameter documentation, with specific settings for semantic_filter, sim_threshold, word_count_threshold, and top_k being defined in child classes.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies ExtractionStrategy as a base class but fails to mention its role in defining parameter configurations that are core to its functionality as described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates a call to 'crawler.arun()' with parameters like 'page_timeout', 'delay_before_return_html', and 'wait_for'. This explicitly maps to the 'AsyncWebCrawler.arun()' method which is invoked in the usage example.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting controls through the **kwargs parameter in arun(), which passes page_timeout, delay_before_return_html, and wait_for options to the crawler_strategy.crawl() method.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() accepts timeout and waiting parameters through its interface, even though it doesn't explicitly mention the **kwargs implementation detail. The core relationship between the documentation and code is accurately captured.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Within the 'AsyncWebCrawler.arun()' method, the crawler's strategy is invoked via its 'crawl()' method. The instance of AsyncPlaywrightCrawlerStrategy processes parameters like 'page_timeout', 'delay_before_return_html', and 'wait_for' to control waiting and timeout behavior. Although this is not directly mentioned in the snippet, it is key to implementing the documented functionality.",
    "ground_truth_relationship": "The code implements the documented timeout and waiting behavior through the smart_wait method, which handles page_timeout for load control and delay_before_return_html for content capture timing, while also supporting CSS selector waiting via wait_for parameter in the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the AsyncPlaywrightCrawlerStrategy implements the documented timeout and waiting behavior through its crawl method, including the key parameters like page_timeout, delay_before_return_html, and wait_for. While it doesn't mention the specific smart_wait method, this is a minor implementation detail that doesn't affect the core relationship being described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timeouts and Waiting\n\nControl page loading behavior:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before content capture\n    wait_for=\"css:.dynamic-content\"  # Wait for specific element\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy is a subclass of AsyncCrawlerStrategy. This abstract base class defines the interface and contract for crawling methods, including those managing timeouts and waiting for elements. Its presence underpins the concrete implementation in AsyncPlaywrightCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundation for implementing webpage crawling with timeout and waiting capabilities through its crawl method that accepts **kwargs parameter where timeout and waiting configurations can be passed.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that AsyncCrawlerStrategy is an abstract base class that provides the interface for crawling functionality with timeout/waiting capabilities through its methods and parameters. The predicted description correctly captures this core relationship and purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the creation of an AsyncWebCrawler instance with proxy parameters. The 'async with AsyncWebCrawler(...) as crawler:' syntax is directly shown, indicating that this class is part of the public interface for configuring proxies.",
    "ground_truth_relationship": "The AsyncWebCrawler class accepts proxy configuration through its constructor's **kwargs parameter, which gets passed to the AsyncPlaywrightCrawlerStrategy for handling HTTP proxy settings during web crawling operations.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core idea that AsyncWebCrawler handles proxy configuration through its constructor parameters. While the predicted focuses on showing example usage and the ground truth explains the internal mechanism (kwargs passed to Strategy), they describe the same fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet explicitly calls the 'arun' method on the AsyncWebCrawler instance (result = await crawler.arun(url=\"https://example.com\")), which confirms that this method is part of the proxy-enabled crawl operation.",
    "ground_truth_relationship": "The arun() method accepts proxy configurations through its underlying crawler_strategy.crawl() call, enabling the proxy usage patterns shown in the documentation for both simple and authenticated proxies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is part of AsyncWebCrawler's functionality, but misses the key relationship with proxy configuration described in the ground truth - that arun() specifically enables proxy usage through crawler_strategy.crawl()",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler instantiates its crawler strategy from AsyncPlaywrightCrawlerStrategy by default. This class internally handles proxy parameters (via 'proxy' and 'proxy_config') during browser initialization, thus providing the backbone for proxy configuration.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its browser_args configuration, where it creates ProxySettings objects using either a simple proxy string or a proxy_config dictionary containing server, username, and password as shown in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncPlaywrightCrawlerStrategy handles proxy configuration functionality, even though it describes it slightly differently than the ground truth. Both descriptions convey the same core functionality of proxy support implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Proxy Configuration\n\nUse proxies for enhanced access:\n\n```python\n# Simple proxy\nasync with AsyncWebCrawler(\n    proxy=\"http://proxy.example.com:8080\"\n) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Proxy with authentication\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\nasync with AsyncWebCrawler(proxy_config=proxy_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract class AsyncCrawlerStrategy. This establishes a hierarchical relationship where the proxy configuration functionality inherited by the AsyncPlaywrightCrawlerStrategy is based on the interface defined in AsyncCrawlerStrategy.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the interface that enables proxy-based crawling functionality through its abstract crawl methods, which the documentation demonstrates how to configure with both simple and authenticated proxy settings.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class that defines an interface, but incorrectly focuses on AsyncPlaywrightCrawlerStrategy which isn't mentioned in the code/documentation. The ground truth more accurately describes the class's role in enabling proxy-based crawling.",
      "error_type": "additional_irrelevant_class"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly invokes the 'arun' method via 'crawler.arun(url=\"https://example.com\")'. This clearly maps to the AsyncWebCrawler.arun() method which initiates the crawling process and returns a CrawlResult object.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the core crawling functionality that powers the link analysis features by asynchronously fetching and processing web pages, returning a CrawlResult object that contains the categorized internal and external links described in the documentation.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the basic connection between arun() and crawling functionality but misses the key relationship to link analysis and categorization capabilities that is central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Link Analysis\n\nCrawl4AI provides sophisticated link analysis capabilities, helping you understand the relationship between pages and identify important navigation patterns.\n\n### Link Classification\nThe library automatically categorizes links into:\n- Internal links (same domain)\n- External links (different domains)\n- Social media links\n- Navigation links\n- Content links\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Analyze internal links\nfor link in result.links[\"internal\"]:\n    print(f\"Internal: {link['href']}\")\n    print(f\"Link text: {link['text']}\")\n    print(f\"Context: {link['context']}\")  # Surrounding text\n    print(f\"Type: {link['type']}\")  # nav, content, etc.\n\n# Analyze external links\nfor link in result.links[\"external\"]:\n    print(f\"External: {link['href']}\")\n    print(f\"Domain: {link['domain']}\")\n    print(f\"Type: {link['type']}\")\n```",
    "artifact_title": "CrawlResult.links",
    "predicted_relationship": "The snippet accesses the 'links' attribute from the resulting object (result.links) and iterates over its 'internal' and 'external' keys. This attribute is defined in the CrawlResult class and is used to store categorized link data returned by the crawling process.",
    "ground_truth_relationship": "The links property of CrawlResult stores categorized link data as a dictionary where keys indicate link types (internal, external, social, etc.) and values are lists of link details like href, text, and context.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core concept that links are stored in a categorized dictionary structure under result.links with different types of links like internal and external. While it doesn't list all categories or link details mentioned in the ground truth, the fundamental relationship is correctly described.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Content Filters\n\nControl what content is included:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per block\n    exclude_external_links=True,    # Remove external links\n    exclude_external_images=True,   # Remove external images\n    excluded_tags=['form', 'nav']   # Remove specific HTML tags\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct call to 'crawler.arun(...)' with filtering parameters such as 'word_count_threshold', 'exclude_external_links', 'exclude_external_images', and 'excluded_tags'. This usage example explicitly maps to the 'AsyncWebCrawler.arun()' method, which is responsible for initiating the crawling process and handling content filtering based on its input parameters.",
    "ground_truth_relationship": "The arun() method implements content filtering by accepting parameters like word_count_threshold, extraction_strategy, and chunking_strategy which are used to process and filter the crawled HTML content according to user-specified criteria.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that the arun() method handles content filtering through various parameters to process crawled content. The predicted description gives specific examples of filtering parameters while the ground truth describes it more generally, but they convey the same fundamental functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of the AsyncWebCrawler class using 'async with AsyncWebCrawler(verbose=True)'. This clearly indicates that the class is a primary component for setting up the crawler.",
    "ground_truth_relationship": "The documentation demonstrates the basic usage of AsyncWebCrawler's arun() method through an async context manager, which is directly implemented in the code through __aenter__ and __aexit__ methods along with the core arun() function that processes web crawling requests.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's context manager usage but focuses only on instantiation, missing the core functionality of arun() method which is central to the crawler's operation as described in the ground truth",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code snippet calls the 'arun' method on an AsyncWebCrawler instance ('result = await crawler.arun(url=\"https://www.nbcnews.com/business\")'), indicating that this method is used to perform the crawling operation as demonstrated in the documentation.",
    "ground_truth_relationship": "The code implements the documented basic usage by providing an arun() method that accepts a URL parameter and handles the core web crawling functionality including HTML extraction, caching, and processing while returning a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used for crawling with a URL parameter, but misses significant aspects of the functionality like caching, HTML processing, and the complete CrawlResult object that are central to the implementation as described in the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet accesses the 'markdown' attribute on the crawl result object ('result.markdown[:500]') indicating that the output of the crawling operation includes a markdown representation of the crawled content.",
    "ground_truth_relationship": "The markdown attribute in CrawlResult stores the extracted text content that gets printed in the example code's output after crawling the website.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute contains the extracted text content from the crawled website that gets printed in the example. The predicted description correctly identifies the key relationship between the result object and its markdown content.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly names 'JsonCssExtractionStrategy', highlighting its capability to handle complex, nested HTML structures. This directly points to the class responsible for extracting nested objects, lists, and nested lists.",
    "ground_truth_relationship": "The code implements JsonCssExtractionStrategy class that recursively processes HTML data using BeautifulSoup and schema-based selectors, enabling the advanced nested extraction capabilities described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's core purpose of handling complex nested HTML structures, which aligns with the ground truth's description of recursive HTML processing using BeautifulSoup and schema-based selectors.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple structures, its true potential shines when dealing with complex, nested HTML structures. This section will explore advanced usage scenarios, demonstrating how to extract nested objects, lists, and nested lists.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy extends ExtractionStrategy. Although not directly mentioned in the snippet, ExtractionStrategy is implicitly relevant as it provides the abstract foundation for all extraction strategies including JsonCssExtractionStrategy.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the foundational architecture for implementing the JsonCssExtractionStrategy mentioned in the documentation by defining abstract methods and parallel processing capabilities that enable extraction of complex nested structures.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the key relationship that JsonCssExtractionStrategy extends ExtractionStrategy as a base class that provides foundational functionality. While it's more concise than the ground truth, it doesn't contradict or misunderstand the relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with 'async with AsyncWebCrawler() as crawler:', demonstrating its use as the central web crawling class.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the core functionality shown in the documentation's comprehensive example by providing async context management and extraction methods that support both structured (JsonCssExtractionStrategy) and LLM-based content extraction through its arun method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in web crawling and its async context manager functionality, but misses the crucial aspect of supporting both structured and LLM-based content extraction strategies shown in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun()' method on the AsyncWebCrawler instance to initiate the crawling process with specified extraction strategies.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method executes the core crawling functionality shown in the documentation example by accepting extraction strategies, processing URLs, and returning structured results that can be used for both pattern-based and LLM-based content extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic function of arun() for crawling but misses crucial aspects about its role in supporting both pattern-based and LLM-based extraction strategies, and returning structured results as shown in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The extraction_strategy parameter is assigned an instance of JsonCssExtractionStrategy (with a provided article_schema) to perform structured content extraction.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured extraction pattern shown in the documentation's comprehensive example by using CSS selectors to extract data according to a predefined schema structure with baseSelector and fields properties.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that JsonCssExtractionStrategy is used for structured content extraction with a schema, which aligns with the ground truth's explanation of its purpose for structured extraction using CSS selectors and schema. While the ground truth provides more implementation details, the core relationship is accurately described.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "An instance of LLMExtractionStrategy is created with provider, schema, and instruction parameters to perform semantic analysis on the article content.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the semantic analysis functionality shown in the documentation example, specifically handling the ArticleAnalysis extraction with custom providers, schemas, and instructions as demonstrated in the crawler.arun() call.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship that LLMExtractionStrategy is used for semantic analysis with provider, schema, and instruction parameters, which aligns with its usage shown in the documentation example.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Both JsonCssExtractionStrategy and LLMExtractionStrategy extend from ExtractionStrategy, ensuring they adhere to a common extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines the core interface and parallel processing functionality that enables both the JsonCssExtractionStrategy and LLMExtractionStrategy implementations shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy and LLMExtractionStrategy inherit from ExtractionStrategy, but misses the crucial aspect of parallel processing functionality that the base class provides",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The 'arun()' method returns a CrawlResult object, which encapsulates the crawling results including fields such as extracted_content and media used later in the combined output.",
    "ground_truth_relationship": "The CrawlResult class serves as the structured output container for the extract_article_content function, storing both the pattern-based extraction results (extracted_content) and media assets referenced in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that CrawlResult is the return type of arun() and contains the essential fields (extracted_content, media) used in the example. While the ground truth adds more context, the core relationship described is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "The 'extracted_content' attribute of CrawlResult is accessed to obtain the structured content extracted by the strategy.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores serialized JSON content from both pattern-based and LLM-based extraction strategies as demonstrated in the comprehensive example where it's accessed via pattern_result.extracted_content and analysis_result.extracted_content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that extracted_content contains extracted structured content, but misses that it stores both pattern-based and LLM-based extraction results as serialized JSON, which is a significant aspect shown in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Comprehensive Example\n\nHere's how to combine different selection methods:\n\n```python\nasync def extract_article_content(url: str):\n    # Define structured extraction\n    article_schema = {\n        \"name\": \"Article\",\n        \"baseSelector\": \"article.main\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"text\"}\n        ]\n    }\n    \n    # Define LLM extraction\n    class ArticleAnalysis(BaseModel):\n        key_points: List[str]\n        sentiment: str\n        category: str\n\n    async with AsyncWebCrawler() as crawler:\n        # Get structured content\n        pattern_result = await crawler.arun(\n            url=url,\n            extraction_strategy=JsonCssExtractionStrategy(article_schema),\n            word_count_threshold=10,\n            excluded_tags=['nav', 'footer'],\n            exclude_external_links=True\n        )\n        \n        # Get semantic analysis\n        analysis_result = await crawler.arun(\n            url=url,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"ollama/nemotron\",\n                schema=ArticleAnalysis.schema(),\n                instruction=\"Analyze the article content\"\n            )\n        )\n        \n        # Combine results\n        return {\n            \"article\": json.loads(pattern_result.extracted_content),\n            \"analysis\": json.loads(analysis_result.extracted_content),\n            \"media\": pattern_result.media\n        }\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The 'media' attribute of CrawlResult is used to retrieve media-related data from the crawl results, which is then included in the final combined output.",
    "ground_truth_relationship": "The CrawlResult.media property is used to store media assets collected during crawling, which is shown being returned as part of the combined results in the comprehensive example's extract_article_content function.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly capture that the media attribute is used to store/retrieve media assets during crawling and is included in the final combined output of the extract_article_content function",
      "error_type": ""
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and uses AsyncWebCrawler as the primary interface. The code shows 'from crawl4ai import AsyncWebCrawler' and the use of 'async with AsyncWebCrawler() as crawler', demonstrating instantiation as the crawling interface.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with an arun method that enables asynchronous web crawling exactly as shown in the basic usage documentation, including the async context manager pattern using __aenter__ and __aexit__ methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as the main interface but focuses only on imports and instantiation, missing the crucial aspect of its asynchronous web crawling functionality and implementation of the arun method described in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method on an AsyncWebCrawler instance (result = await crawler.arun(url=\"https://example.com\")), demonstrating its use to initiate a crawl.",
    "ground_truth_relationship": "The code implements the documented basic usage example by providing an async method 'arun()' that accepts a URL parameter and handles web crawling, content extraction, and caching logic to return a CrawlResult object containing the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for web crawling, but misses crucial aspects of its functionality like content extraction, caching, and returning a CrawlResult object that are central to the ground truth description.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Basic Usage\n\nHere's the simplest way to crawl a webpage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After performing a crawl with AsyncWebCrawler.arun(), the documentation snippet prints 'result.markdown', directly accessing the 'markdown' attribute of the CrawlResult object to display the clean markdown content.",
    "ground_truth_relationship": "The CrawlResult.markdown field stores the cleaned webpage content returned by AsyncWebCrawler.arun() which can be accessed as shown in the basic usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that result.markdown attribute contains clean markdown content obtained from the crawl operation, matching the core relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "The documentation snippet directly accesses 'result.media' when iterating over 'videos' and 'audios'. This explicit usage matches the 'media' attribute defined in the CrawlResult class (artifact_id 16), which stores metadata for video and audio elements.",
    "ground_truth_relationship": "The CrawlResult.media dictionary stores extracted video and audio elements as lists of metadata dictionaries, which are then accessed and printed in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core idea that the media dictionary stores video and audio metadata as lists that can be accessed and printed. The predicted description includes a reference to CrawlResult class which doesn't detract from the main relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Video and Audio Content\nThe library extracts video and audio elements with their metadata:\n\n```python\n# Process videos\nfor video in result.media[\"videos\"]:\n    print(f\"Video source: {video['src']}\")\n    print(f\"Type: {video['type']}\")\n    print(f\"Duration: {video.get('duration')}\")\n    print(f\"Thumbnail: {video.get('poster')}\")\n\n# Process audio\nfor audio in result.media[\"audios\"]:\n    print(f\"Audio source: {audio['src']}\")\n    print(f\"Type: {audio['type']}\")\n    print(f\"Duration: {audio.get('duration')}\")\n```",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "Although the example uses a variable 'result' without naming its type explicitly, the pattern of accessing 'media' implies that 'result' is an instance of the CrawlResult class (artifact_id 8), which provides the public interface including the 'media' attribute.",
    "ground_truth_relationship": "The CrawlResult class stores video and audio metadata in its media dictionary field, which the documentation demonstrates how to iterate through and access using media['videos'] and media['audios'] paths.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify that the CrawlResult class uses its media dictionary field to store and access video/audio metadata. While the predicted focuses more on the class instance usage and the ground truth provides more implementation details, they describe the same core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a usage example that explicitly calls 'crawler.arun(...)', which directly maps to the AsyncWebCrawler.arun() method responsible for crawling a URL and returning a CrawlResult object.",
    "ground_truth_relationship": "The arun() method implements HTML cleaning by processing raw HTML through sanitization and extraction strategies, with configurable excluded tags and data attributes as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its role in crawling URLs, but misses the core focus on HTML cleaning/sanitization functionality emphasized in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Cleaned HTML\n\nSanitized HTML with unnecessary elements removed. Automatically:\n- Removes scripts and styles\n- Cleans up formatting\n- Preserves semantic structure\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    excluded_tags=['form', 'header', 'footer'],  # Additional tags to remove\n    keep_data_attributes=False  # Remove data-* attributes\n)\nprint(result.cleaned_html)\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "In the example, the attribute 'cleaned_html' is accessed on the result of the arun() call (i.e., result.cleaned_html). This directly corresponds to the public attribute 'cleaned_html' defined in CrawlResult.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML string after removing unwanted tags and attributes as shown in the documentation example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that cleaned_html is an attribute accessed from the result of arun(), which aligns with the ground truth showing it stores sanitized HTML after processing.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")' which directly corresponds to the AsyncWebCrawler.arun() method. This method is responsible for orchestrating the crawling process and returning a CrawlResult object.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method processes webpage content and returns a CrawlResult object containing metadata fields like title, description, and language that can be accessed through the .metadata property as shown in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method and its role in crawling and returning a CrawlResult, but misses the crucial aspect about metadata extraction and processing which is the main focus of the ground truth relationship.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Metadata Extraction\n\nCrawl4AI automatically extracts and processes page metadata, providing valuable information about the content:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\nmetadata = result.metadata\nprint(f\"Title: {metadata['title']}\")\nprint(f\"Description: {metadata['description']}\")\nprint(f\"Keywords: {metadata['keywords']}\")\nprint(f\"Author: {metadata['author']}\")\nprint(f\"Published Date: {metadata['published_date']}\")\nprint(f\"Modified Date: {metadata['modified_date']}\")\nprint(f\"Language: {metadata['language']}\")\n```",
    "artifact_title": "CrawlResult.metadata",
    "predicted_relationship": "After calling arun(), the snippet accesses 'result.metadata' to retrieve metadata such as title, description, keywords, and more. This directly maps to the 'metadata' attribute defined in the CrawlResult class.",
    "ground_truth_relationship": "The metadata property of CrawlResult stores extracted page metadata as an optional dictionary containing fields like title, description, keywords, author, and dates as documented in the usage example.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that metadata is accessed through result.metadata and contains extracted page information, which aligns with the ground truth's explanation of metadata being stored as an optional dictionary with page metadata fields",
      "error_type": ""
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly calls 'crawler.arun(url=\"https://example.com\")'. This demonstrates the use of the arun() method to initiate a crawl, which is implemented in the AsyncWebCrawler class.",
    "ground_truth_relationship": "The documented media selection functionality is implemented through the arun() method which performs the web crawl and returns a CrawlResult object containing structured media data that can be accessed through the media dictionary with keys for images, videos, and audios.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the arun() method's role in crawling, but misses the crucial media selection functionality described in the ground truth that allows accessing different media types through the returned CrawlResult object",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Media Selection\n\nSelect specific types of media:\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different media types\nimages = result.media[\"images\"]  # List of image details\nvideos = result.media[\"videos\"]  # List of video details\naudios = result.media[\"audios\"]  # List of audio details\n\n# Image with metadata\nfor image in images:\n    print(f\"URL: {image['src']}\")\n    print(f\"Alt text: {image['alt']}\")\n    print(f\"Description: {image['desc']}\")\n    print(f\"Relevance score: {image['score']}\")\n```",
    "artifact_title": "CrawlResult.media",
    "predicted_relationship": "After invoking 'crawler.arun()', the resulting object 'result' is used to access 'result.media'. This attribute holds a dictionary of media types (images, videos, audios) and is defined as part of the CrawlResult model.",
    "ground_truth_relationship": "The CrawlResult.media dictionary property stores categorized lists of media elements (images, videos, audios) that are accessed by type keys in the documented example code.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that result.media is a dictionary containing categorized media elements (images, videos, audios) that can be accessed by type keys. The predicted description captures the essential relationship between the CrawlResult object and its media dictionary property.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly shows a call to 'crawler.arun()' with parameters 'process_iframes=True' and 'remove_overlay_elements=True'. This explicit usage demonstrates that the async web crawler's 'arun' method is being used to perform crawling with iframe processing and overlay removal.",
    "ground_truth_relationship": "The arun() method implements iframe processing through its kwargs parameter, which accepts process_iframes and remove_overlay_elements options that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun() method handles iframe processing functionality via parameters, which matches the ground truth's explanation that this happens through kwargs being passed to crawler_strategy.crawl()",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "The 'AsyncWebCrawler.arun()' method internally delegates the crawling task to its crawler strategy. In this project, the default strategy is 'AsyncPlaywrightCrawlerStrategy', which examines keyword arguments such as 'process_iframes' and 'remove_overlay_elements'. Although not directly mentioned in the snippet, its behavior is implicitly responsible for processing iframes and removing overlays.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements iframe content processing through its process_iframes method, which locates iframes, extracts their content, and replaces them with div elements containing the extracted content when process_iframes=True is passed to the crawl method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the strategy class handles iframe processing and overlay removal based on passed arguments. While the ground truth provides more technical implementation details, the core relationship and functionality are accurately captured in the predicted description.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Iframe Content\n\nProcess content inside iframes:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    process_iframes=True,  # Extract iframe content\n    remove_overlay_elements=True  # Remove popups/modals that might block iframes\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "The 'AsyncPlaywrightCrawlerStrategy' that processes iframes and removes overlay elements is a concrete implementation of the abstract base class 'AsyncCrawlerStrategy'. This base class defines the crawling interface, establishing the architectural pattern used, even though it is not directly referenced in the snippet.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the crawl method that implements the iframe processing capabilities documented in the example through its **kwargs parameter, which can accept process_iframes and remove_overlay_elements flags.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as an abstract base class defining the crawling interface, but incorrectly focuses on a concrete implementation (AsyncPlaywrightCrawlerStrategy) that isn't shown in the code and misses the direct connection between the abstract class's kwargs parameter and iframe processing capability.",
      "error_type": "focus_shift_and_omission"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports JsonCssExtractionStrategy from crawl4ai.extraction_strategy and instantiates it with a provided schema for structured extraction. This shows its direct usage in converting dynamic page content into structured data.",
    "ground_truth_relationship": "JsonCssExtractionStrategy class implements pattern-based extraction by parsing HTML with BeautifulSoup and applying the schema's CSS selectors to extract structured data from matched elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies JsonCssExtractionStrategy's role in structured extraction, but misses the core implementation detail about using BeautifulSoup and CSS selectors to parse and extract data from HTML elements",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly imports LLMExtractionStrategy and uses it by passing provider details, a schema derived from a ContentAnalysis model, and an extraction instruction. This demonstrates its role in leveraging LLMs to analyze dynamic content during crawling.",
    "ground_truth_relationship": "The LLMExtractionStrategy code directly implements the documented functionality of analyzing dynamic content by processing HTML content through LLM models with configurable providers, schemas, and instructions as shown in the example where it processes content after waiting for '.full-content' elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the code and documentation - both describe LLMExtractionStrategy being used to analyze dynamic content using LLM providers, schemas, and instructions, which matches the implementation shown in the code.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Using with Extraction Strategies\n\nCombine page interaction with structured extraction:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n\n# Pattern-based extraction after interaction\nschema = {\n    \"name\": \"Dynamic Items\",\n    \"baseSelector\": \".item\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n        {\"name\": \"description\", \"selector\": \".desc\", \"type\": \"text\"}\n    ]\n}\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    wait_for=\"css:.item:nth-child(10)\",  # Wait for 10 items\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n\n# Or use LLM to analyze dynamic content\nclass ContentAnalysis(BaseModel):\n    topics: List[str]\n    summary: str\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"document.querySelector('.show-more').click();\",\n    wait_for=\"css:.full-content\",\n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"ollama/nemotron\",\n        schema=ContentAnalysis.schema(),\n        instruction=\"Analyze the full content\"\n    )\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code example calls 'crawler.arun' with parameters including the extraction_strategy argument. Although the snippet does not focus on the crawler's class, the usage of 'arun' demonstrates the integration point where extraction strategies (such as JsonCssExtractionStrategy and LLMExtractionStrategy) are applied during the crawling process.",
    "ground_truth_relationship": "The arun() method implements the documented functionality by accepting extraction_strategy objects like JsonCssExtractionStrategy and LLMExtractionStrategy, along with JavaScript code and wait conditions, to perform web crawling with structured data extraction.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the arun method accepts and integrates extraction strategies during the crawling process, which aligns with the ground truth's description of the method implementing functionality for structured data extraction using different extraction strategy objects.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet explicitly shows a call to 'crawler.arun(url=\"https://example.com\")', which directly invokes the 'arun' method to perform a crawl and retrieve output in various formats.",
    "ground_truth_relationship": "The arun() method implements the core crawling functionality that populates the different output formats (html, cleaned_html, markdown, fit_markdown) mentioned in the documentation through its processing and sanitization of the crawled content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that arun() is used to perform crawling and retrieve different output formats, which aligns with the ground truth's explanation that it implements the core crawling functionality that produces these formats.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.html",
    "predicted_relationship": "In the usage example, 'result.html' is accessed to retrieve the raw HTML output. This directly maps to the 'html' attribute defined in the CrawlResult model.",
    "ground_truth_relationship": "The code defines the 'html' property as a string type to store the raw HTML content mentioned in the documentation's Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the 'html' attribute stores raw HTML output, which aligns with the ground truth's statement about the html property being a string type for storing raw HTML content.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.cleaned_html",
    "predicted_relationship": "The documentation sample uses 'result.cleaned_html' to obtain sanitized HTML. This property is part of the CrawlResult model and represents the cleaned version of the original HTML.",
    "ground_truth_relationship": "The cleaned_html property in CrawlResult stores the sanitized HTML version of the crawled webpage as an optional string value, providing access to a cleaned version of the original HTML content.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that cleaned_html provides access to a sanitized version of the HTML content, which aligns with the ground truth's explanation. The minor omission of it being an optional string value doesn't change the fundamental understanding.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet explicitly assigns 'result.markdown' to a variable, indicating that the markdown version of the webpage is provided as part of the CrawlResult output.",
    "ground_truth_relationship": "The markdown field in CrawlResult stores the HTML content converted to standard markdown format as shown in the documentation example where it can be accessed via result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that markdown is available as an output format through result.markdown, which aligns with the ground truth's explanation of the markdown field storing HTML converted to markdown format.",
      "error_type": ""
    }
  },
  {
    "document_text": "# Output Formats\n\nCrawl4AI provides multiple output formats to suit different needs, from raw HTML to structured data using LLM or pattern-based extraction.\n\n## Basic Formats\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Access different formats\nraw_html = result.html           # Original HTML\nclean_html = result.cleaned_html # Sanitized HTML\nmarkdown = result.markdown       # Standard markdown\nfit_md = result.fit_markdown    # Most relevant content in markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The snippet demonstrates accessing 'result.fit_markdown' to get the most relevant content in a refined markdown format, which is exactly what this attribute represents in the CrawlResult model.",
    "ground_truth_relationship": "The fit_markdown property stores the most relevant content extracted from a webpage in markdown format as documented in the Basic Formats section.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that fit_markdown provides the most relevant content in markdown format, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation code snippet shows 'async with AsyncWebCrawler() as crawler:', which clearly instantiates and uses the AsyncWebCrawler class to perform crawling. This establishes the entry point for the crawling process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the fit_markdown feature through its aprocess_html method, which extracts and processes the main content using a scraping strategy that identifies and preserves relevant content blocks while excluding boilerplate elements.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's role in performing crawling, but misses the key focus of the ground truth - the fit_markdown feature and content extraction functionality for identifying and preserving relevant content while removing boilerplate elements.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'await crawler.arun(url=\"https://example.com\")', indicating that the arun() method of AsyncWebCrawler is directly used to perform the crawl and obtain a CrawlResult. This method is responsible for retrieving the content that will later be processed.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler.arun() method that processes webpage content and enables the fit_markdown feature by passing the crawled HTML through extraction and chunking strategies to identify relevant content blocks while filtering out boilerplate elements.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() performs webpage crawling, but misses the crucial aspect of content extraction and processing through fit_markdown feature mentioned in the ground truth. It doesn't mention the extraction/chunking strategies that filter boilerplate content.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The documentation directly accesses 'result.fit_markdown' (e.g., 'main_content = result.fit_markdown'), showcasing its usage to extract the most relevant and refined content from the crawl result while filtering out extraneous elements.",
    "ground_truth_relationship": "The CrawlResult class defines fit_markdown as an optional string property that stores the extracted main content after applying content extraction heuristics to remove boilerplate elements from webpages.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey that fit_markdown is a property used to access extracted main content after filtering out boilerplate elements from webpages. The predicted description accurately captures the core functionality and purpose, even if it doesn't explicitly mention it being an optional string property.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Fit Markdown: Smart Content Extraction\nOne of Crawl4AI's most powerful features is `fit_markdown`. This feature uses advanced heuristics to identify and extract the main content from a webpage while excluding irrelevant elements.\n\n#### How Fit Markdown Works\n- Analyzes content density and distribution\n- Identifies content patterns and structures\n- Removes boilerplate content (headers, footers, sidebars)\n- Preserves the most relevant content blocks\n- Maintains content hierarchy and formatting\n\n#### Perfect For:\n- Blog posts and articles\n- News content\n- Documentation pages\n- Any page with a clear main content area\n\n#### Not Recommended For:\n- E-commerce product listings\n- Search results pages\n- Social media feeds\n- Pages with multiple equal-weight content sections\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\n\n# Get the most relevant content\nmain_content = result.fit_markdown\n\n# Compare with regular markdown\nall_content = result.markdown\n\nprint(f\"Fit Markdown Length: {len(main_content)}\")\nprint(f\"Regular Markdown Length: {len(all_content)}\")\n```\n\n#### Example Use Case\n```python\nasync def extract_article_content(url: str) -> str:\n    \"\"\"Extract main article content from a blog or news site.\"\"\"\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url)\n        \n        # fit_markdown will focus on the article content,\n        # excluding navigation, ads, and other distractions\n        return result.fit_markdown\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The snippet contrasts 'result.fit_markdown' with 'result.markdown' (e.g., 'all_content = result.markdown'), highlighting that the standard markdown contains all content while 'fit_markdown' focuses on the main content. This demonstrates the feature's purpose of filtering out noise.",
    "ground_truth_relationship": "The markdown property serves as the base text extraction method that captures all webpage content, contrasting with fit_markdown which provides filtered, relevant content only.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey the key relationship between markdown and fit_markdown: that markdown captures all content while fit_markdown provides filtered/relevant content only. The predicted description effectively communicates this contrast through concrete examples.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly shows the instantiation of AsyncWebCrawler using 'async with AsyncWebCrawler() as crawler:', indicating that this is the primary class used for crawling in the proxy rotation example.",
    "ground_truth_relationship": "The AsyncWebCrawler class shown in the code provides the foundational structure referenced in the documentation example, with arun() and __aenter__ methods that enable the async context manager pattern and proxy updating functionality through its crawler_strategy attribute.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is the main class used with async context managers for crawling, which aligns with the ground truth's explanation of the class's role and its async context manager capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The usage example in the snippet includes the call 'result = await crawler.arun(url=url)', which explicitly demonstrates the invocation of the arun() method to perform a crawl for a given URL.",
    "ground_truth_relationship": "The arun() method in AsyncWebCrawler implements the core crawling functionality referenced in the documentation, accepting a URL parameter and supporting proxy configuration through its crawler_strategy component.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted text correctly identifies the core relationship showing how arun() is called to perform crawling operations with a URL parameter, which aligns with the ground truth's description of arun() as implementing core crawling functionality and accepting URL parameters.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned in the snippet, AsyncWebCrawler internally instantiates AsyncPlaywrightCrawlerStrategy as its default crawling strategy. This strategy manages properties such as the proxy setting (through its 'proxy' attribute) that the update_proxy call in the snippet is expected to affect.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements proxy support through its proxy handling in the start() method, where it creates ProxySettings objects that align with the documentation's example of rotating proxies for web crawling.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship regarding proxy support in AsyncPlaywrightCrawlerStrategy, where the class handles proxy configuration for web crawling. While they emphasize different aspects (the predicted focusing on initialization and the ground truth on implementation details), they don't contradict each other.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Rotating Proxies\n\nExample using a proxy rotation service:\n\n```python\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync with AsyncWebCrawler() as crawler:\n    # Update proxy for each request\n    for url in urls:\n        proxy = await get_next_proxy()\n        crawler.update_proxy(proxy)\n        result = await crawler.arun(url=url)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends the abstract class AsyncCrawlerStrategy, thereby conforming to the expected crawling interface. This inheritance relationship provides the underlying contract that governs crawling operations, including proxy handling, even though the proxy update method itself is not directly defined.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class defines the core interface methods that enable proxy rotation functionality shown in the documentation through its update_user_agent method and abstract crawl methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions correctly identify AsyncCrawlerStrategy as an abstract class that defines core interface methods for crawling functionality. While they emphasize different aspects, the predicted description captures the essential relationship of how the abstract class provides the contract for crawling operations.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows a direct usage of the crawler.arun() method in the example (result = await crawler.arun(url=\"https://example.com\")), which clearly indicates that this method is being invoked to perform the crawling operation.",
    "ground_truth_relationship": "The code implements an async crawling method that processes HTML content and extracts markdown-formatted text through an extraction strategy, which directly supports the documented fit_markdown feature for retrieving main content from web pages.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures that arun() is used for crawling, but misses the core functionality of markdown extraction and content processing described in the ground truth.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Fit Markdown\n\nMost relevant content extracted and converted to markdown. Ideal for:\n- Article extraction\n- Main content focus\n- Removing boilerplate\n\n```python\nresult = await crawler.arun(url=\"https://example.com\")\nprint(result.fit_markdown)  # Only the main content\n```",
    "artifact_title": "CrawlResult.fit_markdown",
    "predicted_relationship": "The example code prints result.fit_markdown immediately after the crawl, which indicates that the fit_markdown attribute of the CrawlResult object is used to present the main content extracted and converted to markdown.",
    "ground_truth_relationship": "The fit_markdown property holds a string containing the cleaned and extracted main content from crawled web pages as markdown format, with boilerplate removed.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that fit_markdown contains extracted main content in markdown format, showing the same core relationship as the ground truth.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly demonstrates the usage of the 'arun' method by calling 'crawler.arun()' with various customizable options such as 'word_count_threshold', 'remove_overlay_elements', and 'process_iframes'.",
    "ground_truth_relationship": "The code implements an asynchronous crawler method that accepts the documented options like word_count_threshold and handles various crawling parameters through its **kwargs argument system, allowing for flexible configuration as shown in the documentation examples.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that the arun method accepts various customizable options for crawling configuration, which aligns with the ground truth's explanation of the method's parameter handling and flexibility.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "Although not named in the snippet, the 'crawler' variable is implied to be an instance of AsyncWebCrawler, whose public interface provides the 'arun()' method. This class underpins the execution of the crawl process.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented configuration options through its arun method, which accepts parameters like word_count_threshold and other kwargs that directly correspond to the basic crawling options shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler class provides the arun() method interface for crawling, which aligns with the ground truth's explanation that the class implements the documented configuration options through its arun method. While the predicted description is less detailed, it captures the core relationship accurately.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Adding Basic Options\n\nCustomize your crawl with these common options:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Inside the implementation of AsyncWebCrawler.arun(), the crawl process delegates to an underlying crawler strategy. The options 'remove_overlay_elements' and 'process_iframes' passed in the snippet are handled within AsyncPlaywrightCrawlerStrategy's crawl() method, which processes iframes and removes overlays.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements the documented crawling options through its crawl method, which processes parameters like process_iframes=True for handling iframe content and remove_overlay_elements=True for removing popups/modals through dedicated helper methods.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions explain that the AsyncPlaywrightCrawlerStrategy class implements the documented crawling options (process_iframes and remove_overlay_elements) through its crawl method. The predicted description accurately captures this core relationship, even if it's slightly less detailed than the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly creates an instance of AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler:', demonstrating its role as the high-level orchestrator for crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class supports the wait_for parameter through its arun method's **kwargs parameter, which gets passed to the crawler_strategy's crawl method for handling page load conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler as a high-level orchestrator, but misses the key relationship described in the ground truth about how it handles the wait_for parameter through its arun method and crawler_strategy",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The sample code calls the 'arun()' method on the AsyncWebCrawler instance to perform the crawling. This method is responsible for initiating the crawl process while handling parameters such as 'wait_for' that control dynamic content loading.",
    "ground_truth_relationship": "The `arun()` method implements the documented `wait_for` parameter functionality by accepting it through its `**kwargs` argument and passing it to the crawler_strategy.crawl() method, where it's used to control page loading conditions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() handles crawling and parameters, but misses the crucial aspect that it specifically implements the wait_for functionality by passing it through to crawler_strategy.crawl()",
      "error_type": "missing_key_functionality"
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates JsonCssExtractionStrategy with a defined schema to extract commit information using CSS selectors. This demonstrates its role as a content extraction strategy within the crawling process.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based data extraction functionality used in the documentation's example by parsing HTML elements with BeautifulSoup according to the specified baseSelector and field selectors defined in the schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that JsonCssExtractionStrategy uses a schema to extract data from HTML using CSS selectors. The predicted version mentions the core functionality without contradicting the ground truth's more detailed explanation of how it uses BeautifulSoup for parsing.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The example explicitly calls 'await crawler.crawler_strategy.kill_session(session_id)' to end the crawled session and clean up resources after the crawling operation.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects after the wait_for-based crawling is complete, as shown in the documentation's final step where crawler.crawler_strategy.kill_session(session_id) is called.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture that kill_session is called to clean up browser resources and end the session after crawling is complete, with the predicted description correctly identifying the key functionality even if it doesn't detail all the specific cleanup steps.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not directly mentioned by name in the snippet, AsyncPlaywrightCrawlerStrategy is implicitly used by AsyncWebCrawler as its default crawling strategy. Its internal methods (like smart_wait) handle the 'wait_for' parameter, enabling the dynamic wait functionality described in the documentation.",
    "ground_truth_relationship": "The code implements the documented wait_for functionality through the smart_wait method in AsyncPlaywrightCrawlerStrategy, which evaluates either CSS selectors or JavaScript functions to determine when a page has finished loading dynamic content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that AsyncPlaywrightCrawlerStrategy implements wait_for functionality through the smart_wait method to handle dynamic content loading. The predicted description correctly identifies the core relationship while just being less detailed about the implementation specifics.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Advanced Technique 3: Using the `wait_for` Parameter Crawl4AI provides a `wait_for` parameter that allows you to specify a condition to wait for before considering the page fully loaded. This can be particularly useful for dynamic content. Here's an example: ```python async def wait_for_parameter_crawl(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/example/repo/commits/main\" session_id = \"wait_for_session\" all_commits = [] js_next_page = \"\"\" const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length > 0) { window.lastCommit = commits[0].textContent.trim(); } const button = document.querySelector('a.pagination-next'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.commit-item h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.lastCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.commit-item\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.commit-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3): result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.commit-item\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True ) commits = json.loads(result.extracted_content) all_commits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") await crawler.crawler_strategy.kill_session(session_id) print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\") asyncio.run(wait_for_parameter_crawl()) ``` This technique separates the JavaScript execution (clicking the \"next\" button) from the waiting condition, providing more flexibility and clarity in some scenarios.",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncCrawlerStrategy is the abstract base class for AsyncPlaywrightCrawlerStrategy. While it is not directly instantiated in the sample, it defines the interface that AsyncPlaywrightCrawlerStrategy implements, making it an integral part of the crawling framework.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational structure for implementing waiting behaviors through its crawl method, which the documentation's wait_for parameter utilizes to control page load completion conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture that AsyncCrawlerStrategy is a foundational abstract class that defines core crawling behaviors, with the predicted focusing on inheritance relationships and the ground truth emphasizing its role in wait behaviors. While they emphasize different aspects, they don't contradict each other and both correctly identify the class's fundamental purpose.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly imports RegexChunking from 'crawl4ai.chunking_strategy' and uses it by instantiating it with a regex pattern argument, thereby demonstrating its role as a chunking strategy.",
    "ground_truth_relationship": "The RegexChunking class implementation splits text into chunks using regex patterns supplied in its constructor, with a default pattern of '\\n\\n' that matches double newlines as shown in both the documentation example and code definition.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies RegexChunking as a chunking strategy and its import/usage pattern, but misses the core functionality of splitting text based on regex patterns with a default double newline pattern",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The snippet uses AsyncWebCrawler to establish an asynchronous crawling context. It is instantiated with a verbose flag and later used to initiate the crawl with the RegexChunking strategy.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the documented RegexChunking functionality through its arun() method, which accepts a chunking_strategy parameter defaulting to RegexChunking() for splitting extracted text content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description captures the key relationship of using AsyncWebCrawler with RegexChunking strategy, even though it doesn't mention all implementation details like the default parameter. Both descriptions focus on the core functionality of using AsyncWebCrawler for crawling with RegexChunking.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Adding a Chunking Strategy \ud83e\udde9\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text based on a given regex pattern.\n\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n        )\n        print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "Although not mentioned directly in the documentation snippet, ChunkingStrategy is the base class for RegexChunking. This inheritance relationship makes it an integral part of the traceability chain.",
    "ground_truth_relationship": "The abstract ChunkingStrategy class serves as the base interface that RegexChunking extends to implement text splitting functionality through its required chunk method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that ChunkingStrategy is the base class for RegexChunking and captures the inheritance relationship between them, which aligns with the ground truth's explanation of ChunkingStrategy being the abstract base interface that RegexChunking extends.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet directly calls 'crawler.arun(...)' with parameters such as 'wait_for', 'js_code', 'process_iframes', and 'delay_before_return_html'. This clearly indicates that the arun() method of the AsyncWebCrawler is in use.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements dynamic content handling by accepting custom JavaScript code and wait conditions through **kwargs, which directly supports the documented features like wait_for conditions and js_code execution for lazy-loading content.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() is used with parameters, but misses the key point about how it implements dynamic content handling through kwargs to support JavaScript execution and wait conditions.",
      "error_type": "incomplete_understanding"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not explicitly named in the snippet, the parameters such as 'wait_for', 'process_iframes', 'js_code', and 'delay_before_return_html' are handled internally by the AsyncPlaywrightCrawlerStrategy. This strategy is used by AsyncWebCrawler.arun() to process dynamic content and embedded iframes.",
    "ground_truth_relationship": "The code implements dynamic content handling through the smart_wait and crawl methods that support JavaScript execution, iframe processing, and configurable delays as documented in the example configuration snippets.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that AsyncPlaywrightCrawlerStrategy handles dynamic content through parameters like wait_for, process_iframes, js_code, and delays, which aligns with the ground truth's description of dynamic content handling through smart_wait and crawl methods.",
      "error_type": null
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy, which processes the dynamic content, is a subclass of AsyncCrawlerStrategy. This inheritance establishes the contract for the crawling operations that are ultimately used by the arun() method.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract class provides the foundational methods needed to implement the dynamic content handling capabilities described in the documentation, including crawl() and set_hook() methods that enable JavaScript execution and custom wait conditions.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncCrawlerStrategy as a base class for crawler implementations, but incorrectly focuses on a specific subclass (AsyncPlaywrightCrawlerStrategy) that isn't mentioned in the code/docs and misses the core purpose of providing dynamic content handling capabilities.",
      "error_type": "missing_core_purpose"
    }
  },
  {
    "document_text": "## Handling Dynamic Content\n\nConfigure browser to handle dynamic content:\n\n```python\n# Wait for dynamic content\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    wait_for=\"js:() => document.querySelector('.content').children.length > 10\",\n    process_iframes=True     # Process iframe content\n)\n\n# Handle lazy-loaded images\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n    delay_before_return_html=2.0  # Wait for images to load\n)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The variable 'crawler' in the snippet is understood to be an instance of AsyncWebCrawler, which provides the arun() method. This establishes how the overall crawling functionality is organized within the framework.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements dynamic content handling through its arun method, which accepts parameters like 'wait_for', 'js_code', and 'delay_before_return_html' to control browser behavior for dynamic content processing and iframe handling as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the AsyncWebCrawler class and arun() method correctly, but misses the core relationship about dynamic content handling capabilities described in the ground truth. It omits the key functionality for handling dynamic content, lazy loading, and iframe processing.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet shows an explicit use of AsyncWebCrawler via the 'async with AsyncWebCrawler(verbose=True) as crawler' statement. This clearly demonstrates that the class is being instantiated to perform crawl operations with caching functionality.",
    "ground_truth_relationship": "The code implements caching functionality through the AsyncWebCrawler class which uses async_db_manager to store and retrieve crawl results, while providing a bypass_cache parameter in the arun method that controls whether to use cached results as demonstrated in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's instantiation and caching functionality, but misses the crucial detail about the async_db_manager implementation and the bypass_cache parameter's role in controlling cache behavior",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the code example, the 'arun' method of the AsyncWebCrawler instance is explicitly called to perform the crawling operation. This method is responsible for initiating the crawl and returning the result.",
    "ground_truth_relationship": "The code implements caching by checking for cached content using async_db_manager.aget_cached_url(url) when bypass_cache is False, while the documentation demonstrates this functionality through example code showing two crawls - one that uses cache and another that bypasses it.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic crawling functionality of arun() but misses the core caching relationship that is central to the ground truth. The caching mechanism is a crucial aspect demonstrated in both the code and documentation.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Understanding Parameters \ud83e\udde0\n\nBy default, Crawl4AI caches the results of your crawls. This means that subsequent crawls of the same URL will be much faster! Let's see this in action.\n\n```python\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # First crawl (caches the result)\n        result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(f\"First crawl result: {result1.markdown[:100]}...\")\n\n        # Force to crawl again\n        result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n        print(f\"Second crawl result: {result2.markdown[:100]}...\")\n\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "After the crawl operation, the example code prints a portion of 'result1.markdown', demonstrating an explicit use of the 'markdown' attribute from the CrawlResult object. This attribute holds a processed version of the page content.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the crawled webpage content in markdown format, which is demonstrated in the documentation example where it's accessed to print the first 100 characters of each crawl result.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute stores the crawled webpage content in markdown format and show it being accessed to display content. The predicted description captures the core relationship even if it doesn't mention the specific 100-character limit.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly describes how a schema is used to extract data using CSS selectors along with defined fields. The class 'JsonCssExtractionStrategy' directly implements this approach by taking a schema (with keys such as 'baseSelector' and 'fields') and returning structured JSON output.",
    "ground_truth_relationship": "The code implements the documented schema structure by using BeautifulSoup's select() method to extract data according to the baseSelector and fields defined in the schema configuration.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately capture how the JsonCssExtractionStrategy uses a schema with baseSelector and fields to extract structured data using CSS selectors. The predicted description correctly identifies the core functionality and purpose, even if it uses slightly different wording.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n- `name`: A descriptive name for the extraction task.\n- `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n- `fields`: An array of fields to extract from each element:\n  - `name`: The name to give the extracted data.\n  - `selector`: The CSS selector to find the specific data within the base element.\n  - `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n1. **Speed**: CSS selectors are fast to execute, making this method efficient for large datasets.\n2. **Precision**: You can target exactly the elements you need.\n3. **Structured Output**: The result is already structured as JSON, ready for further processing.\n4. **No External Dependencies**: Unlike LLM-based strategies, this doesn't require any API calls to external services.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "The 'JsonCssExtractionStrategy' class extends the abstract base class 'ExtractionStrategy'. Although the documentation does not mention 'ExtractionStrategy' explicitly, it underpins the extraction functionality by defining the interface that 'JsonCssExtractionStrategy' implements.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the abstract foundation that enables different extraction methods like JsonCssExtractionStrategy to implement specific extraction logic while sharing common parallel processing capabilities through its run() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core relationship that ExtractionStrategy is an abstract base class that provides a foundation for other extraction strategies, with the predicted description correctly identifying the inheritance relationship and interface implementation aspects.",
      "error_type": "none"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly shows the instantiation of AsyncWebCrawler with the 'browser_type' parameter set to 'chromium', 'firefox', or 'webkit'. This demonstrates how users can configure the browser behavior through the public interface of AsyncWebCrawler.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser type selection through its crawler_strategy parameter, which accepts different browser engines (chromium, firefox, webkit) as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "While the predicted description correctly identifies that AsyncWebCrawler supports different browser types, it implies that browser_type is passed directly to AsyncWebCrawler's constructor, when in fact it's handled through the crawler_strategy parameter as noted in the ground truth.",
      "error_type": "implementation_detail_mismatch"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "Within the usage examples, the 'arun()' method of AsyncWebCrawler is called to perform a crawl operation. This reinforces the functional aspect of browser configuration by actually executing the crawl with the specified browser type.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method implements the browser engine selection documented in the configuration guide by accepting a browser_type parameter that determines which engine (Chromium, Firefox, or WebKit) will be used for crawling the specified URL.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies the connection between the arun() method and browser configuration for crawling, but fails to mention the key relationship regarding browser_type selection which is central to the ground truth description",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "AsyncWebCrawler internally instantiates an AsyncPlaywrightCrawlerStrategy when no custom crawler strategy is provided. This strategy is responsible for handling browser-specific configurations (choosing between Chromium, Firefox, or WebKit) based on the 'browser_type' parameter.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser type selection through its start() method, where it uses the browser_type parameter to launch either Chromium (default), Firefox, or WebKit browsers as documented in the configuration guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the same core relationship - that the AsyncPlaywrightCrawlerStrategy handles browser type selection between Chromium (default), Firefox, and WebKit based on the browser_type parameter. The predicted description focuses more on the instantiation aspect while the ground truth focuses on implementation details, but the fundamental relationship being described is the same.",
      "error_type": null
    }
  },
  {
    "document_text": "# Browser Configuration\n\nCrawl4AI supports multiple browser engines and offers extensive configuration options for browser behavior.\n\n## Browser Types\n\nChoose from three browser engines:\n\n```python\n# Chromium (default)\nasync with AsyncWebCrawler(browser_type=\"chromium\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\") as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```",
    "artifact_title": "AsyncCrawlerStrategy",
    "predicted_relationship": "AsyncPlaywrightCrawlerStrategy extends AsyncCrawlerStrategy. This base class defines the abstract methods for crawling, ensuring that the strategy used for browser configuration adheres to the expected interface.",
    "ground_truth_relationship": "The AsyncCrawlerStrategy abstract base class defines the core interface methods that each browser type (Chromium, Firefox, WebKit) must implement to provide unified crawling functionality.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncCrawlerStrategy is a base class defining abstract methods that enforces an interface for browser configuration/crawling, which aligns with the ground truth's explanation of it being an abstract base class defining core interface methods for different browser types.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The documentation snippet explicitly instantiates CosineStrategy with parameters such as 'semantic_filter', 'word_count_threshold', 'top_k', 'sim_threshold', and 'max_dist'. This shows that CosineStrategy is the extraction strategy used for article content extraction, product review analysis, and technical documentation use cases.",
    "ground_truth_relationship": "The CosineStrategy class implements configurable text extraction through cosine similarity matching, where different use cases (article content, product reviews, technical documentation) are achieved by adjusting the semantic_filter, word_count_threshold, top_k, and sim_threshold parameters as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core relationship - that CosineStrategy is used for different text extraction use cases by configuring parameters like semantic_filter, word_count_threshold, top_k, and sim_threshold. Both descriptions emphasize the configurable nature and common use cases.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Use Cases\n\n### 1. Article Content Extraction\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n### 2. Product Review Analysis\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n### 3. Technical Documentation\n```python\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code shows 'result = await crawler.arun(...)' where the 'arun' method is called with an extraction_strategy parameter set to the CosineStrategy instance. This indicates that the AsyncWebCrawler.arun() method is used to execute the crawling process while leveraging the provided extraction strategy.",
    "ground_truth_relationship": "The arun() method implements the documented use cases by accepting flexible extraction_strategy parameters that control content filtering, word count thresholds, and similarity settings as shown in the three example configurations.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that arun() accepts an extraction_strategy parameter and executes crawling, but misses the crucial aspect of how it implements the different use cases with flexible configuration options for content filtering, thresholds and similarity settings",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation explicitly mentions 'LLMExtractionStrategy' as a strategy for using language models to extract structured or relevant content asynchronously. This directly corresponds to the class 'LLMExtractionStrategy' in the code which provides the LLM-based extraction functionality.",
    "ground_truth_relationship": "The code implements an asynchronous web content extraction strategy that uses LLMs to process and structure web data, directly fulfilling the documentation's description of using Language Models for structured data extraction from web pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship - that LLMExtractionStrategy enables LLM-based extraction of structured data from web pages asynchronously, which aligns with the ground truth's explanation.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation clearly states that 'AsyncWebCrawler' enables asynchronous crawling capabilities that support LLM extraction. This aligns with the AsyncWebCrawler class in the code that orchestrates asynchronous web crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements asynchronous web crawling functionality with built-in LLM extraction support through its arun method, which accepts an extraction_strategy parameter for processing crawled content with language models.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core functionality of AsyncWebCrawler enabling asynchronous crawling with LLM extraction support, which aligns with the ground truth's description of the class's key capabilities.",
      "error_type": ""
    }
  },
  {
    "document_text": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract structured data or relevant content from web pages asynchronously. Below are two examples demonstrating how to use `LLMExtractionStrategy` for different purposes with the AsyncWebCrawler.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "Although not explicitly mentioned in the documentation snippet, 'LLMExtractionStrategy' extends the 'ExtractionStrategy' base class. This base class underpins the extraction functionality, making it an implicit part of the LLM extraction pathway.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class defines a foundation for using LLMs to extract structured data from web pages by implementing a parallel processing system through its extract() and run() methods",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy extends ExtractionStrategy, but misses the key functionality of parallel processing and structured data extraction mentioned in the ground truth",
      "error_type": "incomplete_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler via 'async with AsyncWebCrawler(verbose=True) as crawler:', demonstrating its use for crawling operations.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements the functionality shown in the documentation example by providing asynchronous web crawling capabilities with customizable extraction strategies, as demonstrated in the example's use of LLMExtractionStrategy to extract tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used for crawling operations and shows its instantiation, but misses the crucial aspect of its customizable extraction strategies and specific functionality for tech content extraction that's central to the ground truth relationship.",
      "error_type": "incomplete_core_functionality"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example code calls 'await crawler.arun(...)', thereby invoking the arun() method of AsyncWebCrawler to perform the crawl, making it explicitly used.",
    "ground_truth_relationship": "The documentation demonstrates how to use the arun() method with LLMExtractionStrategy to filter website content based on specific criteria, while the code shows the actual implementation that handles crawling, caching, and content extraction with various strategies.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling, but misses the core relationship between the documentation and code - that arun() implements content extraction strategies and filtering as shown in the documentation example",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The snippet explicitly instantiates LLMExtractionStrategy with parameters such as provider, api_token, and an instruction to extract technology\u2010related content, indicating its direct role in the extraction process.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the documented content extraction functionality by processing HTML content through LLM models with specific instructions, as shown in Example 2 where it extracts only technology-related content from NBC News using GPT-4.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures that LLMExtractionStrategy handles content extraction using LLM models with specific parameters and instructions, which aligns with the ground truth's explanation of how it extracts targeted content (e.g., technology-related) using LLM models.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy, so although ExtractionStrategy is not directly mentioned in the snippet, it is implicitly involved via inheritance in the extraction process.",
    "ground_truth_relationship": "The ExtractionStrategy base class provides the core framework that enables the documented example to execute custom content extraction logic through its LLMExtractionStrategy subclass, particularly shown in the example where it extracts tech-related content from NBC News.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the inheritance relationship between LLMExtractionStrategy and ExtractionStrategy, and accurately notes its role in the extraction process. While less detailed than the ground truth, it captures the core relationship.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to technology from the NBC News business page.\n\n```python\nimport os\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nasync def extract_tech_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            extraction_strategy=LLMExtractionStrategy(\n                provider=\"openai/gpt-4o\",\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only content related to technology\"\n            ),\n            bypass_cache=True,\n        )\n\n    tech_content = json.loads(result.extracted_content)\n    print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n\n    with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(tech_content, f, indent=2)\n\nasyncio.run(extract_tech_content())\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the example accesses 'result.extracted_content' to retrieve the extracted tech content. This attribute is part of the public interface of the CrawlResult model.",
    "ground_truth_relationship": "The extracted_content property stores the text content filtered by LLMExtractionStrategy as shown in the example where tech-related content from NBC News is stored and later parsed as JSON.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that extracted_content contains the filtered/extracted content from crawling, with ground truth providing more implementation context but no fundamental contradiction",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet begins with 'async with AsyncWebCrawler(verbose=True) as crawler', which directly instantiates and uses the AsyncWebCrawler class for session-based crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class enables dynamic content crawling through its arun() method which supports session-based browsing, JavaScript execution, and custom extraction strategies as demonstrated in the documentation's GitHub commits crawling example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description captures the basic usage of AsyncWebCrawler with async context manager, but misses crucial functionality around dynamic content crawling, JavaScript execution, and custom extraction strategies that are core to the class's purpose",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls 'await crawler.arun(...)' to crawl multiple pages of GitHub commits. This demonstrates explicit usage of the arun() method to perform the crawling and extraction process.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method provides the core functionality for executing dynamic web crawling with session management, implementing features like bypass_cache and session_id parameters that are demonstrated in the document's GitHub commits crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic functionality of arun() for crawling but misses the crucial aspects of session management and dynamic crawling features mentioned in the ground truth",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "In the code snippet, a new instance of JsonCssExtractionStrategy is created with a defined extraction schema. This clearly exhibits its direct use for extracting commit data via CSS selectors.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the schema-based extraction logic shown in the documentation by parsing HTML elements using BeautifulSoup and applying the defined selectors to extract commit data from GitHub's pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that JsonCssExtractionStrategy uses a schema for extraction via CSS selectors, but omits the crucial BeautifulSoup parsing mechanism and the full extraction process described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "## Dynamic Content with Sessions\n\nHere's a real-world example of crawling GitHub commits across multiple pages:\n\n```python\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        # Define navigation JavaScript\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        # Define wait condition\n        wait_for = \"\"\"() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (commits.length === 0) return false;\n            const firstCommit = commits[0].textContent.trim();\n            return firstCommit !== window.firstCommit;\n        }\"\"\"\n        \n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h4.markdown-title\",\n                    \"type\": \"text\",\n                    \"transform\": \"strip\",\n                },\n            ],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # Crawl multiple pages\n        for page in range(3):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                bypass_cache=True\n            )\n\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "The final part of the snippet calls 'await crawler.crawler_strategy.kill_session(session_id)', which explicitly uses the kill_session() method of AsyncPlaywrightCrawlerStrategy to clean up the session after crawling.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the page and context objects stored in the sessions dictionary, which is demonstrated in the documentation's final step of the GitHub commit crawling example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that kill_session() is used to clean up browser resources after crawling is complete. The predicted and ground truth align on the key relationship of session cleanup.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Getting Started \ud83d\udee0\ufe0f\n\nFirst, let's import the necessary modules and create an instance of `AsyncWebCrawler`. We'll use an async context manager, which handles the setup and teardown of the crawler for us.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # We'll add our crawling code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler with an async context manager. This clearly demonstrates its intended use as the primary class for setting up and managing crawling operations.",
    "ground_truth_relationship": "The code implements an AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly support the documented usage pattern of initializing and cleaning up the crawler within an async context manager block.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship that AsyncWebCrawler is designed to be used with an async context manager, matching the implementation shown in the code with __aenter__ and __aexit__ methods.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly demonstrates the instantiation of AsyncWebCrawler with different browser_type values (e.g., 'firefox', 'webkit') along with other keyword arguments such as 'verbose' and 'headless'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements browser selection through its crawler_strategy parameter, which accepts a PlaywrightCrawlerStrategy instance that can be configured with different browser_type values (firefox, webkit, or chromium) as shown in the documentation examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the core idea that AsyncWebCrawler supports different browser types through configuration parameters, even though it doesn't explicitly mention the crawler_strategy implementation detail. Both descriptions convey the same fundamental browser selection capability.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "In the usage examples, the arun() method is called on the AsyncWebCrawler instance to initiate the crawl of a URL, directly reflecting functionality described in the documentation.",
    "ground_truth_relationship": "The arun() method enables browser selection through the crawler_strategy object which is initialized with the specified browser_type (firefox, webkit, or chromium) when creating the AsyncWebCrawler instance.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling URLs, but misses the key relationship about how browser selection works through the crawler_strategy object based on browser_type parameter",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "### Browser Selection \ud83c\udf10\n\nCrawl4AI supports multiple browser engines. Here's how to use different browsers:\n\n```python\n# Use Firefox\nasync with AsyncWebCrawler(browser_type=\"firefox\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use WebKit\nasync with AsyncWebCrawler(browser_type=\"webkit\", verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n\n# Use Chromium (default)\nasync with AsyncWebCrawler(verbose=True, headless=True) as crawler:\n    result = await crawler.arun(url=\"https://www.example.com\", bypass_cache=True)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Although not mentioned by name in the snippet, AsyncWebCrawler internally creates an instance of AsyncPlaywrightCrawlerStrategy (when no custom strategy is provided) and passes the browser_type parameter. This strategy handles the browser selection (Firefox, WebKit, Chromium) based on the provided arguments.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements browser selection through its start() method, which uses the browser_type parameter to launch either Firefox, WebKit, or Chromium (default) browsers as documented in the browser selection examples.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures that AsyncPlaywrightCrawlerStrategy handles browser selection based on browser_type parameter, allowing choice between Firefox, WebKit, and Chromium browsers, which aligns with the ground truth's explanation of implementation through the start() method.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation snippet shows an explicit usage example calling 'crawler.arun(...)' with timing parameters (page_timeout and delay_before_return_html). This directly maps to the AsyncWebCrawler.arun() method, which is responsible for initiating a crawl with timing controls.",
    "ground_truth_relationship": "The code implements the documented timing controls through the **kwargs parameter in the arun() method, which accepts page_timeout and delay_before_return_html settings that are passed to the crawler_strategy.crawl() function.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that arun() handles timing parameters through its interface, even though it doesn't explicitly mention kwargs. The core relationship between the documentation and code implementation is accurately captured.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Timing Control\n\n### Delays and Timeouts\n\nControl timing of interactions:\n\n```python\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    page_timeout=60000,              # Page load timeout (ms)\n    delay_before_return_html=2.0,    # Wait before capturing content\n)\n```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "Inside AsyncWebCrawler.arun(), the crawl process is delegated to the crawler strategy. The timing parameters (page_timeout and delay_before_return_html) shown in the snippet are handled in the crawl() method of AsyncPlaywrightCrawlerStrategy. Although the snippet does not directly reference this class, its functionality is implicitly utilized.",
    "ground_truth_relationship": "The code implements timing control through parameters like page_timeout and delay_before_return_html in the crawl method of AsyncPlaywrightCrawlerStrategy, where page_timeout controls the maximum wait time for page loads and delay_before_return_html adds a pause before capturing the final HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly captures the main relationship that timing control is handled in the crawl() method through parameters like page_timeout and delay_before_return_html. While it includes a minor irrelevant detail about AsyncWebCrawler.arun(), the core understanding of how timing is controlled matches the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet explicitly imports and instantiates AsyncWebCrawler using 'async with AsyncWebCrawler(verbose=True) as crawler', indicating that this class is the primary entry point for asynchronous crawling.",
    "ground_truth_relationship": "The code implements the AsyncWebCrawler class with async context manager methods (__aenter__ and __aexit__) that directly enable the 'async with' syntax shown in the quickstart documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that AsyncWebCrawler is used with async context management ('async with') syntax, which aligns with the ground truth's explanation of the class implementing __aenter__ and __aexit__ methods to enable this functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The snippet calls the 'arun' method (result = await crawler.arun(url=\"https://www.nbcnews.com/business\")), demonstrating its direct role in performing the crawling operation.",
    "ground_truth_relationship": "The code implements AsyncWebCrawler's arun() method which enables the asynchronous web crawling functionality shown in the quickstart documentation by handling URL fetching, content extraction, and error handling using async/await patterns.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies that arun() is used for crawling operations, but misses crucial aspects about its asynchronous implementation, error handling, and content processing capabilities that are core to the relationship shown in the ground truth.",
      "error_type": "significant_omission"
    }
  },
  {
    "document_text": "## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```",
    "artifact_title": "CrawlResult.markdown",
    "predicted_relationship": "The code snippet prints 'result.markdown', which directly accesses the 'markdown' attribute of the CrawlResult object returned by the crawler. This demonstrates its use for presenting extracted content.",
    "ground_truth_relationship": "The CrawlResult.markdown property stores the extracted content as a formatted string, which is displayed in the Quick Start example when printing result.markdown.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that the markdown attribute/property stores and displays extracted content via result.markdown in the example",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly imports LLMExtractionStrategy (via 'from crawl4ai.extraction_strategy import LLMExtractionStrategy') and instantiates it with a schema and instruction. This demonstrates its direct use for LLM-based content extraction.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements structured content extraction by accepting a schema (like ArticleContent), provider, and instruction parameters, then using LLM completions to parse web content into the specified structured format.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy is used for content extraction with LLMs, but it focuses mainly on the import/instantiation syntax rather than explaining the core functionality of structured content extraction using schemas and LLM completions to parse web content.",
      "error_type": "incomplete_core_concept"
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "LLMExtractionStrategy is a subclass of ExtractionStrategy. Although ExtractionStrategy is not explicitly mentioned in the snippet, it underpins LLMExtractionStrategy by defining the extraction interface.",
    "ground_truth_relationship": "The ExtractionStrategy abstract base class provides the foundation for implementing specialized content extractors like LLMExtractionStrategy shown in the documentation, with its extract() method defining the core interface for pulling structured data from HTML content.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies the core relationship that LLMExtractionStrategy inherits from ExtractionStrategy and captures the essential parent-child class relationship. While it's more concise than the ground truth, it doesn't contradict or misunderstand the fundamental relationship.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The code sample calls 'await crawler.arun(...)' with the extraction_strategy parameter set to the LLMExtractionStrategy instance. This indicates that the arun() method of AsyncWebCrawler is used to initiate the crawl and process the extraction.",
    "ground_truth_relationship": "The arun() method implements the documented LLM-based extraction functionality by accepting an extraction_strategy parameter that processes the crawled content according to the specified schema and instructions.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between arun() and extraction_strategy, showing how they work together for content extraction, even though it doesn't detail all the implementation aspects mentioned in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Structured Content Selection\n\n### Using LLMs for Smart Selection\n\nUse LLMs to intelligently extract specific types of content:\n\n```python\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleContent(BaseModel):\n    title: str\n    main_points: List[str]\n    conclusion: str\n\nstrategy = LLMExtractionStrategy(\n    provider=\"ollama/nemotron\",  # Works with any supported LLM\n    schema=ArticleContent.schema(),\n    instruction=\"Extract the main article title, key points, and conclusion\"\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    extraction_strategy=strategy\n)\narticle = json.loads(result.extracted_content)\n```",
    "artifact_title": "CrawlResult.extracted_content",
    "predicted_relationship": "After crawling, the extracted content is retrieved from the 'extracted_content' attribute of the result. This attribute, defined in CrawlResult, holds the processed output from the extraction strategy.",
    "ground_truth_relationship": "The CrawlResult.extracted_content field stores the LLM-processed structured content as a JSON string that matches the defined Pydantic schema for later parsing into typed objects.",
    "traceability_granularity": "Statement-level",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies that extracted_content stores the processed output, but misses the crucial aspect that it's specifically stored as a JSON string matching a Pydantic schema for typed parsing",
      "error_type": "missing_key_detail"
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation explicitly instructs to use 'CSS Strategy' for well-structured HTML. Although the text uses a shorthand 'CSS Strategy', the corresponding implementation in the code is 'JsonCssExtractionStrategy', which extracts content using CSS selectors based on a defined schema.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class directly implements the CSS Strategy mentioned in the documentation by using CSS selectors to efficiently parse well-structured HTML content as recommended in the strategy selection guide.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that JsonCssExtractionStrategy implements the CSS Strategy mentioned in the documentation and uses CSS selectors for HTML parsing, matching the core relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation clearly recommends using an 'LLM Strategy' for content that is in natural language text. 'LLMExtractionStrategy' is the artifact that handles extraction via language models, and it directly corresponds to the guide's recommendation.",
    "ground_truth_relationship": "The LLMExtractionStrategy class implements the LLM Strategy option described in the documentation, specifically handling natural language text processing with configurable performance based on the provider and offering semantic understanding through its extract() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that LLMExtractionStrategy implements the LLM Strategy recommended for natural language text processing, capturing the core relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Strategy Selection Guide\n\nChoose your strategy based on these factors:\n\n1. **Content Structure**\n   - Well-structured HTML \u2192 Use CSS Strategy\n   - Natural language text \u2192 Use LLM Strategy\n   - Mixed/Complex content \u2192 Use Cosine Strategy\n\n2. **Performance Requirements**\n   - Fastest: CSS Strategy\n   - Moderate: Cosine Strategy\n   - Variable: LLM Strategy (depends on provider)\n\n3. **Accuracy Needs**\n   - Highest structure accuracy: CSS Strategy\n   - Best semantic understanding: LLM Strategy\n   - Best content relevance: Cosine Strategy",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "For content with mixed or complex characteristics, the guide advises using the 'Cosine Strategy'. 'CosineStrategy' performs clustering based on cosine similarity to evaluate semantic relevance, directly aligning with the documentation's recommendations.",
    "ground_truth_relationship": "The CosineStrategy class directly implements the document's 'Mixed/Complex content' recommendation by using cosine similarity and hierarchical clustering to process and analyze text content with moderate performance characteristics.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that Cosine Strategy is recommended for mixed/complex content and accurately describes its core functionality using cosine similarity for semantic analysis, which matches the ground truth's explanation.",
      "error_type": "none"
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The documentation snippet explicitly recommends using JsonCssExtractionStrategy as a powerful tool for extracting highly structured data. It advises users to start with a basic schema, test incrementally, and use keys such as 'default' to handle missing data and 'transform' to clean or format values. These concepts directly align with the intended usage of the JsonCssExtractionStrategy class.",
    "ground_truth_relationship": "The code implements a CSS-based data extraction strategy that directly supports the documented tips, particularly the 'Start Simple' and 'Test Incrementally' recommendations by allowing users to define and process schemas gradually through the schema parameter.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions emphasize how JsonCssExtractionStrategy supports structured data extraction through schemas, with the predicted text accurately capturing the core functionality and alignment with the documented tips, particularly around schema usage and incremental implementation.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Tips for Advanced Usage\n\n1. **Start Simple**: Begin with a basic schema and gradually add complexity.\n2. **Test Incrementally**: Test each part of your schema separately before combining them.\n3. **Use Chrome DevTools**: The Element Inspector is invaluable for identifying the correct selectors.\n4. **Handle Missing Data**: Use the `default` key in your field definitions to handle cases where data might be missing.\n5. **Leverage Transforms**: Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n6. **Consider Performance**: Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy to extract highly structured data from even the most complex web pages, making it a powerful tool for web scraping and data analysis tasks.",
    "artifact_title": "ExtractionStrategy",
    "predicted_relationship": "JsonCssExtractionStrategy is built as an extension of ExtractionStrategy. Although the base class is not explicitly mentioned in the text, its role as the abstract foundation for extraction strategies makes it an implicit part of the extraction approach recommended by the documentation.",
    "ground_truth_relationship": "The ExtractionStrategy base class implements the parallel processing functionality mentioned in the 'Consider Performance' tip by using ThreadPoolExecutor to handle complex data extraction tasks efficiently.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on JsonCssExtractionStrategy being an extension of ExtractionStrategy, while the ground truth describes ExtractionStrategy's parallel processing implementation using ThreadPoolExecutor. These are completely different aspects of the system.",
      "error_type": "wrong_relationship_focus"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation snippet directly instantiates AsyncWebCrawler with 'verbose=True' inside an async context manager ('async with AsyncWebCrawler(verbose=True) as crawler:'), demonstrating its role as the primary crawler for dynamic session\u2010based crawling.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based dynamic content crawling through its arun method, which supports pagination and content updates via custom JavaScript injection (js_next_page) and wait conditions as shown in the documentation example.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description correctly identifies AsyncWebCrawler's usage with async context manager, but misses the crucial functionality of dynamic content crawling through pagination and JavaScript injection described in the ground truth",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The example calls 'crawler.arun(...)' to perform the crawl on a given URL with various parameters such as session_id, css_selector, and extraction_strategy. This explicitly demonstrates the use of the 'arun()' method for dynamic content extraction.",
    "ground_truth_relationship": "The AsyncWebCrawler.arun() method enables session-based dynamic crawling by accepting parameters like session_id, js_code, and wait_for that allow executing JavaScript and waiting for dynamic content updates as described in the documentation example.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description identifies the basic usage of arun() for crawling with parameters, but misses the key aspect of enabling session-based dynamic crawling with JavaScript execution and content update waiting capability that is central to the ground truth.",
      "error_type": "major_omission"
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The snippet creates an extraction strategy by directly instantiating JsonCssExtractionStrategy with a defined schema and verbose flag. This strategy is then passed to the 'arun()' method to extract commit details from the crawled content.",
    "ground_truth_relationship": "The JsonCssExtractionStrategy class implements the structured data extraction logic described in the documentation by parsing the schema's baseSelector and fields to extract commit information from GitHub's dynamic pages.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality of JsonCssExtractionStrategy - using a schema to extract structured data from web content. The predicted description accurately describes the instantiation and usage, while the ground truth provides more implementation details, but the fundamental relationship is the same.",
      "error_type": ""
    }
  },
  {
    "document_text": "### Advanced Session-Based Crawling with Dynamic Content For modern web applications with dynamic content loading, here's how to handle pagination and content updates: ```python async def crawl_dynamic_content(): async with AsyncWebCrawler(verbose=True) as crawler: url = \"https://github.com/microsoft/TypeScript/commits/main\" session_id = \"typescript_commits_session\" js_next_page = \"\"\" const button = document.querySelector('a[data-testid=\"pagination-next-button\"]'); if (button) button.click(); \"\"\" wait_for = \"\"\"() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (commits.length === 0) return false; const firstCommit = commits[0].textContent.trim(); return firstCommit !== window.firstCommit; }\"\"\" schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\", \"transform\": \"strip\", }, ], } extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True) for page in range(3):  # Crawl 3 pages result = await crawler.arun( url=url, session_id=session_id, css_selector=\"li.Box-sc-g0xbh4-0\", extraction_strategy=extraction_strategy, js_code=js_next_page if page > 0 else None, wait_for=wait_for if page > 0 else None, js_only=page > 0, bypass_cache=True, headless=False, ) await crawler.crawler_strategy.kill_session(session_id) ```",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy.kill_session()",
    "predicted_relationship": "After each crawl iteration, the example explicitly calls 'crawler.crawler_strategy.kill_session(session_id)' to terminate the active session. This call manages session-based control and cleanup in the crawling process.",
    "ground_truth_relationship": "The kill_session method cleans up browser resources by closing the Playwright page and context objects when the dynamic content crawling session is complete.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core purpose of the kill_session method - terminating and cleaning up an active session. While it doesn't specifically mention Playwright page/context objects, it correctly describes the high-level relationship and cleanup functionality.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation usage example explicitly shows a call to crawler.arun(\u2026) to initiate a crawl, demonstrating its role in fetching and processing data.",
    "ground_truth_relationship": "The code implements error handling patterns shown in the documentation through try-catch blocks that return CrawlResult objects with error messages, while also incorporating the documented best practices of caching and strategy selection.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description only focuses on the basic arun() method call shown in the documentation example, while missing the crucial error handling and best practices implementation that's the main relationship between code and docs.",
      "error_type": "incomplete_coverage"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CrawlResult",
    "predicted_relationship": "The example inspects properties such as result.success, result.error_message, and result.extracted_content, which belong to the CrawlResult class that encapsulates the crawl outcome.",
    "ground_truth_relationship": "The CrawlResult class captures the success/error states described in the error handling documentation through its success, error_message, and extracted_content fields that enable the demonstrated error checking pattern.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures how the CrawlResult class properties are used for error handling and results inspection, matching the core relationship shown in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "LLMExtractionStrategy",
    "predicted_relationship": "The documentation advises using LLM for complex interpretation of content. This recommendation aligns with the LLMExtractionStrategy class, which implements LLM\u2010based extraction.",
    "ground_truth_relationship": "The code implements error handling and retry logic through ThreadPoolExecutor and try-catch blocks, directly aligning with the documentation's 'Handle Errors' section which shows error checking patterns.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "incorrect",
      "justification": "The predicted description focuses on LLM-based content interpretation, while the ground truth describes error handling and retry logic implementation. These are different functional aspects of the code.",
      "error_type": "wrong_focus"
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "CosineStrategy",
    "predicted_relationship": "The guideline 'Try Cosine for content relevance' maps directly to the CosineStrategy class, which implements a cosine similarity-based approach for assessing content relevance.",
    "ground_truth_relationship": "The CosineStrategy class implements the documented 'Cosine for content relevance' best practice by using cosine similarity metrics in the filter_documents_embeddings method to evaluate and filter content based on semantic relevance scores.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description correctly identifies that the CosineStrategy class implements content relevance assessment using cosine similarity, which aligns with the core relationship described in the ground truth.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Best Practices\n\n1. **Choose the Right Strategy**\n   - Start with CSS for structured data\n   - Use LLM for complex interpretation\n   - Try Cosine for content relevance\n\n2. **Optimize Performance**\n   - Cache LLM results\n   - Keep CSS selectors specific\n   - Tune similarity thresholds\n\n3. **Handle Errors**\n   ```python\n   result = await crawler.arun(\n       url=\"https://example.com\",\n       extraction_strategy=strategy\n   )\n   \n   if not result.success:\n       print(f\"Extraction failed: {result.error_message}\")\n   else:\n       data = json.loads(result.extracted_content)\n   ```\n\nEach strategy has its strengths and optimal use cases. Explore the detailed documentation for each strategy to learn more about their specific features and configurations.",
    "artifact_title": "JsonCssExtractionStrategy",
    "predicted_relationship": "The advice 'Start with CSS for structured data' is directly associated with the JsonCssExtractionStrategy class, which utilizes CSS selectors to extract structured data.",
    "ground_truth_relationship": "The code implements the CSS strategy mentioned in the documentation's best practices by using BeautifulSoup selectors to extract structured data according to a predefined schema.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the core relationship between the documentation's CSS strategy recommendation and the code implementation using BeautifulSoup selectors for structured data extraction.",
      "error_type": "none"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "RegexChunking",
    "predicted_relationship": "The documentation snippet explicitly names RegexChunking and demonstrates its usage through an example that imports the class, instantiates it with a custom 'patterns' parameter, and calls its 'chunk' method on sample text.",
    "ground_truth_relationship": "The RegexChunking code implements text splitting using the re.split() function iteratively over each pattern in self.patterns, which directly corresponds to the documentation's description of using regular expressions to split text based on specified patterns like paragraphs or sentences.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "partially_correct",
      "justification": "The predicted description focuses on showing usage/API examples rather than explaining the core functionality of using re.split() iteratively on patterns, though it does correctly identify the class purpose",
      "error_type": "incomplete_core_mechanism"
    }
  },
  {
    "document_text": "### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for creating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n- Great for structured text with consistent delimiters.\n- Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n- `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n```python\nfrom crawl4ai.chunking_strategy import RegexChunking\n\n# Define patterns for splitting text\npatterns = [r'\\n\\n', r'\\. ']\nchunker = RegexChunking(patterns=patterns)\n\n# Sample text\ntext = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n\n# Chunk the text\nchunks = chunker.chunk(text)\nprint(chunks)\n```",
    "artifact_title": "ChunkingStrategy",
    "predicted_relationship": "RegexChunking is implemented as a subclass of ChunkingStrategy. Although the documentation snippet does not explicitly mention ChunkingStrategy, its presence is implicit through the inheritance relationship which defines the public interface for chunking strategies.",
    "ground_truth_relationship": "The ChunkingStrategy abstract base class defines the foundational interface that RegexChunking must implement to perform text splitting using regular expressions through the required chunk() method.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions convey the core relationship that RegexChunking is a concrete implementation of the abstract ChunkingStrategy class, which defines the interface through the chunk() method. The predicted text captures this inheritance relationship accurately, even if it's less explicit about some implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler",
    "predicted_relationship": "The documentation explicitly mentions 'Crawl4AI's AsyncWebCrawler class' to support persistent session-based crawling. This class is designed to manage sessions through parameters such as 'session_id'.",
    "ground_truth_relationship": "The AsyncWebCrawler class implements session-based crawling through the session_id parameter in its arun method, which allows maintaining persistent browser sessions across multiple requests as documented.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "The predicted description accurately captures the main relationship - that the AsyncWebCrawler class supports session-based crawling through the session_id parameter. Both descriptions emphasize this core functionality for maintaining persistent browser sessions.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncWebCrawler.arun()",
    "predicted_relationship": "The documentation advises using the same 'session_id' across multiple 'arun' calls. This explicitly points to the 'arun' method of AsyncWebCrawler, which manages the crawling requests while preserving session state.",
    "ground_truth_relationship": "The `arun()` method implements session-based crawling by accepting a `session_id` parameter through kwargs and storing it in the CrawlResult object, enabling state persistence across multiple crawl requests.",
    "traceability_granularity": "Method",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions capture the core functionality that the arun method handles session-based crawling through the session_id parameter, enabling state persistence across requests. The predicted description correctly identifies the high-level relationship even if it doesn't mention all implementation details.",
      "error_type": ""
    }
  },
  {
    "document_text": "## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session across multiple requests. This is crucial when:\n\n1. The content changes dynamically without URL changes\n2. You need to interact with the page (e.g., clicking buttons) between requests\n3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the `session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n- **Session ID**: A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n- **JavaScript Execution**: Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n- **CSS Selectors**: Use these to target specific elements for extraction or interaction.\n- **Extraction Strategy**: Define how to extract structured data from the page.\n- **Wait Conditions**: Specify conditions to wait for before considering the page loaded.",
    "artifact_title": "AsyncPlaywrightCrawlerStrategy",
    "predicted_relationship": "While not mentioned by name in the documentation snippet, AsyncWebCrawler internally uses a crawler strategy instance (by default AsyncPlaywrightCrawlerStrategy) which handles session management (via the session_id parameter) and JavaScript execution (via the js_code parameter). This makes it an implicit component in supporting session-based crawling.",
    "ground_truth_relationship": "The AsyncPlaywrightCrawlerStrategy class implements session-based crawling through its sessions dictionary and associated methods like kill_session(), which maintain browser contexts and pages across multiple requests as described in the documentation.",
    "traceability_granularity": "Class",
    "evaluation": {
      "alignment_label": "correct",
      "justification": "Both descriptions accurately convey that AsyncPlaywrightCrawlerStrategy handles session-based crawling through managing browser sessions and contexts. The predicted description correctly identifies its role in session management and JavaScript execution, even though it describes it from the perspective of AsyncWebCrawler using it.",
      "error_type": ""
    }
  }
]